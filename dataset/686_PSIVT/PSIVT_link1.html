<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8"> </head><body>2023
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PSIVT 2023 - Pacific-Rim Symposium on Image and Video Technology - Auckland, New Zealand</title>
    <meta name="description" content="The 11th edition of the Pacific-Rim Symposium on Image and Video Technology (PSIVT 2023). A global conference focused on image and video processing, analysis, and applications. Located in Auckland, New Zealand.">
    <meta name="keywords" content="PSIVT 2023, Pacific-Rim Symposium, Image and Video Technology, Auckland, New Zealand, global conference, image processing, video processing, analysis, applications">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <style>
  .carousel-item img {
    width: 800px;
    height: 600px;
    object-fit: cover;
  }
  .mt-5 {
  padding-top: 100px;
  margin-top: -100px;
}

</style>

  
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
      <div class="container">
        <a class="navbar-brand" href="#">PSIVT 2023</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link" href="PSIVT2023_CfP_Poster.pdf" target="_blank">CoP</a>
            </li>
			<li class="nav-item">
              <a class="nav-link" href="#final">[Final Submission]</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#program">[Program]</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#committee">Committee</a>
            </li>
			      <li class="nav-item">
              <a class="nav-link" href="#pc-members">PC Members</a>
            </li>
			      <li class="nav-item">
              <a class="nav-link" href="#keynote-speakers">Keynote Speakers</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#registration">Registration</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#venue">Venue</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#sponsors">Sponsors</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#committee">Contact</a>
            </li>
          </ul>
        </div>
      </div>
</nav>
    <header class="jumbotron jumbotron-fluid" style="background-image: url('auckland_city.jpg'); background-size: cover; background-position: center; background-color: white; color: black;">
  <div class="container text-center" style="background-color: rgba(255, 255, 255, 0.8); padding: 20px; border-radius: 10px;">
    <img src="logo_200_115.png" alt="PSIVT 2023 Logo">
    <h1 class="display-4">Pacific-Rim Symposium on Image and Video Technology (PSIVT) 2023</h1>
    <!--<h3 class="mb-4">A Hybrid Conference Experience: <a href="https://aut.zoom.us/j/95468713549" target="_blank">Join Zoom Meeting here</a> or <a href="https://aut.zoom.us/meeting/tJEpceGvrzgvHt1SJYDfVo_on7_OnCokU68g/ics?icsToken=98tyKuCtqj0iHdWWtR2ORowMBo_oWe3zmHZYgrcMrzXcJyZlbTz5OLV9HbxcH9Pl" target="_blank">Import to calendar</a></h3>-->
    <p class="lead">Taking place on November 22-24, 2023, both at the <a href="https://goo.gl/maps/7AUyy5urhgo4XCv16" target="_blank">@AUT City Campus</a>, Auckland, New Zealand, and <a href="https://aut.zoom.us/j/95468713549" target="_blank">virtually</a>.</p>
    <p>The proceedings of the PSIVT'23 are now available online. Access the full texts and explore the latest research findings at:</p>
        <p><a href="https://link.springer.com/book/10.1007/978-981-97-0376-0" target="_blank">https://link.springer.com/book/10.1007/978-981-97-0376-0</a></p></div>
</header>

    <main class="container">
      <section id="about" class="mt-5">
        <h2>About PSIVT 2023</h2>
		<hr>
        <p><a href="https://psivt2023.aut.ac.nz/" target="_blank">PSIVT 2023</a> marks the 11th edition of the Pacific-Rim Symposium on Image and Video Technology, a conference that gathers researchers and practitioners from around the globe to discuss the latest breakthroughs in image and video processing, analysis, and applications. This year's symposium is proudly taking place in <a href="https://www.aucklandnz.com/" target="_blank">Auckland, New Zealand</a>, a city celebrated for its natural beauty, cultural diversity, and welcoming atmosphere.</p>
        <p>Over the years, PSIVT has been held in various cities across the Pacific Rim, including Sydney, Australia; Wuhan, China; Guanajuato, Mexico; Singapore; Gwangju, South Korea; Tokyo, Japan; Santiago, Chile; and Hsinchu, Taiwan. The rich history of this symposium highlights its importance and continued relevance to the field of image and video technology.</p>
        <p><a href="https://www.newzealand.com/int/auckland/" target="_blank">Auckland</a>, often referred to as the "City of Sails" due to its thriving waterfront and sailing culture, is the largest city in New Zealand. It is renowned for its scenic landscapes, including stunning beaches, lush rainforests, and dormant volcanic cones. As a top tourist destination, Auckland offers a multitude of attractions, outdoor adventures, and world-class cuisine that cater to a wide range of interests and preferences.</p>
        <p>The conference is organized by <a href="https://www.aut.ac.nz/study/study-options/engineering-computer-and-mathematical-sciences/academic-staff/information-technology-and-software-engineering-department" target="_blank">the Department of Computer Science and Software Engineering</a> within the School of Engineering, Computer, and Mathematical Sciences (ECMS) at <a href="https://www.aut.ac.nz/" target="_blank">Auckland University of Technology (AUT)</a>. AUT is a leading higher education institution in New Zealand, dedicated to fostering innovation, creativity, and academic excellence. The ECMS school, recognized for its cutting-edge research and teaching, provides an outstanding platform for interdisciplinary collaboration, connecting experts from various fields to develop pioneering solutions in image and video technology.</p>
        <p>By choosing Auckland as the venue for PSIVT 2023, we aim to provide a stimulating environment that will inspire our attendees to explore new ideas, engage in thought-provoking discussions, and forge lasting connections. We look forward to welcoming you to Auckland, a city that truly embodies the spirit of innovation and progress that drives the field of image and video technology.</p>
        <div id="carouselExampleControls" class="carousel slide mt-3" data-ride="carousel">
  <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
  <ol class="carousel-indicators">
    <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="3"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="4"></li>
	<li data-target="#carouselExampleIndicators" data-slide-to="5"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="6"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="7"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="8"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="9"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="10"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="11"></li>
    <li data-target="#carouselExampleIndicators" data-slide-to="12"></li>
  </ol>
  <div class="carousel-inner">
    <div class="carousel-item active">
      <img src="auckland/1.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 1">
    </div>
    <div class="carousel-item">
      <img src="auckland/2.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 2">
    </div>
    <div class="carousel-item">
      <img src="auckland/3.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 3">
    </div>
	<div class="carousel-item active">
      <img src="auckland/4.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 4">
    </div>
    <div class="carousel-item">
      <img src="auckland/5.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 5">
    </div>
    <div class="carousel-item">
      <img src="auckland/6.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 6">
    </div>
    <div class="carousel-item active">
      <img src="auckland/7.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 7">
    </div>
    <div class="carousel-item">
      <img src="auckland/8.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 8">
    </div>
    <div class="carousel-item">
      <img src="auckland/9.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 9">
    </div>
	<div class="carousel-item active">
      <img src="auckland/10.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 10">
    </div>
    <div class="carousel-item">
      <img src="auckland/11.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 11">
    </div>
    <div class="carousel-item">
      <img src="auckland/12.jpg" class="d-block w-100" style="object-fit: cover;" alt="Auckland Image 12">
    </div>
  </div>
  <a class="carousel-control-prev" href="#carouselExampleIndicators" role="button" data-slide="prev">
    <span class="carousel-control-prev-icon" aria-hidden="true"></span>
    <span class="sr-only">Previous</span>
  </a>
  <a class="carousel-control-next" href="#carouselExampleIndicators" role="button" data-slide="next">
    <span class="carousel-control-next-icon" aria-hidden="true"></span>
    <span class="sr-only">Next</span>
  </a>
</div>

  <a class="carousel-control-prev" href="#carouselExampleControls" role="button" data-slide="prev">
    <span class="carousel-control-prev-icon" aria-hidden="true"></span>
    <span class="sr-only">Previous</span>
  </a>
  <a class="carousel-control-next" href="#carouselExampleControls" role="button" data-slide="next">
    <span class="carousel-control-next-icon" aria-hidden="true"></span>
    <span class="sr-only">Next</span>
  </a>
</div>

      </section>

	  <section id="committee" class="mt-5">
    <h2>Conference Committee</h2>
	<hr>
    <div class="container">
    <ul class="list-unstyled">
        <li class="media">
            <div class="profile-picture">
			<img src="https://academics.aut.ac.nz/minh.nguyen/photo" class="mr-3" alt="...">
			</div>
            <div class="media-body">
                <h5 class="mt-0 mb-1">General Chair:</h5>
                Minh Nguyen (<a href="mailto:minh.nguyen@aut.ac.nz">minh.nguyen@aut.ac.nz</a>), AUT, New Zealand
            </div>
        </li>
        <li class="media my-4">
            <div class="profile-picture">
			<img src="https://academics.aut.ac.nz/weiqi.yan/photo" class="mr-3" alt="...">
			</div>
            <div class="media-body">
                <h5 class="mt-0 mb-1">Program Chair:</h5>
                Wei Qi Yan (<a href="mailto:weiqi.yan@aut.ac.nz">weiqi.yan@aut.ac.nz</a>), AUT, New Zealand
            </div>
        </li>

		<li class="media my-4">
		<div class="profile-picture">
            <img src="https://academics.aut.ac.nz/parma.nand/photo" class="mr-3" alt="...">
			</div>
            <div class="media-body">
                <h5 class="mt-0 mb-1">Publication Chair:</h5>
                Parma Nand (<a href="mailto:parma.nand@aut.ac.nz">parma.nand@aut.ac.nz</a>), AUT, New Zealand
            </div>
        </li>
		<!--
		<li class="media my-4">
		<div class="profile-picture">
            <img src="terry.jpg" class="mr-3" alt="...">
			</div>
            <div class="media-body">
                <h5 class="mt-0 mb-1">Finance Chair:</h5>
                Terry Brydon (<a href="mailto:terry.brydon@aut.ac.nz">terry.brydon@aut.ac.nz</a>), AUT, New Zealand
            </div>
        </li>
		-->
		<li class="media my-4">
		<div class="profile-picture">
            <img src="https://academics.aut.ac.nz/xuejun.li/photo" class="mr-3" alt="...">
			</div>
            <div class="media-body">
                <h5 class="mt-0 mb-1">Local Arrangement Chair:</h5>
                Xuejun (Jack) Li (<a href="mailto:xuejun.li@aut.ac.nz">xuejun.li@aut.ac.nz</a>), AUT, New Zealand
            </div>
        </li>
  <li class="media my-4">
	<div class="profile-picture">
		<img src="https://academics.aut.ac.nz/raymond.lutui/photo" class="mr-3" alt="...">
	</div>
	<div class="media-body">
		<h5 class="mt-0 mb-1">Publicity Chair:</h5>
		Raymond Lutui (<a href="mailto:raymond.lutui@aut.ac.nz">raymond.lutui@aut.ac.nz</a>), AUT, New Zealand
	</div>
</li>

        <li class="media my-4">
            <h4>Regional Chairs</h4>
        <ul>
            <li>Bok-Suk Shin, Korea Polytechnic, Korea</li>
            <li>Domingo Mery, Universidad Catolica de Chile</li>
            <li>Fawzi Nashashibi, INRIA, France</li>
            <li>Guilin Yang, NIIT, CAS, China</li>
            <li>Nevrezİmamoğlu, AIST, Japan</li>
            <li>Jinjian Wu, Xidian University, China</li>
            <li>Han Wang, Xiamen University Malaysia</li>
            <li>Manoranjan Paul, Charles Sturt University, Australia</li>
        </ul>
        </li>
    </ul>
	<hr>
</div>

</section>

      <section id="topics" class="mt-5">
        <h2>Topics of Interest</h2>
		<hr>
        <p>The conference welcomes submissions on a wide range of topics, including but not limited to:</p>
        <ul>
          <li>3D point cloud processing</li>
          <li>3D vision and modeling</li>
          <li>Adversarial images and anti-attacking</li>
          <li>AI-based image/video processing for autonomous vehicles</li>
          <li>AI-based image/video processing for dynamic scene understanding</li>
          <li>Artificial intelligence in image and video processing</li>
          <li>Biomedical image and video analysis</li>
          <li>Biometrics and image forensics</li>
          <li>Caption and script generation</li>
          <li>Computer vision</li>
          <li>Computational photography and arts</li>
          <li>Deep learning for computer vision</li>
          <li>Document processing applications</li>
          <li>Fact/claim Detection and Verification</li>
          <li>Human-computer interaction</li>
          <li>Image and video analysis applications</li>
          <li>Image and video compression</li>
          <li>Image and video processing</li>
          <li>Image and video retrieval</li>
          <li>Image and video synthesis</li>
		  <li>Image to text and text to image generation</li>
          <li>Image/video coding and transmission</li>
          <li>Imaging and graphics hardware and visualization</li>
          <li>Machine learning for images and videos</li>
          <li>Mis/Dis Information detection</li>
          <li>Multimedia content analysis and understanding</li>
          <li>Object and scene recognition</li>
          <li>Pattern recognition</li>
          <li>Remote sensing and geospatial image analysis</li>
          <li>Robotics and autonomous systems</li>
          <li>Video tracking and motion analysis</li>
          <li>Virtual and augmented reality</li>
          <li>Visual surveillance and security</li>
          <li>Other emerging topics in image, video, and text technology</li>
        </ul>
      </section>





<section id="paper-proceedings" class="mt-5">
  <h2>Paper Submission and Proceedings</h2>
  <hr>
  <p>
    <a href="https://easychair.org/cfp/psivt2023" target="_blank">Call for papers</a>.
  </p>

  <p>
    All PSIVT 2023 accepted papers will be published by <a href="https://www.springer.com/" target="_blank">Springer</a> (formally accepted) as a <a href="https://www.springer.com/gp/computer-science/lncs" target="_blank">Lecture Notes in Computer Science (LNCS) proceedings</a>, predecessor volumes: PSIVT 2022.
    <br>The LNCS series is well-respected for its high-quality publications in the field of computer science.
  </p>
  <div>
    <img src="./Springers/Springer Logo.jpg" alt="Springer LNCS Logo" style="width: 50%; height: auto; margin-top: 20px;">
  </div>
  <div>
    <img src="./Springers/LNCS Logo.jpg" alt="LNCS Logo" style="width: 50%; height: auto; margin-top: 20px;">
  </div>
  <p>
    Authors are required to prepare their manuscripts using the provided templates.
    <br>
    <b>Papers accepted for publication are full papers (the limit is 14 pages including references).</b>
    <br>For those using LaTeX, you can find the template on <a href="https://www.overleaf.com/latex/templates/springer-lecture-notes-in-computer-science/kzwwpvhwnvfj" target="_blank">Overleaf</a>. For Microsoft Word users, the template can be found <a href="https://resource-cms.springernature.com/springer-cms/rest/v1/content/19238706/data" target="_blank">here</a>.
  </p>

  <p>
    Please ensure that your manuscript adheres to the guidelines provided by Springer. You can find detailed information on formatting, submission, and other requirements in the <a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" target="_blank">Conference Proceedings Guidelines</a>.
  </p>

  <p>
    Important: All submitted papers must be reviewed under a <span style="color:red;">double blind</span> review process. This means both reviewers and authors are anonymous. Ensure that no identifying details are present in the manuscript to maintain the integrity of this review process.
  </p>

  <p>
    We are pleased to announce that the submission process for our conference is now open and will be conducted through EasyChair, a conference management system that simplifies the submission and review process. <br>
    You can submit your research papers by visiting the following link:
  </p>
  <p><strong>
    <a href="https://easychair.org/my/conference?conf=psivt2023" target="_blank">
      <span style="border: 1px solid black; padding: 5px; margin-top: 5px; margin-bottom: 5px;">
        https://easychair.org/my/conference?conf=psivt2023
      </span>
    </a>
  </strong></p>

  <p>
    Please ensure your submission aligns with the guidelines provided on our conference website. We look forward to your contributions and an engaging, enlightening conference.
  </p>
</section>

<section id="accepted" class="mt-5">
  <h2>Accepted Papers</h2>
  <hr>
  <p>
    As we progress with preparations for the conference, we understand that financial considerations are paramount in your decision-making process, especially considering the expense of traveling to NZ. We are firmly committed to keeping the conference both affordable and accessible. All fees are in NZD, with the current exchange rate being 1 NZD = 0.6 USD.
  </p>
  <p>
    It's essential to note that each accepted paper must have at least one registration paid, whether online or in-person. While attending the conference dinner is optional, those interested can purchase a dinner ticket separately. Our primary goal is to deliver a high-quality, value-rich conference while being mindful of your budget constraints. We genuinely appreciate your interest and patience during this planning phase and are dedicated to ensuring an enriching and unforgettable experience at PSIVT 2023.
  </p>
  <a href="https://www.eventbrite.co.nz/e/pacific-rim-symposium-on-image-and-video-technology-psivt-2023-tickets-697223663747?aff=oddtdtcreator" target="_blank" class="btn btn-primary">Register Now</a>
</section>

<section id="registration" class="mt-5">
  <h2>Registration</h2>
  <hr>
  <p>
    Recognizing the necessity for clear and comprehensive financial information in your conference planning, we have updated the registration section for PSIVT 2023 to better suit your needs. A detailed fee table has been provided for your convenience, featuring both NZD and approximate USD values, calculated at the current exchange rate of 1 NZD = 0.59 USD (as of 22/09/2023).
  </p>
  <table class="table table-bordered">
    <thead>
      <tr>
        <th>Type</th>
        <th>Price (NZ$)</th>
        <th>Approx. Price (USD)</th>
        <th>Fees (NZ$)</th>
        <th>Sales End Date</th>
        <th>Inclusions</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Early Bird Registration</td>
        <td>500.00</td>
        <td>295.00</td>
        <td>39.26</td>
        <td>Oct 8, 2023</td>
        <td>Access to all sessions, workshops, and keynotes. Conference Dinner included.</td>
      </tr>
      <tr>
        <td>General Registration</td>
        <td>600.00</td>
        <td>354.00</td>
        <td>46.99</td>
        <td>Nov 22, 2023</td>
        <td>Access to all sessions, workshops, and keynotes. Conference Dinner included.</td>
      </tr>
      <tr>
        <td>Student Rate Registration</td>
        <td>450.00</td>
        <td>265.50</td>
        <td>35.41</td>
        <td>Nov 22, 2023</td>
        <td>Access to all sessions, workshops, and keynotes. Must provide valid Student ID. Conference Dinner included.</td>
      </tr>
      <tr>
        <td>Online Attendance</td>
        <td>250.00</td>
        <td>147.50</td>
        <td>19.97</td>
        <td>Nov 22, 2023</td>
        <td>Virtual access to all sessions, workshops, and keynotes. Digital conference materials and proceedings. Interactive Q&amp;A and networking opportunities in the virtual platform.</td>
      </tr>
      <tr>
        <td>Extra Conference Dinner Ticket</td>
        <td>100.00</td>
        <td>59.00</td>
        <td>8.37</td>
        <td>Nov 22, 2023</td>
        <td>Extra dinner ticket for networking opportunity with speakers, attendees, and organizers.</td>
      </tr>
    </tbody>
  </table>
  <p>
    Please note that each accepted paper must have at least one registration paid, whether online or in-person. We are dedicated to ensuring a rich and comprehensive conference experience and appreciate your understanding and cooperation in this matter.
  </p>
  <a href="https://www.eventbrite.co.nz/e/pacific-rim-symposium-on-image-and-video-technology-psivt-2023-tickets-697223663747?aff=oddtdtcreator" target="_blank" class="btn btn-primary">Register Now</a>
</section>

<section id="final" class="mt-5">
  <h2>Final Submission Requirements</h2>
  <hr>
  <p>
    To proceed with the publication of your paper in the PSIVT 2023 Lecture Notes in Computer Science (LNCS) proceedings by Springer, you must submit both your final paper and the corresponding copyright form.
  </p>
  <p>
    Follow the steps below for a smooth submission process:
  </p>
  <ol>
    <li>
      Download the Copyright Form Template: Navigate to the following <a href="https://autuni-my.sharepoint.com/:f:/g/personal/mnguyen_aut_ac_nz/Et-DplC6npFFgs9aG6zJFToB3PD3uT8rScV8TqbvyE5cNg?e=IurpHg">SharePoint Link</a> and download the copyright form template located at <a href="./Copy_right_template.docx">"./Copy_right_template.docx"</a>.
    </li>
    <li>
      Complete the Copyright Form: Fill out the required fields in the downloaded form.
    </li>
    <li>
      Prepare the Source File: Archive your source files into a ZIP file, naming it "source.zip."
    </li>
    <li>
      Upload Files: Return to the <a href="https://autuni-my.sharepoint.com/:f:/g/personal/mnguyen_aut_ac_nz/Et-DplC6npFFgs9aG6zJFToB3PD3uT8rScV8TqbvyE5cNg?e=IurpHg">SharePoint Link</a> and upload the completed copyright form along with the source ZIP file to the folder corresponding to your paper number.
    </li>
    <li>
      Additional Materials: Please also upload your PowerPoint slides and a 10-15 minute video presentation for the conference. These will serve as backup materials.
    </li>
  </ol>
  <p>
    Failure to complete these steps may result in delays or non-publication of your paper. If you encounter any issues, please do not hesitate to reach out.
  </p>
</section>



<section id="important-dates" class="mt-5">
    <h2>Important Dates</h2>
    <hr>
    <ul>
      <li>Submission Deadline: <span style="text-decoration: line-through; color: red;">July 31, 2023</span> <span style="color: green; font-weight: bold;">August 21, 2023</span></li>
      <li>Notification of Acceptance: <span style="text-decoration: line-through; color: red;">September 15, 2023</span> <span style="color: green; font-weight: bold;">September 22, 2023</span></li>
      <li>Camera-Ready Submission: October 8, 2023</li>
      <li>Early-bird Registration Date: October 8, 2023</li>
      <li>Latest Registration Date: November 1, 2023</li>
      <li>Conference Dates: November 22-24, 2023</li>
    </ul>
</section>

<section id="program" class="mt-5">
  <h2>PSIVT 2023 Conference Programme, location: <a href="https://maps.app.goo.gl/vJtZdiwpcCB4D1Ah7" target="_blank">AUT WZ building level 11</a></h2>


  <hr>
  <div class="row mb-4">
        <div class="col">
    <h3>Day 1: Wednesday, 22nd November 2023</h3>
    <ul class="list-group">
        <li class="list-group-item">08:00 - 08:45: Registration &amp; Welcome Tea/Coffee - Offering a selection of teas, freshly brewed coffee, and chilled juice for guests.</li>

        <li class="list-group-item">09:00 - 10:00: Keynote Speaker 1 (60 mins, Session Chair: Wei Qi Yan): <a href="#Domingo">Professor Domingo Mery (Universidad Católica de Chile)</a></li>
        <li class="list-group-item">10:00 - 10:30: Morning Tea - Offering freshly brewed coffee, assorted teas, chilled juice, gourmet finger sandwiches, sliced fruit, and filled profiteroles.</li>

        <li class="list-group-item">10:30 - 12:00: Conference Session 1 (90mins, 6 presentations, The State-of-the-Art Image and Video Technology, Session Chair: Minh Nguyen)
            <ul>
    <li>Paper #9: "Evaluating Mammogram Image Classification: Impact of Model Architectures, Pretraining, and Finetuning" by Kaier Wang, Aristarkh Tikhonov, Melissa Hill, and Lester Litchfield.</li>
    <li>Paper #19: "Cluster-Based Video Summarization with Temporal Context Awareness" by Hai-Dang Huynh-Lam, Ngoc-Phuong Ho-Thi, Minh-Triet Tran, and Trung-Nghia Le.</li>
    <li>Paper #20: "Image Recognition and Threat Detection in Bags Arriving at the Airport" by Ivan Koptev, Andreas Kempa-Liehr, and Cameron Walker.</li>
    <li>Paper #8: "AUAAC: Area Under Accuracy-Accuracy Curve for Evaluating Out-of-Distribution Detection" by Wonjik Kim, Masayuki Tanaka, and Masatoshi Okutomi.</li>
    <li>Paper #39: "Local Brightness Normalization for Image Classification and Object Detection Robust to Illumination Changes" by Yanshuo Lu, Masayuki Tanaka, Rei Kawakami, and Masatoshi Okutomi.</li>
    <li>Paper #50: "Computational Analysis of Table Tennis Matches from Real-Time Videos Using Deep Learning" by Hong Zhou.</li>
</ul>

        </li>
        <li class="list-group-item">12:00 - 01:00: Lunch - Featuring a selection of teas, freshly brewed coffee, chilled juice, Asian Platter, Sushi Platter, Fruit Platter, and Indian Platter with a variety of international cuisine options.</li>

        <li class="list-group-item">01:00 - 02:00: Conference Session 2 (60 mins, 4 presentations, Deep Learning for Computer Vision, Session Chair: Parma Nand)
            <ul>
    <li>Paper #13: "Multiscale Kiwifruit Detection from Digital Images" by Yi Xia, Minh Nguyen, Raymond Lutui, and Wei Qi Yan.</li>
    <li>Paper #15: "A High-Accuracy Deformable Model for Human Face Mask Detection" by Xinyi Gao.</li>
    <li>Paper #18: "MobileNet-SA: Lightweight CNN with Self Attention for Sketch Classification" by Viet-Tham Huynh, Trong-Thuan Nguyen, Tam V. Nguyen, and Minh-Triet Tran.</li>
    <li>Paper #28: "Enhancement of Human Face Mask Detection Performance by Using Ensemble Learning Models" by Xinyi Gao.</li>
	<li>Paper #25: "Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal Embedding" by Tuan-Luc Huynh, Thanh-Danh Le, Van-Tam Nguyen, Trung-Nghia Le, and Minh-Triet Tran.</li>
</ul>

        </li>
        <li class="list-group-item">02:00 - 03:00: Keynote Speaker 2 (60 mins, Session Chair: Wei Qi Yan): <a href="#mohan">Professor Mohan Kankanhalli (National University of Singapore)</a></li>
        <li class="list-group-item">03:00 - 03:30: Afternoon Tea - Offering freshly brewed coffee, assorted teas, chilled juice, freshly baked cookies, and corn fritters with smoked tomato relish.</li>

        <li class="list-group-item">03:30 - 04:30: <a href="https://www.google.com/maps/dir/WZ+Building+5067706,+AUT+WZ+Building,+WZ+Building,+Auckland+1010/Wintergarden+Cafe+-+New+Zealand+Kiosk+Road,+Parnell,+Auckland/@-36.8559064,174.7654793,16z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x6d0d476e123fbe89:0x6246961f609d7d6d!2m2!1d174.7673633!2d-36.8541547!1m5!1m1!1s0x6d0d4872d9d8a513:0xe58161d82be9045e!2m2!1d174.773823!2d-36.859639!3e2?entry=ttu" target="_blank">Auckland Domain Walk tour</a> (Optional).
		If you have more time, <a href="https://static1.squarespace.com/static/608a1a448a9ee166fa74a901/t/609af840deb9716808eb39b6/1620768834465/The+Village+Square+-+Heritage+Walk+Podcast+Map+-+Auckland+Domain.pdf" target="_blank">go for this.</a></li>
        <li class="list-group-item">04:30 - Late: Welcome Reception - Featuring Villa Maria wines, an assortment of beers, Bundaberg range, soft drinks, The Bar platter, The Cheese Platter, and The Vegan platter.</li>

		

    </ul>
</div>

    </div>

    <div class="row mb-4">
    <div class="col">
        <h3>Day 2: Thursday, 23rd November 2023</h3>
        <ul class="list-group">
            <li class="list-group-item">08:30 - 09:00: Morning Coffee - Offering a selection of teas, freshly brewed coffee, and chilled juice for guests.</li>
            <li class="list-group-item">09:00 - 10:00: Keynote Speaker 3 (Session Chair: Wei Qi Yan): <a href="#richard">Professor Richard Green (University of Canterbury)</a></li>
            <li class="list-group-item">10:00 - 10:30: Morning Tea - Offering a selection of teas, freshly brewed coffee, chilled juice, gourmet finger sandwiches, freshly sliced fruit, and cinnamon Danish scrolls with lemon icing.</li>

            <li class="list-group-item">10:30 - 12:00: Conference Session 3 (90 mins, 6 presentations, StereoVision and Depth Estimation, Session Chair: Gisela Klette):
                <ul>
    <li>Paper #38: "Spatial Variation Sequences for Remote Sensing Applications with Small Sample Size" by Hayden Jeune, Niklas Pechan, Sharn-Konet Reitsma, and Andreas W. Kempa-Liehr.</li>
    <li>Paper #48: "Exploring the Potential of High-Resolution Drone Imagery for Improved 3D Human Avatar Reconstruction" by Ali Salim, Marwa Jabberi, Tarek Hamdani, and Adel Alimi.</li>
    <li>Paper #55: "Point Cloud Novelty Detection Based on Latent Representations of a General Feature Extractor" by Shizuka Akahori, Satoshi Iizuka, Ken Mawatari, and Kazuhiro Fukui.</li>
    <li>Paper #63: "Efficient 3DConv Fusion of RGB and Optical Flow for Dynamic Hand Gesture Recognition and Localization" by Gibran Benitez-Garcia and Hiroki Takahashi.</li>
    <li>Paper #52: "An Analysis of Students Perception and their Actual Interaction with their Instructor(s) on the Moodle Page of a Pre-degree English Course Taught in Blended Mode" by Komal Karishma and Krishna Raghuwaiya.</li>
    <li>Paper #60: "An Investigation of Video Vision Transformers for Depression Severity Estimation from Facial Video Data" by Ghazal Bargshady and Roland Goecke.</li>
</ul>

            </li>
            <li class="list-group-item">12:00 - 01:00: Lunch - Offering a selection of teas, freshly brewed coffee, chilled juice, The Italian Platter with cured meats and grilled vegetables, a smoked chicken and pasta salad, and The Pizza Box Platter featuring tandoori chicken and smoked mushroom pizzas.</li>

            <li class="list-group-item">01:00 - 02:00: Conference Session 4 (60 mins, 4 presentations, Visual Object Detection, Session Chair: Jack Li):
                <ul>
    <li>Paper #35: "Real-time Automated Body Condition Scoring of Dairy Cows" by Jia-Hong Lai, Fay Huang, Yi-Hsin Yeh, Kuo-Hua Lee, Kuo-Kai Cheng, and Chao-Chien Chen.</li>
    <li>Paper #40: "Logo-SSL: Self-Supervised Learning with Self-Attention for Efficient Logo Detection" by Yilin Li, Junke Xu, and Alireza Dehghani.</li>
    <li>Paper #68: "HAHANet: Towards Accurate Image Classifiers with Less Parameters" by Arren Matthew Antioquia and Macario Ii Cordel.</li>
    <li>Paper #69: "Melanoma Classification Using Deep Learning" by Yehia Mousa, Radwa Taha, Shereen Afifi, and Ranpreet Kaur.</li>
</ul>

            </li>
            <li class="list-group-item">02:00 - 03:00: Keynote Speaker 4 (Session Chair: Wei Qi Yan): <a href="#huang">Professor Tiejun Huang (Peking University China)</a></li>
            <li class="list-group-item">03:00 - 03:30: Afternoon Tea - Offering a selection of teas, freshly brewed coffee, chilled juice, gourmet mini sausage rolls with house-made tomato sauce, and freshly baked mini sweet muffins.</li>

			<li class="list-group-item">03:30 - 05:00: Conference Session 5 (60 mins, 6 presentations, Online Session, Session Chair: Wenjie Zhu):
                <ul>
    <li>Paper #1: "3D Formation Control of Multiple Cooperating Autonomous Agents Via Leader-Follower Strategy" by Roneel Chand.</li>
    <li>Paper #7: "LAPRNet: Lightweight Airborne Particle Removal Network for LiDAR Point Clouds" by Yanqi Ma, Ziyu Yue, Youwei Wang, Zhixun Su, Risheng Liu, and Junjie Cao.</li>
    <li>Paper #31: "REAL-NET: A Monochromatic Depth Estimation Using REgional Attention and Local feature mapping" by Harsh Bhandari and Sarbani Palit.</li>
    <li>Paper #49: "Spike-EFI: Spiking Neural Network for Event-based Video Frame Interpolation" by Dongsheng Wu and De Ma.</li>
    <li>Paper #54: "Harmonizing Dynamics: Unveiling Lyapunov-Based Control Schemes for Complex Systems" by Jai Raj, Krishna Raghuwaiya, Bibhya Sharma, and Jito Vanualailai.</li>
    <li>Paper #67: "Expanding Aerial Possibilities: Autonomous Landing of a UAV on a Mobile Platform" by Darmesh Kumar, Jai Raj, Krishna Raghuwaiya, and Jito Vanualailai.</li>
</ul>

            </li>
            <li class="list-group-item">06:30 - 09:00: Conference Dinner at <a href="https://www.google.com/maps/dir/?api=1&amp;destination=Palm+Cafe,+2%2F5+Graham+Street,+Auckland+Central" target="_blank">Palm Vietnamese Cuisine</a></li>
			<li class="list-group-item">Late: Auckland By Night Walk to <a href="viaductWalk.png" target="_blank">Viaduct harbour/Wynyard Quarter</a> (Optional - good for photo shooting)</li>
        </ul>
    </div>
</div>


<div class="row mb-4">
    <div class="col">
        <h3>Day 3: Friday, 24th November 2023</h3>
        <ul class="list-group">
            <li class="list-group-item">08:30 - 09:00: Morning Coffee - Offering a selection of teas, freshly brewed coffee, and chilled juice for guests.</li>
            <li class="list-group-item">09:00 - 10:30: Conference Session 6 (90 mins, 6 presentations, Privacy, Safety, and Surveillance, Session Chair: Raymond Lutui):
                <ul>
    <li>Paper #5: "ScrambleMix: A Privacy-Preserving Image Processing for Edge-Cloud Machine Learning" by Koki Madono, Masayuki Tanaka, and Masaki Onishi.</li>
    <li>Paper #51: "A Comparative Analysis of Blended Mode Students Interaction on Moodle with their End of Semester Result in a Pre-degree English Course" by Komal Karishma and Krishna Raghuwaiya.</li>
    <li>Paper #33: "Comparison of Simplified SE-ResNet and SE-DenseNet for Micro-Expression Classification" by Xiangbo Chen, Masahi Nishiyama, and Yoshio Iwai.</li>
    <li>Paper #42: "Facial Deepfake Detection using Gaussian Processes" by Uzoamaka Ezeakunne and Xiuwen Liu.</li>
    <li>Paper #66: "A Novel Steganography Scheme using Logistic Map, BRISK Descriptor, and k-means Clustering" by Hassan Jameel Azooz, Khawla Ben Salah, Monji Kherallah, and Mohamed Saber Naceur.</li>
    <li>Paper #61: "A Holistic Approach to Elderly Safety: Sensor Fusion, Fall Detection, and Privacy-Preserving Techniques" by Hoa Nguyen, Thu Giang Mai, and Minh Nguyen.</li>
</ul>

            </li>
            <li class="list-group-item">10:30 - 11:00: Morning Tea - Offering a selection of teas, freshly brewed coffee, chilled juice, gourmet finger sandwiches, freshly sliced fruit, and chia seed pudding.</li>

            <li class="list-group-item">11:00 - 12:30: Conference Session 7 (90 mins, 6 presentations, AI for Medical Imaging, Session Chair: Sam Madanian):
                <ul>

    <li>Paper #43: "On Deploying Mobile Deep Learning to Segment COVID-19 RT-PCR Test Tube Images" by Ting Xiang, Richard Dean, Jiawei Zhao, and Ninh Pham.</li>
    <li>Paper #53: "Enhancing Safety during Surgical Procedures with Computer Vision, Artificial Intelligence, and Natural Language Processing" by Okeke Stephen and Minh Nguyen.</li>
    <li>Paper #62: "Deep Learning Model with Atrous Convolutions for Improving Skin Cancer Classification" by Ranpreet Kaur and Hamid Gholamhosseini.</li>
    <li>Paper #11: "Fusion-based Approach to Enhance Markerless Motion Capture Efficiency for on-site Analysis: Feasibility and Accuracy" by Abderrahman Ben Abdeljelil, Mohamed Hédi Bedoui, and Khalil Ben Mansour.</li>
    <li>Paper #47: "Using an optimal then enhanced YOLO Model for Multi-lingual scene text detection containing Arabic scripts" by Houssem Turki, Mohamed Elleuch, and Monji Kherallah.</li>
</ul>

            </li>
            <li class="list-group-item">12:30 - 01:30: Lunch - Offering a selection of teas, freshly brewed coffee, chilled juice, The Asian Platter with dumplings, teriyaki salmon bites, and more, The Sushi Platter, and a masala spiced lentil salad with minted cucumber and other fresh ingredients.</li>
            <li class="list-group-item">04:00 - Late: Survival Party with all the school's staff</li>
        </ul>
    </div>
</div>

</section>

<section id="awards" class="mt-5">
  <h2>Awards</h2>
  <hr>
  <!--
  <p>
    In honor of Professor Reinhard Klette, the founding Director of our research center, we will be presenting a special award called the "Reinhard Klette Best Paper Awards." This award aims to memorialize his significant contributions to the field of image and video technology, as well as his dedication to the research community.
  </p>
  -->
  <p>
    Professor Reinhard Klette (1950-2020) was a distinguished academic who achieved numerous accolades throughout his illustrious career. He was a Fellow of the Royal Society of New Zealand (RSNZ), a Helmholtz International Fellow (Germany), a Friendship Ambassador of Shandong province (Shandong, China), and a winner of the Quancheng Friendship Award (Jinan, China). To learn more about Professor Klette and his work, you can visit his <a href="https://research.com/u/reinhard-klette" target="_blank">profile on Research.com</a>.
  </p>
  <p>
    The "Reinhard Klette Best Paper Awards" will recognize outstanding research papers presented at the conference, continuing Professor Klette's legacy of promoting excellence and innovation in the field.
  </p>
</section>


<section id="keynote-speakers" class="mt-5">
<h2>Keynote Speakers:</h2>
  <hr>
  <div class="speaker" id="huang">
    <div class="profile-picture">
        <img src="https://cfcs.pku.edu.cn/english/images/content/2020-07/20200707110530230406.png" alt="Professor Huang Tiejun">
    </div>
    <div class="speaker-info">
	    <h4>Professor Huang Tiejun (Peking University)</h4>
    <ul>
		<li>Director, National Key Laboratory of Multimedia Information Processing</li>
		<li>Professor, School of Computer Science</li>
    </ul>
    <p><b>Email:</b> <a href="mailto:tjhuang@pku.edu.cn">tjhuang@pku.edu.cn</a><br>
	<b>Website:</b> <a href="http://eecs.pku.edu.cn/info/1011/1230.htm">http://eecs.pku.edu.cn/info/1011/1230.htm</a><br>
	<b>LinkedIn:</b> <a href="https://www.linkedin.com/in/huang-tiejun-486872a3/">https://www.linkedin.com/in/huang-tiejun-486872a3</a>  <br>
	<b>Research Interests:</b> Professor Huang Tiejun is a leader in visual information processing and brain-like intelligence. He is the inventor of the Spiking Continuous Photography principle and the creator of ultra-high-speed visual sensor chips, camera, and system.
He has received several prestigious awards for his work, including the Second Prize of the National Technology Invention Award in 2017 for his efficient visual feature analysis and compression key technologies, the Second Prize of the National Science and Technology Progress Award in 2012 for his contribution to the development and industrial application of national video coding standards. Recently, in 2022, he received the Outstanding Contribution Award for Chinese Standard Innovation and the Outstanding Contribution Award for the Wu Wenjun Artificial Intelligence Science and Technology Award.
<br>He has also received several distinctions such as being a National Distinguished Young Scholar, a Changjiang Scholar, and a Fellow of the CAAI (Chinese Association for Artificial Intelligence), CCF (China Computer Federation), CSIG (China Society of Image and Graphics), and CIE (Chinese Institute of Electronics).
  <br>
<b>Speech Abstract:</b>  Video in the form of image sequences, which lose the temporal process of the photons flow, leads to an irreconcilable dilemma between high-dynamic and high-speed imaging. Based on the fact that the pixels of the photoelectric sensor are indepen⁃ dent, a new continuous photographing principle is
 invented: for each pixel, accumulate charge from the reset state, generate a spike as a flag once the specified threshold is reached, reset and repeat. The duration that a spike takes to be fired is called its spiking width, which is inversely proportional to the light intensity during this period.
Based on this, the light intensity during this period can be estimated. The array of the spike sequences of all pixels is so-called “viform”, which contains rich spatial and temporal information of the light process. A n image at any moment can be calculated from viform, thereby ultrahigh-speed, high-dynamic and non-blurred continuous im⁃ aging achieved.Employing mature CMOS photonic devices and standard processes, two spike continuous photographing chips and spike cameras with spatial resolutions of 0.1 million and 1 million pixels have been developed, respectively.
Viform will fundamentally reshapes computer vision and visual information processing technology and industries. An open source algorithm framework, SpikeCV, achieves high-speed target detection, tracking, and recognition system that is faster than the human eye by thousands of times, with low computational complexity.
</p>
    </div>
</div>
<hr>





<h2>Keynote Speakers:</h2>
  <hr>
  <div class="speaker" id="mohan">
    <div class="profile-picture">
        <img src="https://www.comp.nus.edu.sg/~mohan/IMG1C_20_5_2013.jpg" alt="Professor Mohan Kankanhalli">
    </div>
    <div class="speaker-info">

        <h4>Mohan Kankanhalli, School of Computing (National University of Singapore)</h4>
        <ul>
			<li>Deputy Executive Chairman, AI Singapore</li>
			<li>Provost's Chair Professor of Computer Science</li>
        </ul>
        <p><b>Email:</b> <a href="mailto:mohan@comp.nus.edu.sg">mohan@comp.nus.edu.sg</a><br>
		<b>Website:</b> <a href="https://www.comp.nus.edu.sg/~mohan/">https://www.comp.nus.edu.sg/~mohan</a><br>
		<b>LinkedIn:</b> <a href="https://www.linkedin.com/in/mohan-kankanhalli-583417221/">https://www.linkedin.com/in/mohan-kankanhalli-583417221</a>  <br>
		<b>Research Interests:</b> Mohan Kankanhalli is Provost's Chair Professor of Computer Science at the National University of Singapore (NUS). He is deeply interested in the intersection of technology, society, and digital trust. His primary areas of research encompass Multimedia Computing, Computer Vision, and Information Security &amp; Privacy. He has made fundamental contributions in the area of multimedia &amp; vision - image and video understanding, multimodal fusion, visual saliency as well as in multimedia security - content authentication, privacy and trustworthy AI. As Director of the NUS Center for Research in Privacy Technologies (N-CRiPT), he leads initiatives focusing on end-to-end privacy research for both structured and unstructured data, encompassing various fields such as multimedia privacy, privacy in machine learning, and federated approaches to privacy. Furthermore, as Deputy Executive Chairman of AI Singapore, he is involved in the management and development of Singapore's National AI Program, contributing to fundamental research, tackling technology grand challenges, and enhancing industry research. Lastly, he is engaged in leadership roles in multimedia computing such as being the Senior Editor of ACM Transactions on Multimedia Computing journal and the Associate Editor-in-Chief of IEEE Multimedia magazine.
<br>
Mohan is a member of World Economic Forum's Global Future Council on the Future of Artificial Intelligence. He is an IEEE Fellow.

<br><b>Speech Abstract:</b> Machine Unlearning. <br>Big data has been one of the important enablers of the recent advances in Artificial Intelligence. Its use for training machine learning models has enabled many useful applications.  However, there are also some downsides when the data contains sensitive, often personal, information. Recent privacy and data protection regulations across the world have recognized the “right to be forgotten” as a fundamental right of citizens. Enabling this in practice not only requires deletion of data from institutional databases but also necessitates deletion of data from trained machine learning models. Machine Unlearning refers to the process of selectively removing or forgetting some specific data or a class of data from a machine learning (often deep learning) model. This talk will introduce the emerging field of machine unlearning, discuss the motivations behind it, the techniques employed, and its implications for data privacy, fairness, and interpretability. We will present several approaches for unlearning, including the selective modification of model parameters, regularization techniques, data-free methods, and teacher-student frameworks. We will then discuss the challenges and open research questions in the field. Machine Unlearning can thus be seen to contribute towards building more accountable AI systems that can engender trust among users and stakeholders. </p>
    </div>
</div>
<hr>


<div class="speaker" id="Domingo">

    <div class="profile-picture">
        <!-- Add your profile picture link here -->
        <img src="https://portales.inacap.cl/Assets/eventos/img/2019/imasd/Domingo%20Mery.jpg" alt="Professor Domingo Mery">
    </div>
    <div class="speaker-info">
        <h4>Professor Domingo Mery (Universidad Católica de Chile)</h4>
        <ul>
            <li>Full Professor, Department of Computer Science</li>
            <li>Chair of the Computer Science Department (2005-2009)</li>
        </ul>
        <p><b>Email:</b> <a href="mailto:domingo.mery@uc.cl">domingo.mery@uc.cl</a><br>
        <b>Website:</b> <a href="https://domingomery.ing.puc.cl/">https://domingomery.ing.puc.cl</a><br>
        <b>LinkedIn:</b> <a href="https://cl.linkedin.com/in/domingomery">https://cl.linkedin.com/in/domingomery</a><br> <br>
        <b>Research Interests:</b> Professor Domingo Mery's research interests encompass X-ray testing, biometrics, machine vision, computer vision, image processing, pattern recognition, and food engineering. His significant contributions include the development of image processing for fault detection in aluminum castings, X-ray imaging, real-time programming, and computer vision. He also developed and provides useful resources for the research community, including a database of X-rays and various toolboxes for image processing, pattern recognition, and computer vision. In addition to his research, Professor Mery is known for his excellence in teaching, for which he received an award from the Universidad Católica de Chile in 2020. He has made a notable impact in the field, publishing over 60 technical SCI publications and more than 70 conference papers. He has also held prestigious positions such as Chair of the Computer Science Department at UC, Chile, Associate Visiting Professor at the Computer Vision Research Lab of the University of Notre Dame, and Director of Research and Innovation of the School of Engineering at UC.
        Professor Mery supervises a number of graduate and under-graduate students, nurturing the next generation of computer scientists. He has held key roles in international conferences such as the Pacific-Rim Symposium on Image and Video Technology (PSIVT) and the Iberoamerican Congress on Pattern Recognition. He was a recipient of multiple awards including the Ron Halmshaw Award for publishing the best paper on industrial radiography in the journal Insight. His work on X-ray imaging has made significant contributions to the field of non-destructive testing.
		<br><b>Speech Abstract:</b> Tracing the Evolution of Face Recognition Technology.<br>Face recognition is one of computer vision's most active and successful areas, with various applications, including security, surveillance, and social media. In recent years, there has been significant progress in the development of face recognition algorithms, which can now achieve high accuracy even on challenging datasets. However, several challenges still need to be addressed before face recognition can be widely deployed in real-world applications. This keynote will discuss the state of the art in face recognition, focusing on facial analysis and the challenges that remain. In the first part of the talk, we will review recent advances in face detection, facial attribute recognition, and other facial analysis tasks. We will also discuss the impact of these advances on applications such as security and surveillance. In the second part of the talk, we will discuss the challenges that remain in face recognition. These challenges include face recognition in low-quality images, explainability, fairness, social relationships, and ethics. We will discuss the current state of research on these challenges and propose potential directions for future work.
.</p>
	</div>
</div>
  <hr>
  <div class="speaker" id="richard">
    <div class="profile-picture">
        <img src="https://www.canterbury.ac.nz/engineering/contact-us/people/1658110294997_Richard-Green-low.jpg" alt="Professor Richard Green">
    </div>
    <div class="speaker-info">

        <h4>Professor Richard Green (University of Canterbury)</h4>
        <ul>
            <li>Doctor of Philosophy (University of Sydney)</li>
            <li>IEEE (Professional Organisation) Member</li>
        </ul>
        <p><b>Email:</b> <a href="mailto:richard.green@canterbury.ac.nz">richard.green@canterbury.ac.nz</a><br>
        <b>Website:</b> <a href="https://www.canterbury.ac.nz/engineering/contact-us/people/richard-green.html">https://www.canterbury.ac.nz/engineering/contact-us/people/richard-green.html</a><br>
        <b>LinkedIn:</b> <a href="https://www.linkedin.com/in/richardgreen">https://www.linkedin.com/in/richardgreen</a>  <br>
		<b>Research Interests:</b> Professor Richard Green's research interests span across artificial intelligence, computer vision, machine intelligence, human-computer interaction, cognitive science, education, and biomechanics, with a robust cross-disciplinary approach which integrates fields such as agriculture, aquaculture, forestry studies, computer graphics and image processing, high performance computing (HPC), virtual reality, biosecurity/biosafety, marine ecology, and computer &amp; software engineering. Notably, he has pioneered significant advancements in computer vision, including the development of an improved semi-synthetic approach for creating visual-inertial odometry datasets, research on outlier detection for visual odometry in vegetated scenes, detection and tracking of Pinus Radiata Catkins, and automated apple fruitlet thinning. His contributions extend to exploring applications and innovations in artificial intelligence in New Zealand, reflecting his commitment to technological advancement and societal integration of AI technologies. Additionally, his work in vision-based automated pruning and autonomous forest pruning have added significantly to the field of machine intelligence. Concurrently, his research on mobility models using stochastic differential equations showcases his diverse interests in AI. Committed to academia, Professor Green is actively supervising multiple PhD and Masters students, working on a wide array of research projects. He also holds an editorial role at the IEEE Transactions on Circuits and Systems for Video Technology, reinforcing his dedication to the dissemination of knowledge. Ultimately, Professor Green's work signifies his dedication to leveraging the power of AI and machine intelligence to address real-world challenges, while propelling the fundamental understanding of these fields and their applications.
    <br><b>Speech Abstract:</b> See the unseen – we cannot automate what we cannot see in agriculture and aquaculture.
	<br>The recent explosion of AI/ML/Vision algorithm accuracy, efficiency and low processing cost has suddenly enabled applications undreamt of even five years ago – especially for challenging outdoor agritech applications. Our research is interested in these real-world applications, such as rapid data reduction of petabytes of data from scanning a farm (such as orchards or vineyards) from sub-mm under-canopy/underwater proximal sensing. We cannot automate what we cannot see – so our recent breakthroughs with NeRF and Gaussian splatting is helping to solve leaf occlusion to enable a rapid uptake of agricultural automation. I will describe our contributions across these research areas, including recent autonomous systems research into drones pruning forests, robots pruning vineyards, autonomous underwater vehicles (AUVs) inspecting mussel lines to detect invasive biofouling species and AUVs mapping the seabed to locate scallops. We will also discuss the challenges that remain and so propose potential directions for future work.
	</p>
	</div>
</div>
<hr>
<style>
    .speaker {
        display: flex;
        align-items: center;
        gap: 20px;
    }

    .profile-picture img {
        border-radius: 50%;
        width: 150px;
        height: 150px;
        object-fit: cover;
    }

    .speaker-info {
        font-family: Arial, sans-serif;
    }
</style>

</section>

<section id="conference-dinner" class="mt-5">
  <h2>Conference Dinner</h2>
  <hr>
  <p>
    The conference dinner will be held at <a href="https://palmcafe.nz/" target="_blank">Palm Cafe</a>, a delightful Vietnamese restaurant located at 2/5 Graham Street, Auckland Central.
	Attendees will be treated to a delicious selection of 5-7 options from the rich Vietnamese cuisine, known for its fresh ingredients and vibrant flavors.
	<br>
  <img src="palmcafe_col.jpeg" alt="Palm Cafe" style="width: 450px; height: 450px; margin-top: 20px;"><br>
  <br><a href="https://www.google.com/maps/dir/?api=1&amp;destination=Palm+Cafe,+2%2F5+Graham+Street,+Auckland+Central" target="_blank">Get Directions to Palm Cafe</a><br>
  <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3192.805378349092!2d174.75687017658714!3d-36.84713687941654!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x6d0d47f1a632f677%3A0x4459cb96e437b900!2sPalm%20Vietnamese%20Cuisine!5e0!3m2!1sen!2snz!4v1699388122748!5m2!1sen!2snz" width="450" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
  </p>
  <p>
    Along with the exquisite food, guests can enjoy unlimited beers, wines, juices, and coffee, providing a perfect opportunity for networking and socializing with fellow conference participants. To top off the evening, don't forget to try the authentic Vietnamese coffee, a truly unique and unforgettable experience.
  </p>
  <div>

  </div>
</section>

	  <section id="accommodation" class="mt-5">
  <h2>Accommodation</h2>
  <hr>
  <ul>
    <li>
  <a href="https://www.ihg.com/holidayinn/hotels/us/en/auckland/aklms/hoteldetail" target="_blank">
    Holiday Inn Express Auckland City
  </a>
  - Price Range: NZD 150-230 per night
</li>
<li>
  <a href="https://www.questapartments.co.nz/properties/north-island/auckland/quest-on-queen/overview" target="_blank">
    Quest on Queen
  </a>
  - Price Range: NZD 180-230 per night
</li>
<li>
  <a href="https://www.ramada.nz/hotel/ramada-suites-by-wyndham-auckland-federal-street/" target="_blank">
    Ramada Suites by Wyndham Auckland Federal Street
  </a>
  - Price Range: NZD 150-220 per night
</li>
<li>
  <a href="https://www.oakshotels.com/en/oaks-auckland-harbour" target="_blank">
    Oaks Auckland Harbour
  </a>
  - Price Range: NZD 180-230 per night
</li>
<li>
  <a href="https://www.expedia.co.nz/Auckland-Hotels-Econo-Lodge-City-Central.h1364171.Hotel-Information" target="_blank">
    Econo Lodge City Central
  </a>
  - Please refer to the link for current pricing
</li>
<li>
  <a href="https://www.expedia.co.nz/Auckland-Hotels-Albion-Of-Auckland.h8076334.Hotel-Information" target="_blank">
    Albion of Auckland
  </a>
  - Please refer to the link for current pricing
</li>
<li>
  <a href="https://www.expedia.co.nz/Auckland-Hotels-President-Hotel.h575589.Hotel-Information" target="_blank">
    President Hotel
  </a>
  - Please refer to the link for current pricing
</li>

  </ul>
</section>
<!--
      <section id="sponsors" class="mt-5">
  <h2>Sponsors</h2>
  <hr>
  <ul>

	<li>School of Engineering, Computer & Mathematical Sciences: 3000.00 NZD</li>
    <li>Computer Science and Software Engineering Department: 1000.00 NZD</li>
    <li>International Association for Pattern Recognition (IAPR)</li>
    <li>Lecture Notes in Computer Science (LNCS): Submission and Publication</li>
  </ul>
  <h3>Call for Sponsors</h3>
  <p>
    Sponsorship Options: Sponsors will be acknowledged according to the category of sponsorship. Acknowledgement includes free exhibit space, organisation/company name or/and logo printed in conference materials, an entry on the conference website, as well as complimentary conference registrations(s) and more. Sponsors can support PSIVT 2023 in other formats, e.g., contribute to coffee breaks, conference bags, and USB disks, in addition to the following sponsorship levels.
  </p>
  <h4>Levels of sponsorship</h4>
  <ul>
    <li>
      <strong>Platinum (>= $5000):</strong> benefits of Gold, plus
      <ul>
        <li>Sole sponsor of first (and foremost) invited speaker</li>
        <li>1 free registration, plus 1 for every $1000 over $5000</li>
      </ul>
    </li>
    <li>
      <strong>Gold (>= $3000):</strong> benefits of Silver, plus
      <ul>
        <li>Sponsor an invited speaker (logo on slides before and after talk)</li>
        <li>Opportunity to present at the conference dinner</li>
        <li>1 free registration including full conference benefits</li>
      </ul>
    </li>
    <li>
      <strong>Silver (>= $1000):</strong>
      <ul>
        <li>Logo appears on marketing (proceedings, default slide during breaks, etc)</li>
        <li>Advertising material in satchel</li>
      </ul>
    </li>
  </ul>
  <h4>Benefits of Sponsoring</h4>
  <ul>
    <li>Showcase the State-of-the-Art Image and Video Technologies and Applications</li>
    <li>Promote R&D Advancements and Industrial Inventions</li>
    <li>Support Student Development</li>
    <li>Foster Collaborations between Academia & Industry</li>
  </ul>
</section>
-->
<section id="pc-members" class="mt-5">
        <h2>Programme Commitee Members</h2>
		<hr>
        <p>We would like to express our heartfelt gratitude to the following PC Members who have generously offered their support and expertise in reviewing the papers for this conference:</p>
        <ol>
<li>Aarij Mahmood Hussaan, PhD, Iqra University, Pakistan</li><!-- aarij hussaan <aarijhussaan@yahoo.com> -->
<li>Abdul Bais, PhD, University of Regina, Canada</li><!-- abdul.bais@uregina.ca -->
<li>Aisha Ajmal, MSC, Victoria University of Wellington, New Zealand</li><!-- Aisha Ajmal <aisha.ajmal@vuw.ac.nz> -->
<li>Akbar Ghobakhlou, PhD, AUT, New Zealand</li><!-- Akbar Ghobakhlou <akbar.ghobakhlou@aut.ac.nz> -->
<li>Ali Ahsan, PhD, Torrens University, Australia</li><!-- Ali Ahsan <ali.ahsan@Torrens.edu.au>  -->
<li>Ali Reza ALAEI, PhD, Southern Cross University, Australia</li><!-- Ali Reza Alaei <Ali.Alaei@scu.edu.au> -->
<li>Andreas W. Kempa-Liehr, Dipl.-Phys., Dr. rer. nat., The University of Auckland, New Zealand</li><!-- Andreas Kempa-Liehr <a.kempa-liehr@auckland.ac.nz> -->
<li>Atiya Masood, PhD, Iqra University, Karachi, Pakistan</li><!-- Atiya Rana <masoodatiya20@gmail.com> -->
<li>Binh P. Nguyen, PhD, Victoria University of Wellington, New Zealand</li><!-- binh.p.nguyen@vuw.ac.nz -->
<li>Boris Bacic, PhD, AUT, New Zealand</li><!-- Boris Bacic <boris.bacic@aut.ac.nz> -->
<li>Brendan McCane, PhD, Otago University, New Zealand</li><!-- Brendan McCane <brendan.mccane@otago.ac.nz> -->
<li>Burkhard Claus Wuensche, PhD, University of Auckland, New Zealand</li><!-- Burkhard Wuensche <burkhard@cs.auckland.ac.nz> -->
<li>Chiou-Shann Fuh, PhD, National Taiwan University, Taiwan</li><!-- Prof. Chiou-Shann Fuh 傅楸善 <fuh@csie.ntu.edu.tw> -->
<li>Chunhong Yoon, PhD Elec Eng, SLAC national accelerator laboratory, United States</li><!-- Yoon, Chun Hong <yoon82@slac.stanford.edu> -->
<li>Daisuke Miyazaki, PhD, Hiroshima City University, Japan</li><!-- MIYAZAKI, Daisuke <miyazaki@hiroshima-cu.ac.jp> -->
<li>Daniel Riccio, PhD, university of Naples Federico II, Italy</li><!-- daniel.riccio <daniel.riccio@unina.it> -->
<li>David Berry, BE (Mech), Dip Bus (InfoSys), Control Vision, New Zealand</li><!-- David Berry <david.berry@controlvision.co.nz> -->
<li>Dharmendra Sharma, AM, PhD, University of Canberra, Australia</li><!-- Dharmendra.Sharma <Dharmendra.Sharma@canberra.edu.au>  -->
<li>Domingo Mery, PhD, Pontificia Universidad Católica de Chile, Chile</li><!-- Domingo Mery <domingo.mery@uc.cl> -->
<li>Donald Bailey, PhD, Massey University, New Zealand</li><!-- Donald Bailey <D.G.Bailey@massey.ac.nz> -->
<li>Du Huynh, PhD, University of Western Australia, Australia</li><!-- Du Huynh <du.huynh@uwa.edu.au> -->
<li>Erik Meijering, PhD, FIEEE, University of New South Wales, Australia</li><!-- Erik Meijering <erik.meijering@unsw.edu.au> -->
<li>Fang-Lue Zhang, PhD, Organization: Victoria University of Wellington, New Zealand</li><!-- Fang Lue Zhang <fanglue.zhang@vuw.ac.nz> -->
<li>Faranak Tohidi, PhD, Charles Sturt University, Australia</li><!-- Tohidi, Faranak <ftohidi@csu.edu.au> -->
<li>Fatih Kurugollu, PhD, University of Sharjah, United Arab Emirates</li><!-- Fatih Kurugollu <fkurugollu@sharjah.ac.ae> -->
<li>Fay Huang, PhD, National Ilan University, Taiwan</li><!-- Fay Huang <fay@niu.edu.tw> -->
<li>Gisela Klette, PhD, Retired, New Zealand</li><!-- Gisela Klette <gklette@gmail.com> -->
<li>Hamid GholamHosseini, PhD, AUT, New Zealand</li><!-- Hamid GholamHosseini <hamid.gholamhosseini@aut.ac.nz> -->
<li>Harith Al-Sahaf, PhD, Victoria University of Wellington, New Zealand</li><!-- Harith Al-Sahaf <harith.al-sahaf@ecs.vuw.ac.nz> -->
<li>Harvey Ho, PhD, The University of Auckland, New Zealand</li><!-- Harvey Ho <harvey.ho@auckland.ac.nz> -->
<li>Huy Hoang Nguyen, PhD, Hanoi University of Science and Technology, Vietnam</li><!-- Nguyen Huy Hoang - Vien Dien Tu Vien Thong <hoang.nguyenhuy@hust.edu.vn> -->
<li>Héctor Allende-Cid, PhD, Pontificia Universidad Catolica  De Valparaaiso, Chile</li><!-- Héctor Allende-Cid (PhD) email: hector.allende@pucv.cl -->
<li>Ibrahim Rahman, PhD, Open Polytechnic, New Zealand</li><!-- Ibrahim Rahman <Ibrahim.Rahman@openpolytechnic.ac.nz> -->
<li>Jaco Fourie, PhD, Machine Vision Senior Scientist and Team Leader, Lincoln Agritech Ltd, New Zealand</li><!-- Jaco Fourie <Jaco.Fourie@LincolnAgritech.co.nz> -->
<li>Jacques Blanc-Talon, PhD, DGA TA, France</li><!-- Jacques Blanc-Talon <sci.blanctalon@sfr.fr> -->
<li>Jean-Bernard Hayet, PhD, CIMAT, A.C., México</li><!-- Jean-Bernard Hayet <jbhayet@cimat.mx> -->
<li>Jeremiah Deng, PhD, University of Otago, New Zealand</li><!-- Jeremiah Deng <jeremiah.deng@otago.ac.nz> -->
<li>Jinsheng Xiao, PhD, Wuhan University, People's Republic of China</li><!-- 肖进胜 <xiaojs@whu.edu.cn> -->
<li>Jules-Raymond Tapamo, PhD, University of KwaZulu-Natal, South Africa</li><!-- Jules-Raymond Tapamo <Tapamoj@ukzn.ac.za> -->
<li>Junjie Cao, PhD, Dalian University of Technology, China</li><!-- Junjie Cao <jjcao@dlut.edu.cn> -->
<li>Kaier Wang, PhD, Volpara Health Technologies, New Zealand</li><!-- Kyle Wang, PhD <Kyle.Wang@volparahealth.com> -->
<li>Kar-Ann Toh, PhD, Yonsei University, Seoul, Korea</li><!-- katoh@yonsei.ac.kr   -->
<li>Kaushik Roy, PhD, West Bengal State University, Barasat, India</li><!-- Kaushik Roy <kaushik.mrg@gmail.com> -->
<li>Kourosh Neshatian, PhD, University of Canterbury, New Zealand</li><!-- Kourosh Neshatian <kourosh.neshatian@canterbury.ac.nz> -->
<li>Krishna Raghuwaiya, PhD, University of the South Pacific, Suva, Fiji</li><!-- krishna.raghuwaiya@usp.ac.fj -->
<li>Lee Streeter, PhD, University of Waikato, New Zealand</li><!-- Lee Streeter <lee.streeter@waikato.ac.nz> -->
<li>Li Cheng, PhD, University of Alberta, Canada</li><!-- Li Cheng <lcheng5@ualberta.ca> -->
<li>Lihong Zheng Charles, PhD, Sturt University, Australia</li><!-- Zheng, Lihong <lzheng@csu.edu.au> -->
<li>Loulin Huang, PhD, AUT, New Zealand</li><!-- Loulin Huang <loulin.huang@aut.ac.nz> -->
<li>Mahdi Setayesh, PhD, Principle Software Engineer, Microsoft, United States</li><!-- Mahdi Setayesh <m.setayesh@gmail.com> -->
<li>Manoranjan Paul, PhD, Charles Sturt University, Australia</li><!-- Paul, Manoranjan <mpaul@csu.edu.au> -->
<li>Mansoor Ebrahim, PhD, Iqra University, Pakistan</li><!-- Mansoor Ebrahim <mebrahim@iqra.edu.pk> -->
<li>Mariano Rivera Meraz, PhD, Gobierno De Mexico, México</li><!-- Mariano Rivera Meraz <mrivera@cimat.mx> -->
<li>Martin Stommel, PhD, AUT, New Zealand</li><!-- Martin Stommel <martin.stommel@aut.ac.nz> -->
<li>Michael Cree, PhD, University of Waikato, New Zealand</li><!-- Michael Cree <michael.cree@waikato.ac.nz> -->
<li>Minh Nguyen, PhD, AUT, New Zealand</li><!-- Minh Nguyen <minh.nguyen@aut.ac.nz> -->
<li>Muhammad Rafiqul Islam, PhD, Melbourne Institute of Technology, Sydney, Australia </li><!-- Islam, Muhammad Rafiqul <muislam@csu.edu.au> -->
<li>Mukesh Prasad, PhD, University of Technology Sydney, Australia</li><!-- Mukesh Prasad <Mukesh.Prasad@uts.edu.au> -->
<li>Mukku Nisanth Karthee, PhD, VIT University, India</li><!-- Nisanth Kartheek <nisanthkartheek@gmail.com> -->
<li>Parma Nand, PhD, AUT, New Zealand</li><!-- Parma Nand <parma.nand@aut.ac.nz> -->
<li>Pascal Peter, PhD, Saarland University, Germany</li><!-- peter@mia.uni-saarland.de -->
<li>Patrice Delmas, PhD, The University of Auckland, New Zealand</li><!-- Patrice Delmas <p.delmas@auckland.ac.nz> -->
<li>Peter Chong, PhD, AUT, New Zealand</li><!-- Peter Chong <peter.chong@aut.ac.nz> -->
<li>Qurrat Ul Ain, PhD, Victoria University of Wellington, New Zealand</li><!-- Qurrat Ul Ain <qurratulain3@vuw.ac.nz> -->
<li>Raghavendra Bhalerao, PhD, Institute of Infrastructure, Technology, Research and Management, Ahmedabad, India</li><!-- Raghavendra Bhalerao <raghavendra.bhalerao@iitram.ac.in> -->
<li>Ramesh Rayudu, PhD, Victoria University of Wellington, New Zealand</li><!-- rkrayudu@ecs.vuw.ac.nz -->
<li>Raymond Lutui, PhD, AUT, New Zealand</li><!-- Raymond Lutui <raymond.lutui@aut.ac.nz> -->
<li>Richard Clare, PhD, University of Canterbury, New Zealand</li><!-- Richard Clare <richard.clare@canterbury.ac.nz> -->
<li>Ryszard Kozera, PhD, Warsaw University of Life Sciences - SGGW (Institute of Information Technology), Warsaw ,Poland</li><!-- Ryszard Kozera <ryszard.kozera@gmail.com> -->
<li>Sanjoy Pratihar, PhD, Indian Institute of Information Technology Kalyani, India</li><!-- Sanjoy Pratihar <sanjoy.pratihar@gmail.com> -->
<li>Sarbani Palit, PhD, Indian Statistical Institute, India</li><!-- sarbanip <sarbanip@isical.ac.in> -->
<li>Shang-Hong Lai, PhD, National Tsing Hua University, Taiwan</li><!-- Taiwan Shang-Hong Lai <lai@cs.nthu.edu.tw> -->
<li>Shihua Zhou,  PhD,  Associate Professor, from Dalian University, China</li><!-- 周士华 <zhoushihua@dlu.edu.cn> -->
<li>Shilpa Gite, PhD, Organisation-Symbiosis Institute of Technology, Pune, India</li><!-- Shilpa Gite <shilpa.gite@sitpune.edu.in> -->
<li>Shmuel Peleg, PhD, The Hebrew University of Jerusalem, Israel</li><!-- Shmuel Peleg <peleg@mail.huji.ac.il> -->
<li>Shuhei Tarashima, M.S., NTT Communications Corporation, Japan</li><!-- Shuhei TARASHIMA <tarashima@acm.org> -->
<li>Sobhan Kanti Dhara, PhD, National Institute of Technology Rourkela, India</li><!-- Sobhan Kanti Dhara <dhara.sk@gmail.com> -->
<li>Subrata Chakraborty, PhD, University of New England, Australia</li><!-- Subrata Chakraborty <Subrata.Chakraborty@une.edu.au> -->
<li>Thanh Hai Tran, PhD, Hanoi University of Science and Technology, Vietnam</li><!-- Tran Thi Thanh Hai <thanh-hai.tran@mica.edu.vn> -->
<li>Thi-Lan Le, PhD, Hanoi University of Science and Technology, Vietnam</li><!-- Le Thi Lan <thi-lan.le@mica.edu.vn> -->
<li>Vijay John, PhD, RIKEN, Japan</li><!-- Vijay John <vijayjohn@outlook.com> -->
<li>Wang Han, PhD, Xiamen University, Malaysia</li><!-- Wang Han <han.wang@xmu.edu.my> -->
<li>Wei Qi Yan, PhD, AUT, New Zealand</li><!-- Wei Qi Yan <weiqi.yan@aut.ac.nz> -->
<li>Xinyi Gao, MSC, AUT, New Zealand</li><!-- Xinyi Gao <xinyi.gao@autuni.ac.nz> -->
<li>Xiping Fu, PhD, PredictHQ, New Zealand</li><!-- fxpfxp0607@gmail.com -->
<li>Xu Zezhong, PhD, Changzhou Institute of Technology, China.</li><!-- 徐则中 <xuzz@czu.cn> -->
<li>Xuejun Li, PhD, AUT, New Zealand</li><!-- Xuejun Li <xuejun.li@aut.ac.nz> -->
<li>Yalin Zheng, PhD, University of Liverpool, United Kingdom</li><!-- Zheng, Yalin <yzheng@liverpool.ac.uk> -->
<li>Yasushi Yagi, PhD, SANKEN, Osaka University, Japan</li><!-- 八木 康史 <yagi@am.sanken.osaka-u.ac.jp> -->
<li>Ying Bi, PhD, Victoria University of Wellington, New Zealand</li><!-- Ying Bi <yingbi@ecs.vuw.ac.nz> -->
<li>Yuanyuanv(Derek) Zhang, PhD, Auckland Transport, New Zealand</li><!-- Derek Zhang (AT) <Derek.Zhang@at.govt.nz> -->
<li>Zhixun Su, PhD, Dalian University of Technology, China </li><!-- 苏志勋 <zxsu@dlut.edu.cn> -->


			<!--Hamid GholamHosseini, PhD
			<!--<li>temp, PhD, AUT, New Zealand</li><!-- temp -->

        </ol>
      </section>



    </main>
	<p></p>
    <footer class="bg-dark text-white text-center py-3">
      <p>© PSIVT 2023. All Rights Reserved.</p>
    </footer>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
  
</body></html>