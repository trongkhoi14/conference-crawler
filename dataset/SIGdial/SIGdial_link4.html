<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>SIGDial Conference (2023) - ACL Anthology</title><meta name="generator" content="Hugo 0.118.2"><link href="/aclicon.ico" rel="shortcut icon" type="image/x-icon"><link rel="stylesheet" href="/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css" media="screen"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous"><link rel="stylesheet" href="/css/academicons.min.css"></head><body><nav class="navbar navbar-expand-sm navbar-light bg-light bg-gradient-light shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id="navbar-container" class="container"><a class="navbar-brand" href="/"><img src="/images/acl-logo.svg" width="56" alt="ACL Logo">
<span class="d-inline pl-2">ACL Anthology</span></a>
<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
<span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav flex-grow-1 pr-md-2"><li class="nav-item"><a class="nav-link" href="/posts/">News<span class="sr-only">(current)</span></a></li><li class="nav-item"><a class="nav-link" href="/faq/">FAQ<span class="sr-only">(current)</span></a></li><li class="nav-item"><a class="nav-link" href="/info/corrections/">Corrections<span class="sr-only">(current)</span></a></li><li class="nav-item"><a class="nav-link" href="/info/contrib/">Submissions<span class="sr-only">(current)</span></a></li><li class="nav-item"><a class="nav-link" href="https://github.com/acl-org/acl-anthology/"><i class="fab fa-github pr-1"></i>Github</a></li></ul><form class="form-inline my-2 my-lg-0 flex-nowrap" action="/search/?" method="get"><input id="acl-search-box" class="form-control mr-sm-2" name="q" type="search" placeholder="Search..." aria-label="Search">
<button class="btn btn-outline-primary" type="submit"><i class="fas fa-search"></i></button></form></div></div></nav><div id="main-container" class="container"><section id="main"><h2 id="title">SIGDial Conference (2023)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class="card-body"><h4 class="card-title">Volumes</h4><ul class="list-pl-responsive"><li><a class="align-middle" href="#2023inlg-main">Proceedings of the 16th International Natural Language Generation Conference</a>
<span class="badge badge-info align-middle ml-1">37&nbsp;papers</span></li><li><a class="align-middle" href="#2023inlg-demos">Proceedings of the 16th International Natural Language Generation Conference: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class="align-middle" href="#2023inlg-genchal">Proceedings of the 16th International Natural Language Generation Conference: Generation Challenges</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class="align-middle" href="#2023sigdial-1">Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue</a>
<span class="badge badge-info align-middle ml-1">61&nbsp;papers</span></li><li><a class="align-middle" href="#2023cs4oa-1">Proceedings of the 1st Workshop on CounterSpeech for Online Abuse (CS4OA)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class="align-middle" href="#2023dstc-1">Proceedings of The Eleventh Dialog System Technology Challenge</a>
<span class="badge badge-info align-middle ml-1">29&nbsp;papers</span></li><li><a class="align-middle" href="#2023icard-1">Proceedings of the First Workshop on Connecting Multiple Disciplines to AI Techniques in Interaction-centric Autism Research and Diagnosis (ICARD 2023)</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class="align-middle" href="#2023mmnlg-1">Proceedings of the Workshop on Multimodal, Multilingual Natural Language Generation and Multilingual WebNLG Challenge (MM-NLG 2023)</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class="align-middle" href="#2023tllm-1">Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants!</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class="align-middle" href="#2023yrrsds-1">Proceedings of the 19th Annual Meeting of the Young Reseachers' Roundtable on Spoken Dialogue Systems</a>
<span class="badge badge-info align-middle ml-1">25&nbsp;papers</span></li></ul></div></div><button class="btn btn-sm btn-info d-block mb-3" id="toggle-all-abstracts" data-toggle-state="hide">
<span class="on-toggle-state-hide">Show all abstracts<i class="ml-2 fas fa-angle-double-down"></i></span><span class="on-toggle-state-show">Hide all abstracts<i class="ml-2 fas fa-angle-double-up"></i></span></button><div id="2023inlg-main"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.inlg-main.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.inlg-main/">Proceedings of the 16th International Natural Language Generation Conference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.0.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.0.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.0/">Proceedings of the 16th International Natural Language Generation Conference</a></strong><br><a href="/people/c/c-maria-keet/">C. Maria Keet</a>
|
<a href="/people/h/hung-yi-lee/">Hung-Yi Lee</a>
|
<a href="/people/s/sina-zarriess/">Sina Zarrieß</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.1/">Guided Beam Search to Improve Generalization in Low-Resource Data-to-Text Generation</a></strong><br><a href="/people/n/nicolas-garneau/">Nicolas Garneau</a>
|
<a href="/people/l/luc-lamontagne/">Luc Lamontagne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--1"><div class="card-body p-3 small">In this paper, we introduce a new beam search algorithm that improves the generalization of neural generators to unseen examples, especially in low-resource data-to-text settings. Our algorithm aims to reduce the number of omissions and hallucinations during the decoding process. For this purpose, it relies on two regression models to explicitly characterize factual errors. We explain how to create a new dataset to train these models given an original training set of less than a thousand data points. We apply our approach in the low-resource, legal setting using the French Plum2Text dataset, as well as in English using WebNLG. We observe in our experiment that this combination improves the faithfulness of pre-trained neural text generators using both human and automatic evaluation. Moreover, our approach offers a level of interpretability by predicting the number of omissions and hallucinations present in a given generation with respect to the input data. Finally, we visualize our algorithm’s exploration of the hypothesis space at different steps during the decoding process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.2/"><span class="acl-fixed-case">XF</span>2<span class="acl-fixed-case">T</span>: Cross-lingual Fact-to-Text Generation for Low-Resource Languages</a></strong><br><a href="/people/s/shivprasad-sagare/">Shivprasad Sagare</a>
|
<a href="/people/t/tushar-abhishek/">Tushar Abhishek</a>
|
<a href="/people/b/bhavyajeet-singh/">Bhavyajeet Singh</a>
|
<a href="/people/a/anubhav-sharma/">Anubhav Sharma</a>
|
<a href="/people/m/manish-gupta/">Manish Gupta</a>
|
<a href="/people/v/vasudeva-varma/">Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--2"><div class="card-body p-3 small">Multiple business scenarios require an automated generation of descriptive human-readable text from structured input data. This has resulted into substantial work on fact-to-text generation systems recently. Unfortunately, previous work on fact-to-text (F2T) generation has focused primarily on English mainly due to the high availability of relevant datasets. Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed for generation across multiple languages alongwith a dataset, XAlign for eight languages. However, there has been no rigorous work on the actual XF2T generation problem. We extend XAlign dataset with annotated data for four more languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive study using popular Transformer-based text generation models on our extended multi-lingual dataset, which we call XAlignV2. Further, we investigate the performance of different text generation strategies: multiple variations of pretraining, fact-aware embeddings and structure-aware input encoding. Our extensive experiments show that a multi-lingual mT5 model which uses fact-aware embeddings with structure-aware input encoding leads to best results (30.90 BLEU, 55.12 METEOR and 59.17 chrF++) across the twelve languages. We make our code, dataset and model publicly available, and hope that this will help advance further research in this critical area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.3/">Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy</a></strong><br><a href="/people/d/daphne-ippolito/">Daphne Ippolito</a>
|
<a href="/people/f/florian-tramer/">Florian Tramer</a>
|
<a href="/people/m/milad-nasr/">Milad Nasr</a>
|
<a href="/people/c/chiyuan-zhang/">Chiyuan Zhang</a>
|
<a href="/people/m/matthew-jagielski/">Matthew Jagielski</a>
|
<a href="/people/k/katherine-lee/">Katherine Lee</a>
|
<a href="/people/c/christopher-choquette-choo/">Christopher Choquette Choo</a>
|
<a href="/people/n/nicholas-carlini/">Nicholas Carlini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--3"><div class="card-body p-3 small">Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data and aids in the development of countermeasures. Many prior works—and some recently deployed defenses—focus on “verbatim memorization”, defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense that _perfectly_ prevents all verbatim memorization. And yet, we demonstrate that this “perfect” filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified “style-transfer” prompts—and in some cases even the non-modified original prompts—to extract memorized information. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.4/">Fine-Tuning <span class="acl-fixed-case">GPT</span>-3 for Synthetic <span class="acl-fixed-case">D</span>anish News Generation</a></strong><br><a href="/people/m/mina-almasi/">Mina Almasi</a>
|
<a href="/people/a/anton-schionning/">Anton Schiønning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--4"><div class="card-body p-3 small">While GPT-3 has garnered significant attention for its capabilities in natural language generation, research on its use outside of English is still relatively limited. We focus on how GPT-3 can be fine-tuned for generating synthetic news articles in a low-resource language, namely Danish. The model’s performance is evaluated on the dimensions of human and machine detection in two separate experiments. When presented with either a real or GPT-3 generated news article, human participants achieve a 58.1% classification accuracy. Contrarily, a fine-tuned BERT classifier obtains a 92.7% accuracy on the same task. This discrepancy likely pertains to the fine-tuned GPT-3 model oversampling high-likelihood tokens in its text generation. Although this is undetectable to the human eye, it leaves a statistical discrepancy for machine classifiers to detect. We address how decisions in the experimental design favoured the machine classifiers over the human evaluators, and whether the produced synthetic articles are applicable in a real-world context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.5.Supplementary_Attachment.zip" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.5/"><span class="acl-fixed-case">GAN</span>-<span class="acl-fixed-case">LM</span>: Generative Adversarial Network using Language Models for Downstream Applications</a></strong><br><a href="/people/d/dae-yon-hwang/">Dae Yon Hwang</a>
|
<a href="/people/y/yaroslav-nechaev/">Yaroslav Nechaev</a>
|
<a href="/people/c/cyprien-de-lichy/">Cyprien de Lichy</a>
|
<a href="/people/r/renxian-zhang/">Renxian Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--5"><div class="card-body p-3 small">In this work, we investigate Data Augmentation methods to improve the performance of state-of-the-art models for four different downstream tasks. Specifically, we propose Generative Adversarial Network using Language Models (GAN-LM) approach that combines a deep generative model with a pre-trained language model to produce diverse augmentations. We compare the GAN-LM to various conventional methods in non-contextual- and contextual-levels on four public datasets: ZESHEL for zero-shot entity linking, TREC for question classification, STS-B for sentence pairs semantic textual similarity (STS), and mSTS for multilingual sentence pairs STS. Additionally, we subsample these datasets to study the impact of such augmentations in low-resource settings where limited amounts of training data is available. Compared to the state-of-the-art methods in downstream tasks, we mostly achieve the best performance using GAN-LM approach. Finally, we investigate the way of combining the GAN-LM with other augmentation methods to complement our proposed approach. The developed code for reproducibility is included in the supplementary material.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.6.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.6.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--6" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.6" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.6.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.6/">Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization</a></strong><br><a href="/people/c/chieh-yang-huang/">Chieh-Yang Huang</a>
|
<a href="/people/t/ting-yao-hsu/">Ting-Yao Hsu</a>
|
<a href="/people/r/ryan-rossi/">Ryan Rossi</a>
|
<a href="/people/a/ani-nenkova/">Ani Nenkova</a>
|
<a href="/people/s/sungchul-kim/">Sungchul Kim</a>
|
<a href="/people/g/gromit-yeuk-yin-chan/">Gromit Yeuk-Yin Chan</a>
|
<a href="/people/e/eunyee-koh/">Eunyee Koh</a>
|
<a href="/people/c/c-lee-giles/">C Lee Giles</a>
|
<a href="/people/t/ting-hao-huang/">Ting-Hao Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--6"><div class="card-body p-3 small">Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., “Figure 3 shows...”) into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and data are available at: https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.7.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.7.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--7" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.7" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.7/">Models of reference production: How do they withstand the test of time?</a></strong><br><a href="/people/f/fahime-same/">Fahime Same</a>
|
<a href="/people/g/guanyi-chen/">Guanyi Chen</a>
|
<a href="/people/k/kees-van-deemter/">Kees van Deemter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--7"><div class="card-body p-3 small">In recent years, many NLP studies have focused solely on performance improvement. In this work, we focus on the linguistic and scientific aspects of NLP. We use the task of generating referring expressions in context (REG-in-context) as a case study and start our analysis from GREC, a comprehensive set of shared tasks in English that addressed this topic over a decade ago. We ask what the performance of models would be if we assessed them (1) on more realistic datasets, and (2) using more advanced methods. We test the models using different evaluation metrics and feature selection experiments. We conclude that GREC can no longer be regarded as offering a reliable assessment of models’ ability to mimic human reference production, because the results are highly impacted by the choice of corpus and evaluation metrics. Our results also suggest that pre-trained language models are less dependent on the choice of corpus than classic Machine Learning models, and therefore make more robust class predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.8.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.8.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--8" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.8" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.8.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.8/">Generating Faithful Text From a Knowledge Graph with Noisy Reference Text</a></strong><br><a href="/people/t/tahsina-hashem/">Tahsina Hashem</a>
|
<a href="/people/w/weiqing-wang/">Weiqing Wang</a>
|
<a href="/people/d/derry-tanti-wijaya/">Derry Tanti Wijaya</a>
|
<a href="/people/m/mohammed-eunus-ali/">Mohammed Eunus Ali</a>
|
<a href="/people/y/yuan-fang-li/">Yuan-Fang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--8"><div class="card-body p-3 small">Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model’s ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level of hallucination in the generated text by employing a controllable text generation technique. We evaluate our model’s performance through the standard quantitative metrics as well as a ChatGPT-based quantitative and qualitative analysis. Our evaluation demonstrates the superior performance of our model over state-of-the-art KG-to-text models on faithfulness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.9.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.9.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--9" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.9" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.9/">Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings</a></strong><br><a href="/people/l/laura-mascarell/">Laura Mascarell</a>
|
<a href="/people/r/ribin-chalumattu/">Ribin Chalumattu</a>
|
<a href="/people/j/julien-heitmann/">Julien Heitmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--9"><div class="card-body p-3 small">Research in Multi-document Summarization (MDS) mostly focuses on the English language and depends on large MDS datasets that are not available for other languages. Some of these approaches concatenate the source documents, resulting in overlong model inputs. Existing transformer architectures are unable to process such long inputs entirely, omitting documents in the summarization process. Other solutions address this issue by implementing multi-stage approaches that also require changes in the model architecture. In this paper, we introduce various sampling approaches based on information entropy that allow us to perform MDS in a single stage. These approaches also consider all source documents without using MDS training data nor changing the model’s architecture. Besides, we build a MDS test set of German news articles to assess the performance of our methods on abstractive multi-document summaries. Experimental results show that our entropy-based approaches outperform previous state-of-the-art on German MDS, while still remaining primarily abstractive. We release our code and MDS test set to encourage further research in German abstractive MDS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.10.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.10.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--10" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.10" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.10/">Claim Optimization in Computational Argumentation</a></strong><br><a href="/people/g/gabriella-skitalinskaya/">Gabriella Skitalinskaya</a>
|
<a href="/people/m/maximilian-spliethover/">Maximilian Spliethöver</a>
|
<a href="/people/h/henning-wachsmuth/">Henning Wachsmuth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--10"><div class="card-body p-3 small">An optimal delivery of arguments is key to persuasion in any debate, both for humans and for AI systems. This requires the use of clear and fluent claims relevant to the given debate. Prior work has studied the automatic assessment of argument quality extensively. Yet, no approach actually improves the quality so far. To fill this gap, this paper proposes the task of claim optimization: to rewrite argumentative claims in order to optimize their delivery. As multiple types of optimization are possible, we approach this task by first generating a diverse set of candidate claims using a large language model, such as BART, taking into account contextual information. Then, the best candidate is selected using various quality metrics. In automatic and human evaluation on an English-language corpus, our quality-based candidate selection outperforms several baselines, improving 60% of all claims (worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our approach often specifies claims with details, whereas it adds less evidence than humans do. Moreover, its capabilities generalize well to other domains, such as instructional texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.11.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.11.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--11" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.11" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.11.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.11/"><span class="acl-fixed-case">C</span>hat<span class="acl-fixed-case">GPT</span>’s Information Seeking Strategy: Insights from the 20-Questions Game</a></strong><br><a href="/people/l/leonardo-bertolazzi/">Leonardo Bertolazzi</a>
|
<a href="/people/d/davide-mazzaccara/">Davide Mazzaccara</a>
|
<a href="/people/f/filippo-merlo/">Filippo Merlo</a>
|
<a href="/people/r/raffaella-bernardi/">Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--11"><div class="card-body p-3 small">Large Language Models, and ChatGPT in particular, have recently grabbed the attention of the community and the media. Having reached high language proficiency, attention has been shifting toward its reasoning capabilities. In this paper, our main aim is to evaluate ChatGPT’s question generation in a task where language production should be driven by an implicit reasoning process. To this end, we employ the 20-Questions game, traditionally used within the Cognitive Science community to inspect the information seeking-strategy’s development. This task requires a series of interconnected skills: asking informative questions, stepwise updating the hypothesis space, and stopping asking questions when enough information has been collected. We build hierarchical hypothesis spaces, exploiting feature norms collected from humans vs. ChatGPT itself, and we inspect the efficiency and informativeness of ChatGPT’s strategy. Our results show that ChatGPT’s performance gets closer to an optimal agent only when prompted to explicitly list the updated space stepwise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.12.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.12.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--12" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.12" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.12/">This is not correct! Negation-aware Evaluation of Language Generation Systems</a></strong><br><a href="/people/m/miriam-anschutz/">Miriam Anschütz</a>
|
<a href="/people/d/diego-miguel-lozano/">Diego Miguel Lozano</a>
|
<a href="/people/g/georg-groh/">Georg Groh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--12"><div class="card-body p-3 small">Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models’ performances on other perturbations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.13.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.13.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--13" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.13" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.13/">Guidance in Radiology Report Summarization: An Empirical Evaluation and Error Analysis</a></strong><br><a href="/people/j/jan-trienes/">Jan Trienes</a>
|
<a href="/people/p/paul-youssef/">Paul Youssef</a>
|
<a href="/people/j/jorg-schlotterer/">Jörg Schlötterer</a>
|
<a href="/people/c/christin-seifert/">Christin Seifert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--13"><div class="card-body p-3 small">Automatically summarizing radiology reports into a concise impression can reduce the manual burden of clinicians and improve the consistency of reporting. Previous work aimed to enhance content selection and factuality through guided abstractive summarization. However, two key issues persist. First, current methods heavily rely on domain-specific resources to extract the guidance signal, limiting their transferability to domains and languages where those resources are unavailable. Second, while automatic metrics like ROUGE show progress, we lack a good understanding of the errors and failure modes in this task. To bridge these gaps, we first propose a domain-agnostic guidance signal in form of variable-length extractive summaries. Our empirical results on two English benchmarks demonstrate that this guidance signal improves upon unguided summarization while being competitive with domain-specific methods. Additionally, we run an expert evaluation of four systems according to a taxonomy of 11 fine-grained errors. We find that the most pressing differences between automatic summaries and those of radiologists relate to content selection including omissions (up to 52%) and additions (up to 57%). We hypothesize that latent reporting factors and corpus-level inconsistencies may limit models to reliably learn content selection from the available data, presenting promising directions for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.14.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.14.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--14" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.14" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.14.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.14/">A Zero-Shot Approach for Multi-User Task-Oriented Dialog Generation</a></strong><br><a href="/people/s/shiv-surya/">Shiv Surya</a>
|
<a href="/people/y/yohan-jo/">Yohan Jo</a>
|
<a href="/people/a/arijit-biswas/">Arijit Biswas</a>
|
<a href="/people/a/alexandros-potamianos/">Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--14"><div class="card-body p-3 small">Prior art investigating task-oriented dialog and automatic generation of such dialogs have focused on single-user dialogs between a single user and an agent. However, there is limited study on adapting such AI agents to multi-user conversations (involving multiple users and an agent). Multi-user conversations are richer than single-user conversations containing social banter and collaborative decision making. The most significant challenge impeding such studies is the lack of suitable multi-user task-oriented dialogs with annotations of user belief states and system actions. One potential solution is multi-user dialog generation from single-user data. Many single-user dialogs datasets already contain dialog state information (intents, slots), thus making them suitable candidates. In this work, we propose a novel approach for expanding single-user task-oriented dialogs (e.g. MultiWOZ) to multi-user dialogs in a zero-shot setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.15.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.15.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--15" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.15" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.15/">Beyond the Bias: Unveiling the Quality of Implicit Causality Prompt Continuations in Language Models</a></strong><br><a href="/people/j/judith-sieker/">Judith Sieker</a>
|
<a href="/people/o/oliver-bott/">Oliver Bott</a>
|
<a href="/people/t/torgrim-solstad/">Torgrim Solstad</a>
|
<a href="/people/s/sina-zarriess/">Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--15"><div class="card-body p-3 small">Recent studies have used human continuations of Implicit Causality (IC) prompts collected in linguistic experiments to evaluate discourse understanding in large language models (LLMs), focusing on the well-known IC coreference bias in the LLMs’ predictions of the next word following the prompt. In this study, we investigate how continuations of IC prompts can be used to evaluate the text generation capabilities of LLMs in a linguistically controlled setting. We conduct an experiment using two open-source GPT-based models, employing human evaluation to assess different aspects of continuation quality. Our findings show that LLMs struggle in particular with generating coherent continuations in this rather simple setting, indicating a lack of discourse knowledge beyond the well-known IC bias. Our results also suggest that a bias congruent continuation does not necessarily equate to a higher continuation quality. Furthermore, our study draws upon insights from the Uniform Information Density hypothesis, testing different prompt modifications and decoding procedures and showing that sampling-based methods are particularly sensitive to the information density of the prompts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.16.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.16.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--16" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.16" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.16.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.16/">Enhancing factualness and controllability of Data-to-Text Generation via data Views and constraints</a></strong><br><a href="/people/c/craig-thomson/">Craig Thomson</a>
|
<a href="/people/c/clement-rebuffel/">Clement Rebuffel</a>
|
<a href="/people/e/ehud-reiter/">Ehud Reiter</a>
|
<a href="/people/l/laure-soulier/">Laure Soulier</a>
|
<a href="/people/s/somayajulu-sripada/">Somayajulu Sripada</a>
|
<a href="/people/p/patrick-gallinari/">Patrick Gallinari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--16"><div class="card-body p-3 small">Neural data-to-text systems lack the control and factual accuracy required to generate useful and insightful summaries of multidimensional data. We propose a solution in the form of data views, where each view describes an entity and its attributes along specific dimensions. A sequence of views can then be used as a high-level schema for document planning, with the neural model handling the complexities of micro-planning and surface realization. We show that our view-based system retains factual accuracy while offering high-level control of output that can be tailored based on user preference or other norms within the domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.17.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.17.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--17" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.17" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.17/">Memories for Virtual <span class="acl-fixed-case">AI</span> Characters</a></strong><br><a href="/people/f/fabian-landwehr/">Fabian Landwehr</a>
|
<a href="/people/e/erika-varis-doggett/">Erika Varis Doggett</a>
|
<a href="/people/r/romann-m-weber/">Romann M. Weber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--17"><div class="card-body p-3 small">In this paper, we present a system for augmenting virtual AI characters with long-term memory, enabling them to remember facts about themselves, their world, and past experiences. We propose a memory-creation pipeline that converts raw text into condensed memories and a memory-retrieval system that utilizes these memories to generate character responses. Using a fact-checking pipeline based on GPT-4, our evaluation demonstrates that the character responses are grounded in the retrieved memories and maintain factual accuracy. We discuss the implications of our system for creating engaging and consistent virtual characters and highlight areas for future research, including large language model (LLM) guardrailing and virtual character personality development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.18.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.18.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--18" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.18" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.18/">Metric-Based In-context Learning: A Case Study in Text Simplification</a></strong><br><a href="/people/s/subhadra-vadlamannati/">Subhadra Vadlamannati</a>
|
<a href="/people/g/gozde-gul-sahin/">Gözde Şahin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--18"><div class="card-body p-3 small">In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust to example orderings and out-of-domain test sets, and outperforms strong baselines and state-of-the-art finetuned language models. Finally, we show that the behavior of large GPT models can be implicitly controlled by the chosen metric. Our research provides a new framework for selecting examples in ICL, and demonstrates its effectiveness in text simplification tasks, breaking new ground for more accurate and efficient NLG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.19.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.19.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--19" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.19" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.19/">Exploring the Naturalness of Cognitive Status-Informed Referring Form Selection Models</a></strong><br><a href="/people/g/gabriel-del-castillo/">Gabriel Del Castillo</a>
|
<a href="/people/g/grace-clark/">Grace Clark</a>
|
<a href="/people/z/zhao-han/">Zhao Han</a>
|
<a href="/people/t/tom-williams/">Tom Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--19"><div class="card-body p-3 small">Language-capable robots must be able to efficiently and naturally communicate about objects in the environment. A key part of communication is Referring Form Selection (RFS): the process of selecting a form like it, that, or the N to use when referring to an object. Recent cognitive status-informed computational RFS models have been evaluated in terms of goodness-of-fit to human data. But it is as yet unclear whether these models actually select referring forms that are any more natural than baseline alternatives, regardless of goodness-of-fit. Through a human subject study designed to assess this question, we show that even though cognitive status-informed referring selection models achieve good fit to human data, they do not (yet) produce concrete benefits in terms of naturality. On the other hand, our results show that human utterances also had high variability in perceived naturality, demonstrating the challenges of evaluating RFS naturality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.20.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.20.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--20" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.20" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.20/">System-Initiated Transitions from Chit-Chat to Task-Oriented Dialogues with Transition Info Extractor and Transition Sentence Generator</a></strong><br><a href="/people/y/ye-liu/">Ye Liu</a>
|
<a href="/people/s/stefan-ultes/">Stefan Ultes</a>
|
<a href="/people/w/wolfgang-minker/">Wolfgang Minker</a>
|
<a href="/people/w/wolfgang-maier/">Wolfgang Maier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--20"><div class="card-body p-3 small">In this work, we study dialogue scenarios that start from chit-chat but eventually switch to task-related services, and investigate how a unified dialogue model, which can engage in both chit-chat and task-oriented dialogues, takes the initiative during the dialogue mode transition from chit-chat to task-oriented in a coherent and cooperative manner. We firstly build a <i>transition info extractor</i> (TIE) that keeps track of the preceding chit-chat interaction and detects the potential user intention to switch to a task-oriented service. Meanwhile, in the unified model, a <i>transition sentence generator</i> (TSG) is extended through efficient Adapter tuning and transition prompt learning. When the TIE successfully finds task-related information from the preceding chit-chat, such as a transition domain (“train” in Figure fig: system-initiated transition from chit-chat to task-oriented.), then the TSG is activated automatically in the unified model to initiate this transition by generating a transition sentence under the guidance of transition information extracted by TIE. The experimental results show promising performance regarding the proactive transitions. We achieve an additional large improvement on TIE model by utilizing Conditional Random Fields (CRF). The TSG can flexibly generate transition sentences while maintaining the unified capabilities of normal chit-chat and task-oriented response generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.21.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.21.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--21" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.21" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.21/"><span class="acl-fixed-case">HL</span> Dataset: Visually-grounded Description of Scenes, Actions and Rationales</a></strong><br><a href="/people/m/michele-cafagna/">Michele Cafagna</a>
|
<a href="/people/k/kees-van-deemter/">Kees van Deemter</a>
|
<a href="/people/a/albert-gatt/">Albert Gatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--21"><div class="card-body p-3 small">Current captioning datasets focus on object-centric captions, describing the visible objects in the image, often ending up stating the obvious (for humans), e.g. “people eating food in a park”. Although these datasets are useful to evaluate the ability of Vision &amp; Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict (“people at a holiday resort”) and the actions they perform (“people having a picnic”). Such concepts are based on personal experience and contribute to forming common sense assumptions. We present the High-Level Dataset, a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions and rationales. We further extend this dataset with confidence scores collected from an independent set of readers, as well as a set of narrative captions generated synthetically, by combining each of the three axes. We describe this dataset and analyse it extensively. We also present baseline results for the High-Level Captioning task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.22.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.22.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--22" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.22" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.22.Supplementary_Attachment.zip" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.22/">Validating Predictive Models Of Evaluative Language For Controllable <span class="acl-fixed-case">D</span>ata2<span class="acl-fixed-case">T</span>ext Generation</a></strong><br><a href="/people/m/maurice-langner/">Maurice Langner</a>
|
<a href="/people/r/ralf-klabunde/">Ralf Klabunde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--22"><div class="card-body p-3 small">In data2text generation, tabular data is transformed into a text that expresses information from that source domain. While some text types, such as instructions, demand objective and neutral language without any expressive and evaluative content, many other text types are expected to provide expressions for these kinds of subjective meanings. In controllable, pipelined neural NLG separate learning models, notably regression models, can be used to predict whether some feature deviates sufficiently strongly from an expected value, so that evaluative language would be appropriate for verbalizing this finding. In this paper, we present an empirical study on the comprehension of evaluative adverbs and adjectival modifiers in car reviews, a text type that is characterized by a mixture of factual information with evaluations expressing positive or negative surprise. We show to what extend regression-based decision boundaries for producing evaluative content in controllable data2text NLG match the reader’s expectations that are raised by those evaluative markers. Finally we show that regression values in combination with standard deviation of the technical input data constitute reasonable Boolean thresholds for both positive and negative surprise, which provide the basis for the development of more complex models that also include the scalar base of adverbs and modifiers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.23.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.23.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--23" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.23" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.23.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.23/">The Next Chapter: A Study of Large Language Models in Storytelling</a></strong><br><a href="/people/z/zhuohan-xie/">Zhuohan Xie</a>
|
<a href="/people/t/trevor-cohn/">Trevor Cohn</a>
|
<a href="/people/j/jey-han-lau/">Jey Han Lau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--23"><div class="card-body p-3 small">To enhance the quality of generated stories, recent story generation models have been investigating the utilization of higher-level attributes like plots or commonsense knowledge. The application of prompt-based learning with large language models (LLMs), exemplified by GPT-3, has exhibited remarkable performance in diverse natural language processing (NLP) tasks. This paper conducts a comprehensive investigation, utilizing both automatic and human evaluation, to compare the story generation capacity of LLMs with recent models across three datasets with variations in style, register, and length of stories. The results demonstrate that LLMs generate stories of significantly higher quality compared to other story generation models. Moreover, they exhibit a level of performance that competes with human authors, albeit with the preliminary observation that they tend to replicate real stories in situations involving world knowledge, resembling a form of plagiarism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.24.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.24.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--24" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.24" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.24/">Trustworthiness of Children Stories Generated by Large Language Models</a></strong><br><a href="/people/p/prabin-bhandari/">Prabin Bhandari</a>
|
<a href="/people/h/hannah-brennan/">Hannah Brennan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--24"><div class="card-body p-3 small">Large Language Models (LLMs) have shown a tremendous capacity for generating literary text. However, their effectiveness in generating children’s stories has yet to be thoroughly examined. In this study, we evaluate the trustworthiness of children’s stories generated by LLMs using various measures, and we compare and contrast our results with both old and new children’s stories to better assess their significance. Our findings suggest that LLMs still struggle to generate children’s stories at the level of quality and nuance found in actual stories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.25.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.25.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--25" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.25" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.25/">On Text Style Transfer via Style-Aware Masked Language Models</a></strong><br><a href="/people/s/sharan-narasimhan/">Sharan Narasimhan</a>
|
<a href="/people/p/pooja-h/">Pooja H</a>
|
<a href="/people/s/suvodip-dey/">Suvodip Dey</a>
|
<a href="/people/m/maunendra-sankar-desarkar/">Maunendra Sankar Desarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--25"><div class="card-body p-3 small">Text Style Transfer (TST) is performable through approaches such as latent space disentanglement, cycle-consistency losses, prototype editing etc. The prototype editing approach, which is known to be quite successful in TST, involves two key phases a) Masking of source style-associated tokens and b) Reconstruction of this source-style masked sentence conditioned with the target style. We follow a similar transduction method, in which we transpose the more difficult direct source to target TST task to a simpler Style-Masked Language Model (SMLM) Task, wherein, similar to BERT (CITATION), the goal of our model is now to reconstruct the source sentence from its style-masked version. We arrive at the SMLM mechanism naturally by formulating prototype editing/ transduction methods in a probabilistic framework, where TST resolves into estimating a hypothetical parallel dataset from a partially observed parallel dataset, wherein each domain is assumed to have a common latent style-masked prior. To generate this style-masked prior, we use “Explainable Attention” as our choice of attribution for a more precise style-masking step and also introduce a cost-effective and accurate “Attribution-Surplus” method of determining the position of masks from any arbitrary attribution model in O(1) time. We empirically show that this non-generational approach well suites the “content preserving” criteria for a task like TST, even for a complex style like Discourse Manipulation. Our model, the Style MLM, outperforms strong TST baselines and is on par with state-of-the-art TST models, which use complex architectures and orders of more parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.26.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.26.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--26" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.26" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.26/">Affective Natural Language Generation of Event Descriptions through Fine-grained Appraisal Conditions</a></strong><br><a href="/people/y/yarik-menchaca-resendiz/">Yarik Menchaca Resendiz</a>
|
<a href="/people/r/roman-klinger/">Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--26"><div class="card-body p-3 small">Models for affective text generation have shown a remarkable progress, but they commonly rely only on basic emotion theories or valance/arousal values as conditions. This is appropriate when the goal is to create explicit emotion statements (“The kid is happy.”). Emotions are, however, commonly communicated implicitly. For instance, the emotional interpretation of an event (“Their dog died.”) does often not require an explicit emotion statement. In psychology, appraisal theories explain the link between a cognitive evaluation of an event and the potentially developed emotion. They put the assessment of the situation on the spot, for instance regarding the own control or the responsibility for what happens. We hypothesize and subsequently show that including appraisal variables as conditions in a generation framework comes with two advantages. (1) The generation model is informed in greater detail about what makes a specific emotion and what properties it has. This leads to text generation that better fulfills the condition. (2) The variables of appraisal allow a user to perform a more fine-grained control of the generated text, by stating properties of a situation instead of only providing the emotion category. Our Bart and T5-based experiments with 7 emotions (Anger, Disgust, Fear, Guilt, Joy, Sadness, Shame), and 7 appraisals (Attention, Responsibility, Control, Circumstance, Pleasantness, Effort, Certainty) show that (1) adding appraisals during training improves the accurateness of the generated texts by 10 pp in F1. Further, (2) the texts with appraisal variables are longer and contain more details. This exemplifies the greater control for users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.27.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.27.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--27" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.27" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.27/">Leveraging Low-resource Parallel Data for Text Style Transfer</a></strong><br><a href="/people/s/sourabrata-mukherjee/">Sourabrata Mukherjee</a>
|
<a href="/people/o/ondrej-dusek/">Ondrej Dusek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--27"><div class="card-body p-3 small">Text style transfer (TST) involves transforming a text into a desired style while approximately preserving its content. The biggest challenge in TST in the general lack of parallel data. Many existing approaches rely on complex models using substantial non-parallel data, with mixed results. In this paper, we leverage a pretrained BART language model with minimal parallel data and incorporate low-resource methods such as hyperparameter tuning, data augmentation, and self-training, which have not been explored in TST. We further include novel style-based rewards in the training loss. Through extensive experiments in sentiment transfer, a sub-task of TST, we demonstrate that our simple yet effective approaches achieve well-balanced results, surpassing non-parallel approaches and highlighting the usefulness of parallel data even in small amounts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.28.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.28.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--28" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.28" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.28/">Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System</a></strong><br><a href="/people/d/daphne-ippolito/">Daphne Ippolito</a>
|
<a href="/people/n/nicholas-carlini/">Nicholas Carlini</a>
|
<a href="/people/k/katherine-lee/">Katherine Lee</a>
|
<a href="/people/m/milad-nasr/">Milad Nasr</a>
|
<a href="/people/y/yun-william-yu/">Yun William Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--28"><div class="card-body p-3 small">Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model’s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.29.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.29.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--29" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.29" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.29.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.29/">Controlling keywords and their positions in text generation</a></strong><br><a href="/people/y/yuichi-sasazawa/">Yuichi Sasazawa</a>
|
<a href="/people/t/terufumi-morishita/">Terufumi Morishita</a>
|
<a href="/people/h/hiroaki-ozaki/">Hiroaki Ozaki</a>
|
<a href="/people/o/osamu-imaichi/">Osamu Imaichi</a>
|
<a href="/people/y/yasuhiro-sogawa/">Yasuhiro Sogawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--29"><div class="card-body p-3 small">One of the challenges in text generation is to control text generation as intended by the user. Previous studies proposed specifying the keywords that should be included in the generated text. However, this approach is insufficient to generate text that reflect the user’s intent. For example, placing an important keyword at the beginning of the text would help attract the reader’s attention; however, existing methods do not enable such flexible control. In this paper, we tackle a novel task of controlling not only keywords but also the position of each keyword in the text generation. To this end, we propose a task-independent method that uses special tokens to control the relative position of keywords. Experimental results on summarization and story generation tasks show that the proposed method can control keywords and their positions. The experimental results also demonstrate that controlling the keyword positions can generate summary texts that are closer to the user’s intent than baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.30.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.30.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--30" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.30" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.30/">Tackling Hallucinations in Neural Chart Summarization</a></strong><br><a href="/people/s/saad-obaid-ul-islam/">Saad Obaid ul Islam</a>
|
<a href="/people/i/iza-skrjanec/">Iza Škrjanec</a>
|
<a href="/people/o/ondrej-dusek/">Ondrej Dusek</a>
|
<a href="/people/v/vera-demberg/">Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--30"><div class="card-body p-3 small">Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations. We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.31.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.31.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--31" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.31" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.31/">Learning Disentangled Meaning and Style Representations for Positive Text Reframing</a></strong><br><a href="/people/x/xu-sheng/">Xu Sheng</a>
|
<a href="/people/f/fumiyo-fukumoto/">Fumiyo Fukumoto</a>
|
<a href="/people/j/jiyi-li/">Jiyi Li</a>
|
<a href="/people/g/go-kentaro/">Go Kentaro</a>
|
<a href="/people/y/yoshimi-suzuki/">Yoshimi Suzuki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--31"><div class="card-body p-3 small">The positive text reframing (PTR) task which generates a text giving a positive perspective with preserving the sense of the input text, has attracted considerable attention as one of the NLP applications. Due to the significant representation capability of the pre-trained language model (PLM), a beneficial baseline can be easily obtained by just fine-tuning the PLM. However, how to interpret a diversity of contexts to give a positive perspective is still an open problem. Especially, it is more serious when the size of the training data is limited. In this paper, we present a PTR framework, that learns representations where the meaning and style of text are structurally disentangled. The method utilizes pseudo-positive reframing datasets which are generated with two augmentation strategies. A simple but effective multi-task learning-based model is learned to fuse the generation capabilities from these datasets. Experimental results on Positive Psychology Frames (PPF) dataset, show that our approach outperforms the baselines, BART by five and T5 by six evaluation metrics. Our source codes and data are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.32.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.32.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--32" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.32" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.32/">Generating clickbait spoilers with an ensemble of large language models</a></strong><br><a href="/people/m/mateusz-wozny/">Mateusz Woźny</a>
|
<a href="/people/m/mateusz-lango/">Mateusz Lango</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--32"><div class="card-body p-3 small">Clickbait posts are a widespread problem in the webspace. The generation of spoilers, i.e. short texts that neutralize clickbait by providing information that makes it uninteresting, is one of the proposed solutions to the problem. Current state-of-the-art methods are based on passage retrieval or question answering approaches and are limited to generating spoilers only in the form of a phrase or a passage. In this work, we propose an ensemble of fine-tuned large language models for clickbait spoiler generation. Our approach is not limited to phrase or passage spoilers, but is also able to generate multipart spoilers that refer to several non-consecutive parts of text. Experimental evaluation demonstrates that the proposed ensemble model outperforms the baselines in terms of BLEU, METEOR and BERTScore metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.33.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.33.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--33" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.33" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.33.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.33/">Reducing named entity hallucination risk to ensure faithful summary generation</a></strong><br><a href="/people/e/eunice-akani/">Eunice Akani</a>
|
<a href="/people/b/benoit-favre/">Benoit Favre</a>
|
<a href="/people/f/frederic-bechet/">Frederic Bechet</a>
|
<a href="/people/r/romain-gemignani/">Romain Gemignani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--33"><div class="card-body p-3 small">The faithfulness of abstractive text summarization at the named entities level is the focus of this study. We propose to add a new criterion to the summary selection method based on the “risk” of generating entities that do not belong to the source document. This method is based on the assumption that Out-Of-Document entities are more likely to be hallucinations. This assumption was verified by a manual annotation of the entities occurring in a set of generated summaries on the CNN/DM corpus. This study showed that only 29% of the entities outside the source document were inferrable by the annotators, leading to 71% of hallucinations among OOD entities. We test our selection method on the CNN/DM corpus and show that it significantly reduces the hallucination risk on named entities while maintaining competitive results with respect to automatic evaluation metrics like ROUGE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.34.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.34.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--34" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.34" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-main.34.Supplementary_Attachment.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.34/">Building a dual dataset of text- and image-grounded conversations and summarisation in Gàidhlig (<span class="acl-fixed-case">S</span>cottish <span class="acl-fixed-case">G</span>aelic)</a></strong><br><a href="/people/d/david-m-howcroft/">David M. Howcroft</a>
|
<a href="/people/w/william-lamb/">William Lamb</a>
|
<a href="/people/a/anna-groundwater/">Anna Groundwater</a>
|
<a href="/people/d/dimitra-gkatzia/">Dimitra Gkatzia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--34"><div class="card-body p-3 small">Gàidhlig (Scottish Gaelic; gd) is spoken by about 57k people in Scotland, but remains an under-resourced language with respect to natural language processing in general and natural language generation (NLG) in particular. To address this gap, we developed the first datasets for Scottish Gaelic NLG, collecting both conversational and summarisation data in a single setting. Our task setup involves dialogues between a pair of speakers discussing museum exhibits, grounding the conversation in images and texts. Then, both interlocutors summarise the dialogue resulting in a secondary dialogue summarisation dataset. This paper presents the dialogue and summarisation corpora, as well as the software used for data collection. The corpus consists of 43 conversations (13.7k words) and 61 summaries (2.0k words), and will be released along with the data collection interface.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.35.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.35.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--35" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.35" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.35/">Generating Multiple Questions from Presentation Transcripts: A Pilot Study on Earnings Conference Calls</a></strong><br><a href="/people/y/yining-juan/">Yining Juan</a>
|
<a href="/people/c/chung-chi-chen/">Chung-Chi Chen</a>
|
<a href="/people/h/hen-hsen-huang/">Hen-Hsen Huang</a>
|
<a href="/people/h/hsin-hsi-chen/">Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--35"><div class="card-body p-3 small">In various scenarios, such as conference oral presentations, company managers’ talks, and politicians’ speeches, individuals often contemplate the potential questions that may arise from their presentations. This common practice prompts the research question addressed in this study: to what extent can models generate multiple questions based on a given presentation transcript? To investigate this, we conduct pilot explorations using earnings conference call transcripts, which serve as regular meetings between professional investors and company managers. We experiment with different task settings and methods and evaluate the results from various perspectives. Our findings highlight that incorporating key points retrieval techniques enhances the accuracy and diversity of the generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-main.36.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-main.36.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-main--36" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-main.36" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-main.36/">Mod-<span class="acl-fixed-case">D</span>2<span class="acl-fixed-case">T</span>: A Multi-layer Dataset for Modular Data-to-Text Generation</a></strong><br><a href="/people/s/simon-mille/">Simon Mille</a>
|
<a href="/people/f/francois-lareau/">Francois Lareau</a>
|
<a href="/people/s/stamatia-dasiopoulou/">Stamatia Dasiopoulou</a>
|
<a href="/people/a/anja-belz/">Anya Belz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-main--36"><div class="card-body p-3 small">Rule-based text generators lack the coverage and fluency of their neural counterparts, but have two big advantages over them: (i) they are entirely controllable and do not hallucinate; and (ii) they can fully explain how an output was generated from an input. In this paper we leverage these two advantages to create large and reliable synthetic datasets with multiple human-intelligible intermediate representations. We present the Modular Data-to-Text (Mod-D2T) Dataset which incorporates ten intermediate-level representations between input triple sets and output text; the mappings from one level to the next can broadly be interpreted as the traditional modular tasks of an NLG pipeline. We describe the Mod-D2T dataset, evaluate its quality via manual validation and discuss its applications and limitations. Data, code and documentation are available at https://github.com/mille-s/Mod-D2T.</div></div></div><hr><div id="2023inlg-demos"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-demos.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.inlg-demos.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.inlg-demos/">Proceedings of the 16th International Natural Language Generation Conference: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-demos.0.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-demos.0.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-demos.0/">Proceedings of the 16th International Natural Language Generation Conference: System Demonstrations</a></strong><br><a href="/people/c/c-maria-keet/">C. Maria Keet</a>
|
<a href="/people/h/hung-yi-lee/">Hung-Yi Lee</a>
|
<a href="/people/s/sina-zarriess/">Sina Zarrieß</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-demos.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-demos.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-demos--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-demos.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://aclanthology.org/attachments/2023.inlg-demos.1.Supplementary_Attachment.zip" data-toggle="tooltip" data-placement="top" title="" data-original-title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class="d-block"><strong><a class="align-middle" href="/2023.inlg-demos.1/">Overview of <span class="acl-fixed-case">M</span>i<span class="acl-fixed-case">R</span>eportor: Generating Reports for Multimodal Medical Images</a></strong><br><a href="/people/x/xuwen-wang/">Xuwen Wang</a>
|
<a href="/people/h/hetong-ma/">Hetong Ma</a>
|
<a href="/people/z/zhen-guo/">Zhen Guo</a>
|
<a href="/people/j/jiao-li/">Jiao Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-demos--1"><div class="card-body p-3 small">This demo paper presents a brief introduction of MiReportor, a computer-aided medical imaging report generator, which leverages a unified framework of medical image understanding and generation to predict readable descriptions for medical images, and assists radiologists in imaging reports writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-demos.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-demos.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-demos--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-demos.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-demos.2/">enunlg: a Python library for reproducible neural data-to-text experimentation</a></strong><br><a href="/people/d/david-m-howcroft/">David M. Howcroft</a>
|
<a href="/people/d/dimitra-gkatzia/">Dimitra Gkatzia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-demos--2"><div class="card-body p-3 small">Over the past decade, a variety of neural architectures for data-to-text generation (NLG) have been proposed. However, each system typically has its own approach to pre- and post-processing and other implementation details. Diversity in implementations is desirable, but it also confounds attempts to compare model performance: are the differences due to the proposed architectures or are they a byproduct of the libraries used or a result of pre- and post-processing decisions made? To improve reproducibility, we re-implement several pre-Transformer neural models for data-to-text NLG within a single framework to facilitate direct comparisons of the models themselves and better understand the contributions of other design choices. We release our library at https://github.com/NapierNLP/enunlg to serve as a baseline for ongoing work in this area including research on NLG for low-resource languages where transformers might not be optimal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-demos.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-demos.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-demos--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-demos.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-demos.3/"><span class="acl-fixed-case">V</span>isua<span class="acl-fixed-case">LLM</span>: Easy Web-based Visualization for Neural Language Generation</a></strong><br><a href="/people/f/frantisek-trebuna/">František Trebuňa</a>
|
<a href="/people/o/ondrej-dusek/">Ondrej Dusek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-demos--3"><div class="card-body p-3 small">VisuaLLM is a Python library that enables interactive visualization of common tasks in natural language generation with pretrained language models (using HuggingFace’s model API), with tight integration of benchmark datasets and fine-grained generation control. The system runs as a local generation backend server and features a web-based frontend, allowing simple interface configuration by minimal Python code. The currently implemented views include data visualization, next-token prediction with probability distributions, and decoding parameter control, with simple extension to additional tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-demos.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-demos.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-demos--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-demos.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-demos.4/">Audio Commentary System for Real-Time Racing Game Play</a></strong><br><a href="/people/t/tatsuya-ishigaki/">Tatsuya Ishigaki</a>
|
<a href="/people/g/goran-topic/">Goran Topić</a>
|
<a href="/people/y/yumi-hamazono/">Yumi Hamazono</a>
|
<a href="/people/i/ichiro-kobayashi/">Ichiro Kobayashi</a>
|
<a href="/people/y/yusuke-miyao/">Yusuke Miyao</a>
|
<a href="/people/h/hiroya-takamura/">Hiroya Takamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-demos--4"><div class="card-body p-3 small">Live commentaries are essential for enhancing spectators’ enjoyment and understanding during sports events or e-sports streams. We introduce a live audio commentator system designed specifically for a racing game, driven by the high demand in the e-sports field. While a player is playing a racing game, our system tracks real-time user play data including speed and steer rotations, and generates commentary to accompany the live stream. Human evaluation suggested that generated commentary enhances enjoyment and understanding of races compared to streams without commentary. Incorporating additional modules to improve diversity and detect irregular events, such as course-outs and collisions, further increases the preference for the output commentaries.</div></div></div><hr><div id="2023inlg-genchal"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.inlg-genchal.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.inlg-genchal/">Proceedings of the 16th International Natural Language Generation Conference: Generation Challenges</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.0.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.0.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.0/">Proceedings of the 16th International Natural Language Generation Conference: Generation Challenges</a></strong><br><a href="/people/s/simon-mille/">Simon Mille</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.1/"><span class="acl-fixed-case">LOWRECORP</span>: the Low-Resource <span class="acl-fixed-case">NLG</span> Corpus Building Challenge</a></strong><br><a href="/people/k/khyathi-raghavi-chandu/">Khyathi Raghavi Chandu</a>
|
<a href="/people/d/david-m-howcroft/">David M. Howcroft</a>
|
<a href="/people/d/dimitra-gkatzia/">Dimitra Gkatzia</a>
|
<a href="/people/y/yi-ling-chung/">Yi-Ling Chung</a>
|
<a href="/people/y/yufang-hou/">Yufang Hou</a>
|
<a href="/people/c/chris-chinenye-emezue/">Chris Chinenye Emezue</a>
|
<a href="/people/p/pawan-rajpoot/">Pawan Rajpoot</a>
|
<a href="/people/t/tosin-adewumi/">Tosin Adewumi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--1"><div class="card-body p-3 small">Most languages in the world do not have sufficient data available to develop neural-network-based natural language generation (NLG) systems. To alleviate this resource scarcity, we propose a novel challenge for the NLG community: low-resource language corpus development (LOWRECORP). We present an innovative framework to collect a single dataset with dual tasks to maximize the efficiency of data collection efforts and respect language consultant time. Specifically, we focus on a text-chat-based interface for two generation tasks – conversational response generation grounded in a source document and/or image and dialogue summarization (from the former task). The goal of this shared task is to collectively develop grounded datasets for local and low-resourced languages. To enable data collection, we make available web-based software that can be used to collect these grounded conversations and summaries. Submissions will be assessed for the size, complexity, and diversity of the corpora to ensure quality control of the datasets as well as any enhancements to the interface or novel approaches to grounding conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.2/">Long Story Generation Challenge</a></strong><br><a href="/people/n/nikolay-mikhaylovskiy/">Nikolay Mikhaylovskiy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--2"><div class="card-body p-3 small">We propose a shared task of human-like long story generation, LSG Challenge, that asks models to output a consistent human-like long story (a Harry Potter generic audience fanfic in English), given a prompt of about 1K tokens. We suggest a novel statistical metric of the text structuredness, GloVe Autocorrelations Power/ Exponential Law Mean Absolute Percentage Error Ratio (GAPELMAPER) and the use of previously-known UNION metric and a human evaluation protocol. We hope that LSG can open new avenues for researchers to investigate sampling approaches, prompting strategies, autoregressive and non-autoregressive text generation architectures and break the barrier to generate consistent long (40K+ word) texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.3/">Visually Grounded Story Generation Challenge</a></strong><br><a href="/people/x/xudong-hong/">Xudong Hong</a>
|
<a href="/people/k/khushboo-mehra/">Khushboo Mehra</a>
|
<a href="/people/a/asad-sayeed/">Asad Sayeed</a>
|
<a href="/people/v/vera-demberg/">Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--3"><div class="card-body p-3 small">Recent large pre-trained models have achieved strong performance in multimodal language generation, which requires a joint effort of vision and language modeling. However, most previous generation tasks are based on single image input and produce short text descriptions that are not grounded on the input images. In this work, we propose a shared task on visually grounded story generation. The input is an image sequence, and the output is a story that is conditioned on the input images. This task is particularly challenging because: 1) the protagonists in the generated stories need to be grounded in the images and 2) the output story should be a coherent long-form text. We aim to advance the study of vision-based story generation by accepting submissions that propose new methods as well as new evaluation measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.4/">The <span class="acl-fixed-case">VDG</span> Challenge: Response Generation and Evaluation in Collaborative Visual Dialogue</a></strong><br><a href="/people/n/nikolai-ilinykh/">Nikolai Ilinykh</a>
|
<a href="/people/s/simon-dobnik/">Simon Dobnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--4"><div class="card-body p-3 small">We propose the VDG Challenge: a shared task that addresses and benchmarks the task of utterance generation in collaborative visual dialogue. The task features two challenging datasets, an evaluation protocol and a tentative schedule. Our shared task will allow researchers to unravel problems of modelling multi-modal interaction and fit of the existing approaches in the NLP and NLG communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.5/">Identifying Feedback Types to Augment Feedback Comment Generation</a></strong><br><a href="/people/m/maja-stahl/">Maja Stahl</a>
|
<a href="/people/h/henning-wachsmuth/">Henning Wachsmuth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--5"><div class="card-body p-3 small">In the context of language learning, feedback comment generation is the task of generating hints or explanatory notes for learner texts that help understand why a part of text is erroneous. This paper presents our approach to the Feedback Comment Generation Shared Task, collocated with the 16th International Natural Language Generation Conference (INLG 2023). The approach augments the generation of feedback comments by a self-supervised identification of feedback types in a multitasklearning setting. Within the shared task, other approaches performed more effective, yet the combined modeling of feedback type classification and feedback comment generation is superior to performing eedback generation only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.6.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.6.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--6" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.6" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.6/">Error syntax aware augmentation of feedback comment generation dataset</a></strong><br><a href="/people/n/nikolay-babakov/">Nikolay Babakov</a>
|
<a href="/people/m/maria-lysyuk/">Maria Lysyuk</a>
|
<a href="/people/a/alexander-shvets/">Alexander Shvets</a>
|
<a href="/people/l/lilya-kazakova/">Lilya Kazakova</a>
|
<a href="/people/a/alexander-panchenko/">Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--6"><div class="card-body p-3 small">This paper presents a solution to the GenChal 2022 shared task dedicated to feedback comment generation for writing learning. In terms of this task given a text with an error and a span of the error, a system generates an explanatory note that helps the writer (language learner) to improve their writing skills. Our solution is based on fine-tuning the T5 model on the initial dataset augmented according to syntactical dependencies of the words located within indicated error span. The solution of our team ‘nigula’ obtained second place according to manual evaluation by the organizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.7.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.7.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--7" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.7" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.7/">A Report on <span class="acl-fixed-case">FCG</span> <span class="acl-fixed-case">G</span>en<span class="acl-fixed-case">C</span>hal 2022: Shared Task on Feedback Comment Generation for Language Learners</a></strong><br><a href="/people/r/ryo-nagata/">Ryo Nagata</a>
|
<a href="/people/m/masato-hagiwara/">Masato Hagiwara</a>
|
<a href="/people/k/kazuaki-hanawa/">Kazuaki Hanawa</a>
|
<a href="/people/m/masato-mita/">Masato Mita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--7"><div class="card-body p-3 small">We report on the results of the first ever shared task on feedback comment generation for language learners held as Generation Challenge (GenChal) in INLG 2022, which we call FCG GenChal. Feedback comment generation for language learners is a task where, given a text and a span, a system generates, for the span, an explanatory note that helps the writer (language learner) improve their writing skills. We show how well we can generate feedback comments with present techniques. We also shed light on the task properties and the difficulties in this task, with insights into the task including data development, evaluation, and comparisons of generation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.8.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.8.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--8" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.8" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.8/">Sentence-level Feedback Generation for <span class="acl-fixed-case">E</span>nglish Language Learners: Does Data Augmentation Help?</a></strong><br><a href="/people/s/shabnam-behzad/">Shabnam Behzad</a>
|
<a href="/people/a/amir-zeldes/">Amir Zeldes</a>
|
<a href="/people/n/nathan-schneider/">Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--8"><div class="card-body p-3 small">In this paper, we present strong baselines for the task of Feedback Comment Generation for Writing Learning. Given a sentence and an error span, the task is to generate a feedback comment explaining the error. Sentences and feedback comments are both in English. We experiment with LLMs and also create multiple pseudo datasets for the task, investigating how it affects the performance of our system. We present our results for the task along with extensive analysis of the generated comments with the aim of aiding future studies in feedback comment generation for English language learners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.9.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.9.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--9" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.9" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.9/">Retrieval, Masking, and Generation: Feedback Comment Generation using Masked Comment Examples</a></strong><br><a href="/people/m/mana-ihori/">Mana Ihori</a>
|
<a href="/people/h/hiroshi-sato/">Hiroshi Sato</a>
|
<a href="/people/t/tomohiro-tanaka/">Tomohiro Tanaka</a>
|
<a href="/people/r/ryo-masumura/">Ryo Masumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--9"><div class="card-body p-3 small">In this paper, we propose a novel method, retrieval, masking, and generation, for feedback comment generation. Feedback comment generation is a task in which a system generates feedback comments such as hints or explanatory notes for language learners, given input text and position showing where to comment. In the conventional study, the retrieve-and-edit method for retrieving feedback comments in the data pool and editing the comments has been thought effective for this task. However, the performance of this method does not perform as well as other conventional methods because its model learns to edit tokens that do not need to be rewritten in the retrieved comments. To mitigate this problem, we propose a method for combining retrieval, masking, and generation based on the retrieve-and-edit method. Specifically, tokens of feedback comments retrieved from the data pool are masked, and this masked feedback comment is used as a template to generate feedback comments. The proposed method should prevent unnecessary conversion by using not retrieved feedback comments directly but masking them. Our experiments on feedback comment generation demonstrate that the proposed method outperforms conventional methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.10.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.10.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--10" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.10" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.10/"><span class="acl-fixed-case">TMU</span> Feedback Comment Generation System Using Pretrained Sequence-to-Sequence Language Models</a></strong><br><a href="/people/n/naoya-ueda/">Naoya Ueda</a>
|
<a href="/people/m/mamoru-komachi/">Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--10"><div class="card-body p-3 small">In this paper, we introduce our Tokyo Metropolitan University Feedback Comment Generation system submitted to the feedback comment generation task for INLG 2023 Generation Challenge. In this task, a source sentence and offset range of preposition uses are given as the input. Then, a system generates hints or explanatory notes about preposition uses as the output. To tackle this generation task, we finetuned pretrained sequence-to-sequence language models. The models using BART and T5 showed significant improvement in BLEU score, demonstrating the effectiveness of the pretrained sequence-to-sequence language models in this task. We found that using part-of-speech tag information as an auxiliary input improves the generation quality of feedback comments. Furthermore, we adopt a simple postprocessing method that can enhance the reliability of the generation. As a result, our system achieved the F1 score of 47.4 points in BLEU-based evaluation and 60.9 points in manual evaluation, which ranked second and third on the leaderboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.11.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.11.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--11" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.11" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.11/">The <span class="acl-fixed-case">T</span>okyo Tech and <span class="acl-fixed-case">AIST</span> System at the <span class="acl-fixed-case">G</span>en<span class="acl-fixed-case">C</span>hal 2022 Shared Task on Feedback Comment Generation</a></strong><br><a href="/people/s/shota-koyama/">Shota Koyama</a>
|
<a href="/people/h/hiroya-takamura/">Hiroya Takamura</a>
|
<a href="/people/n/naoaki-okazaki/">Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--11"><div class="card-body p-3 small">This paper describes the Tokyo Tech and AIST system in the GenChal 2022 shared task, which is the first shared task of feedback comment generation. We adopted five methods: data cleaning, fine-tuning pre-trained models, correcting errors in learners’ sentences, appending a correcting operation, and filtering out irrelevant outputs. Our system achieved F1 = 43.4 on the test dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.12.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.12.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--12" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.12" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.12/">Feedback comment generation using predicted grammatical terms</a></strong><br><a href="/people/k/kunitaka-jimichi/">Kunitaka Jimichi</a>
|
<a href="/people/k/kotaro-funakoshi/">Kotaro Funakoshi</a>
|
<a href="/people/m/manabu-okumura/">Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--12"><div class="card-body p-3 small">The purpose of feedback comment generation is to provide useful feedback comments for a wide range of errors in learners’ essays from a language learning perspective. Since it is difficult to obtain appropriate comments at a practical level with rule-based or retrieval- based methods, we explore neural-based gen- erative methods with pre-trained models. We further assume the effectiveness of consider- ing grammatical terms in generating feedback comments. Specifically, this paper proposes T5-based models using predicted grammati- cal terms, submitted to FCG GenChal, and presents their results. By using correct gram- matical terms, our model could improve the BLEU score by 19.0 points, compared with the baseline T5 without grammatical terms on the development dataset. Furthermore, by using predicted grammatical terms, our model could improve the manual evaluation score by 2.33 points, compared with the baseline T5 without grammatical terms on the test dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.13.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.13.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--13" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.13" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.13/"><span class="acl-fixed-case">AIW</span>olf<span class="acl-fixed-case">D</span>ial 2023: Summary of Natural Language Division of 5th International <span class="acl-fixed-case">AIW</span>olf Contest</a></strong><br><a href="/people/y/yoshinobu-kano/">Yoshinobu Kano</a>
|
<a href="/people/n/neo-watanabe/">Neo Watanabe</a>
|
<a href="/people/k/kaito-kagaminuma/">Kaito Kagaminuma</a>
|
<a href="/people/c/claus-aranha/">Claus Aranha</a>
|
<a href="/people/j/jaewon-lee/">Jaewon Lee</a>
|
<a href="/people/b/benedek-hauer/">Benedek Hauer</a>
|
<a href="/people/h/hisaichi-shibata/">Hisaichi Shibata</a>
|
<a href="/people/s/soichiro-miki/">Soichiro Miki</a>
|
<a href="/people/y/yuta-nakamura/">Yuta Nakamura</a>
|
<a href="/people/t/takuya-okubo/">Takuya Okubo</a>
|
<a href="/people/s/soga-shigemura/">Soga Shigemura</a>
|
<a href="/people/r/rei-ito/">Rei Ito</a>
|
<a href="/people/k/kazuki-takashima/">Kazuki Takashima</a>
|
<a href="/people/t/tomoki-fukuda/">Tomoki Fukuda</a>
|
<a href="/people/m/masahiro-wakutani/">Masahiro Wakutani</a>
|
<a href="/people/t/tomoya-hatanaka/">Tomoya Hatanaka</a>
|
<a href="/people/m/mami-uchida/">Mami Uchida</a>
|
<a href="/people/m/mikio-abe/">Mikio Abe</a>
|
<a href="/people/a/akihiro-mikami/">Akihiro Mikami</a>
|
<a href="/people/t/takashi-otsuki/">Takashi Otsuki</a>
|
<a href="/people/z/zhiyang-qi/">Zhiyang Qi</a>
|
<a href="/people/k/kei-harada/">Kei Harada</a>
|
<a href="/people/m/michimasa-inaba/">Michimasa Inaba</a>
|
<a href="/people/d/daisuke-katagami/">Daisuke Katagami</a>
|
<a href="/people/h/hirotaka-osawa/">Hirotaka Osawa</a>
|
<a href="/people/f/fujio-toriumi/">Fujio Toriumi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--13"><div class="card-body p-3 small">We held our 5th annual AIWolf international contest to automatically play the Werewolf game “Mafia”, where players try finding liars via conversations, aiming at promoting developments in creating agents of more natural conversations in higher level, such as longer contexts, personal relationships, semantics, pragmatics, and logics, revealing the capabilities and limits of the generative AIs. In our Natural Language Division of the contest, we had six Japanese speaking agents from five teams, and three English speaking agents, to mutually run games. By using the game logs, We performed human subjective evaluations and detailed log analysis. We found that the entire system performance has largely improved over the previous year, due to the recent advantages of the LLMs. However, it is not perfect at all yet; the generated talks are sometimes inconsistent with the game actions, it is still doubtful that the agents could infer roles by logics rather than superficial utterance generations. It is not explicitly observed in this log but it would be still difficult to make an agent telling a lie, pretend as a villager but it has an opposite goal inside. Our future work includes to reveal the capability of the LLMs, whether they can make the duality of the “liar”, in other words, holding a “true” and a “false” circumstances of the agent at the same time, even holding what these circumstances look like from other agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.14.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.14.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--14" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.14" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.14/">Team Zoom @ <span class="acl-fixed-case">A</span>uto<span class="acl-fixed-case">M</span>in 2023: Utilizing Topic Segmentation And <span class="acl-fixed-case">LLM</span> Data Augmentation For Long-Form Meeting Summarization</a></strong><br><a href="/people/f/felix-schneider/">Felix Schneider</a>
|
<a href="/people/m/marco-turchi/">Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--14"><div class="card-body p-3 small">This paper describes Zoom’s submission to the Second Shared Task on Automatic Minuting at INLG 2023. We participated in Task A: generating abstractive summaries of meetings. Our final submission was a transformer model utilizing data from a similar domain and data augmentation by large language models, as well as content-based segmentation. The model produces summaries covering meeting topics and next steps and performs comparably to a large language model at a fraction of the cost. We also find that re-summarizing the summaries with the same model allows for an alternative, shorter summary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.15.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.15.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--15" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.15" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.15/">Team Synapse @ <span class="acl-fixed-case">A</span>uto<span class="acl-fixed-case">M</span>in 2023: Leveraging <span class="acl-fixed-case">BART</span>-Based Models for Automatic Meeting Minuting</a></strong><br><a href="/people/k/kristyna-klesnilova/">Kristýna Klesnilová</a>
|
<a href="/people/m/michelle-elizabeth/">Michelle Elizabeth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--15"><div class="card-body p-3 small">This paper describes the approach we followed for our submission to the Second Run of the Automatic Minuting Shared Task. Our methodology centers around employing BART-based models fine-tuned on diverse summarization corpora. The segmented meeting transcripts are fed into the models, generating summaries that are subsequently combined and formatted into the final meeting minutes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.16.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.16.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--16" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.16" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.16/">Team Iterate @ <span class="acl-fixed-case">A</span>uto<span class="acl-fixed-case">M</span>in 2023 - Experiments with Iterative Minuting</a></strong><br><a href="/people/f/frantisek-kmjec/">František Kmječ</a>
|
<a href="/people/o/ondrej-bojar/">Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--16"><div class="card-body p-3 small">This report describes the development of our system for automatic minuting created for the AutoMin 2023 Task A. As a baseline, we utilize a system based on the BART encoder-decoder model paired with a preprocessing pipeline similar to the one introduced by the winning solutions at AutoMin 2021. We then further explore the possibilities for iterative summarization by constructing an iterative minuting dataset from the provided data, finetuning on it and feeding the model previously generated minutes. We also experiment with adding more context by utilizing the Longformer encoder-decoder model and finetuning it on the SAMSum dataset. Our submitted solution is of the baseline approach, since we were unable to match its performance with our iterative variants. With the baseline, we achieve a ROUGE-1 score of 0.368 on the ELITR minuting corpus development set. We finally explore the performance of Vicuna 13B quantized language model for summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.17.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.17.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--17" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.17" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.17/">Darbarer @ <span class="acl-fixed-case">A</span>uto<span class="acl-fixed-case">M</span>in2023: Transcription simplification for concise minute generation from multi-party conversations</a></strong><br><a href="/people/i/ismael-rousseau/">Ismaël Rousseau</a>
|
<a href="/people/l/loic-fosse/">Loïc Fosse</a>
|
<a href="/people/y/youness-dkhissi/">Youness Dkhissi</a>
|
<a href="/people/g/geraldine-damnati/">Geraldine Damnati</a>
|
<a href="/people/g/gwenole-lecorve/">Gwénolé Lecorvé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--17"><div class="card-body p-3 small">This document reports the approach of our team Darbarer for the main task (Task A) of the AutoMin 2023 challenge. Our system is composed of four main modules. The first module relies on a text simplification model aiming at standardizing the utterances of the conversation and compressing the input in order to focus on informative content. The second module handles summarization by employing a straightforward segmentation strategy and a fine-tuned BART-based generative model. Then a titling module has been trained in order to propose a short description of each summarized block. Lastly, we apply a post-processing step aimed at enhancing readability through specific formatting rules. Our contributions lie in the first, third and last steps. Our system generates precise and concise minutes. We provide a detailed description of our modules, discuss the difficulty of evaluating their impact and propose an analysis of observed errors in our generated minutes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.18.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.18.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--18" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.18" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.18/">Team <span class="acl-fixed-case">NTR</span> @ <span class="acl-fixed-case">A</span>uto<span class="acl-fixed-case">M</span>in 2023: Dolly <span class="acl-fixed-case">LLM</span> Improves Minuting Performance, Semantic Segmentation Doesn’t</a></strong><br><a href="/people/e/eugene-borisov/">Eugene Borisov</a>
|
<a href="/people/n/nikolay-mikhaylovskiy/">Nikolay Mikhaylovskiy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--18"><div class="card-body p-3 small">This paper documents the approach of Team NTR for the Second Shared Task on Automatic Minuting (AutoMin) at INLG 2023. The goal of this work is to develop a module for automatic generation of meeting minutes based on a meeting transcript text produced by an Automated Speech Recognition (ASR) system (Task A). We consider minuting as a supervised machine learning task on pairs of texts: the transcript of the meeting and its minutes. We use a two-staged minuting pipeline that consists of segmentation and summarization. We experiment with semantic segmentation and multi-language approaches and Large Language Model Dolly, and achieve Rouge1-F of 0.2455 and BERT-Score of 0.8063 on the English part of ELITR test set and Rouge1-F of 0.2430 and BERT-Score of 0.8332 on the EuroParl dev set with the submitted Naive Segmentation + Dolly7b pipeline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.inlg-genchal.19.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.inlg-genchal.19.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--inlg-genchal--19" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.inlg-genchal.19" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.inlg-genchal.19/">Overview of the Second Shared Task on Automatic Minuting (<span class="acl-fixed-case">A</span>uto<span class="acl-fixed-case">M</span>in) at <span class="acl-fixed-case">INLG</span> 2023</a></strong><br><a href="/people/t/tirthankar-ghosal/">Tirthankar Ghosal</a>
|
<a href="/people/o/ondrej-bojar/">Ondřej Bojar</a>
|
<a href="/people/m/marie-hledikova/">Marie Hledíková</a>
|
<a href="/people/t/tom-kocmi/">Tom Kocmi</a>
|
<a href="/people/a/anna-nedoluzhko/">Anna Nedoluzhko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--inlg-genchal--19"><div class="card-body p-3 small">In this article, we report the findings of the second shared task on Automatic Minuting (AutoMin) held as a Generation Challenge at the 16th International Natural Language Generation (INLG) Conference 2023. The second Automatic Minuting shared task is a successor to the first AutoMin which took place in 2021. The primary objective of the AutoMin shared task is to garner participation of the speech and natural language processing and generation community to create automatic methods for generating minutes from multi-party meetings. Five teams from diverse backgrounds participated in the shared task this year. A lot has changed in the Generative AI landscape since the last AutoMin especially with the emergence and wide adoption of Large Language Models (LLMs) to different downstream tasks. Most of the contributions are based on some form of an LLM and we are also adding current outputs of GPT4 as a benchmark. Furthermore, we examine the applicability of GPT-4 for automatic scoring of minutes. Compared to the previous instance of AutoMin, we also add another domain, the minutes for EU Parliament sessions, and we experiment with a more fine-grained manual evaluation. More details on the event can be found at https://ufal.github.io/automin-2023/.</div></div></div><hr><div id="2023sigdial-1"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.sigdial-1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.sigdial-1/">Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.0.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.0.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.0/">Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></strong><br><a href="/people/s/svetlana-stoyanchev/">Svetlana Stoyanchev</a>
|
<a href="/people/s/shafiq-joty/">Shafiq Joty</a>
|
<a href="/people/d/david-schlangen/">David Schlangen</a>
|
<a href="/people/o/ondrej-dusek/">Ondrej Dusek</a>
|
<a href="/people/c/casey-kennington/">Casey Kennington</a>
|
<a href="/people/m/malihe-alikhani/">Malihe Alikhani</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.1/">Sources of Noise in Dialogue and How to Deal with Them</a></strong><br><a href="/people/d/derek-chen/">Derek Chen</a>
|
<a href="/people/z/zhou-yu/">Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--1"><div class="card-body p-3 small">Training dialogue systems often entails dealing with noisy training examples and unexpected user inputs. Despite their prevalence, there currently lacks an accurate survey of dialogue noise, nor is there a clear sense of the impact of each noise type on task performance. This paper addresses this gap by first constructing a taxonomy of noise encountered by dialogue systems. In addition, we run a series of experiments to show how different models behave when subjected to varying levels of noise and types of noise. Our results reveal that models are quite robust to label errors commonly tackled by existing denoising algorithms, but that performance suffers from dialogue-specific noise. Driven by these observations, we design a data cleaning algorithm specialized for conversational settings and apply it as a proof-of-concept for targeted dialogue denoising.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.2/">Investigating Explicitation of Discourse Connectives in Translation using Automatic Annotations</a></strong><br><a href="/people/f/frances-yung/">Frances Yung</a>
|
<a href="/people/m/merel-scholman/">Merel Scholman</a>
|
<a href="/people/e/ekaterina-lapshinova-koltunski/">Ekaterina Lapshinova-Koltunski</a>
|
<a href="/people/c/christina-pollklasener/">Christina Pollkläsener</a>
|
<a href="/people/v/vera-demberg/">Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--2"><div class="card-body p-3 small">Discourse relations have different patterns of marking across different languages. As a result, discourse connectives are often added, omitted, or rephrased in translation. Prior work has shown a tendency for explicitation of discourse connectives, but such work was conducted using restricted sample sizes due to difficulty of connective identification and alignment. The current study exploits automatic methods to facilitate a large-scale study of connectives in English and German parallel texts. Our results based on over 300 types and 18000 instances of aligned connectives and an empirical approach to compare the cross-lingual specificity gap provide strong evidence of the Explicitation Hypothesis. We conclude that discourse relations are indeed more explicit in translation than texts written originally in the same language. Automatic annotations allow us to carry out translation studies of discourse relations on a large scale. Our methodology using relative entropy to study the specificity of connectives also provides more fine-grained insights into translation patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.3/">What’s Hard in <span class="acl-fixed-case">E</span>nglish <span class="acl-fixed-case">RST</span> Parsing? Predictive Models for Error Analysis</a></strong><br><a href="/people/y/yang-janet-liu/">Yang Janet Liu</a>
|
<a href="/people/t/tatsuya-aoyama/">Tatsuya Aoyama</a>
|
<a href="/people/a/amir-zeldes/">Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--3"><div class="card-body p-3 small">Despite recent advances in Natural Language Processing (NLP), hierarchical discourse parsing in the framework of Rhetorical Structure Theory remains challenging, and our understanding of the reasons for this are as yet limited. In this paper, we examine and model some of the factors associated with parsing difficulties in previous work: the existence of implicit discourse relations, challenges in identifying long-distance relations, out-of-vocabulary items, and more. In order to assess the relative importance of these variables, we also release two annotated English test-sets with explicit correct and distracting discourse markers associated with gold standard RST relations. Our results show that as in shallow discourse parsing, the explicit/implicit distinction plays a role, but that long-distance dependencies are the main challenge, while lack of lexical overlap is less of a problem, at least for in-domain parsing. Our final model is able to predict where errors will occur with an accuracy of 76.3% for the bottom-up parser and 76.6% for the top-down parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.4/">Grounded Complex Task Segmentation for Conversational Assistants</a></strong><br><a href="/people/r/rafael-ferreira/">Rafael Ferreira</a>
|
<a href="/people/d/david-semedo/">David Semedo</a>
|
<a href="/people/j/joao-magalhaes/">Joao Magalhaes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--4"><div class="card-body p-3 small">Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions. Hence, when conversational assistants walk users through the steps of complex tasks, there is a need to structure the task into manageable pieces of information of the right length and complexity. In this paper, we tackle the recipes domain and convert reading structured instructions into conversational structured ones. We annotated the structure of instructions according to a conversational scenario, which provided insights into what is expected in this setting. To computationally model the conversational step’s characteristics, we tested various Transformer-based architectures, showing that a token-based approach delivers the best results. A further user study showed that users tend to favor steps of manageable complexity and length, and that the proposed methodology can improve the original web-based instructional text. Specifically, 86% of the evaluated tasks were improved from a conversational suitability point of view.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.5/">A Statistical Approach for Quantifying Group Difference in Topic Distributions Using Clinical Discourse Samples</a></strong><br><a href="/people/g/grace-o-lawley/">Grace O. Lawley</a>
|
<a href="/people/p/peter-a-heeman/">Peter A. Heeman</a>
|
<a href="/people/j/jill-k-dolata/">Jill K. Dolata</a>
|
<a href="/people/e/eric-fombonne/">Eric Fombonne</a>
|
<a href="/people/s/steven-bedrick/">Steven Bedrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--5"><div class="card-body p-3 small">Topic distribution matrices created by topic models are typically used for document classification or as features in a separate machine learning algorithm. Existing methods for evaluating these topic distributions include metrics such as coherence and perplexity; however, there is a lack of statistically grounded evaluation tools. We present a statistical method for investigating group differences in the document-topic distribution vectors created by Latent Dirichlet Allocation (LDA) that uses Aitchison geometry to transform the vectors, multivariate analysis of variance (MANOVA) to compare sample means, and partial eta squared to calculate effect size. Using a corpus of dialogues between Autistic and Typically Developing (TD) children and trained examiners, we found that the topic distributions of Autistic children differed from those of TD children when responding to questions about social difficulties (p = .0083, partial eta squared = .19). Furthermore, the examiners’ topic distributions differed between the Autistic and TD groups when discussing emotions (p = .0035, partial eta squared = .20), social difficulties (p &lt; .001, partial eta squared = .30), and friends (p = .0224, partial eta squared = .17). These results support the use of topic modeling in studying clinically relevant features of social communication such as topic maintenance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.6.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.6.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--6" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.6" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.6/"><span class="acl-fixed-case">O</span>pinion<span class="acl-fixed-case">C</span>onv: Conversational Product Search with Grounded Opinions</a></strong><br><a href="/people/v/vahid-sadiri-javadi/">Vahid Sadiri Javadi</a>
|
<a href="/people/m/martin-potthast/">Martin Potthast</a>
|
<a href="/people/l/lucie-flek/">Lucie Flek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--6"><div class="card-body p-3 small">When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision making.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.7.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.7.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--7" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.7" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.7/">Dial-<span class="acl-fixed-case">M</span>: A Masking-based Framework for Dialogue Evaluation</a></strong><br><a href="/people/s/suvodip-dey/">Suvodip Dey</a>
|
<a href="/people/m/maunendra-sankar-desarkar/">Maunendra Sankar Desarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--7"><div class="card-body p-3 small">In dialogue systems, automatically evaluating machine-generated responses is critical and challenging. Despite the tremendous progress in dialogue generation research, its evaluation heavily depends on human judgments. The standard word-overlapping based evaluation metrics are ineffective for dialogues. As a result, most of the recently proposed metrics are model-based and reference-free, which learn to score different aspects of a conversation. However, understanding each aspect requires a separate model, which makes them computationally expensive. To this end, we propose Dial-M, a Masking-based reference-free framework for Dialogue evaluation. The main idea is to mask the keywords of the current utterance and predict them, given the dialogue history and various conditions (like knowledge, persona, etc.), thereby making the evaluation framework simple and easily extensible for multiple datasets. Regardless of its simplicity, Dial-M achieves comparable performance to state-of-the-art metrics on several dialogue evaluation datasets. We also discuss the interpretability of our proposed metric along with error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.8.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.8.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--8" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.8" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.8/">From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue</a></strong><br><a href="/people/s/shutong-feng/">Shutong Feng</a>
|
<a href="/people/n/nurul-lubis/">Nurul Lubis</a>
|
<a href="/people/b/benjamin-ruppik/">Benjamin Ruppik</a>
|
<a href="/people/c/christian-geishauser/">Christian Geishauser</a>
|
<a href="/people/m/michael-heck/">Michael Heck</a>
|
<a href="/people/h/hsien-chin-lin/">Hsien-chin Lin</a>
|
<a href="/people/c/carel-van-niekerk/">Carel van Niekerk</a>
|
<a href="/people/r/renato-vukovic/">Renato Vukovic</a>
|
<a href="/people/m/milica-gasic/">Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--8"><div class="card-body p-3 small">Emotion recognition in conversations (ERC) is a crucial task for building human-like conversational agents. While substantial efforts have been devoted to ERC for chit-chat dialogues, the task-oriented counterpart is largely left unattended. Directly applying chit-chat ERC models to task-oriented dialogues (ToDs) results in suboptimal performance as these models overlook key features such as the correlation between emotions and task completion in ToDs. In this paper, we propose a framework that turns a chit-chat ERC model into a task-oriented one, addressing three critical aspects: data, features and objective. First, we devise two ways of augmenting rare emotions to improve ERC performance. Second, we use dialogue states as auxiliary features to incorporate key information from the goal of the user. Lastly, we leverage a multi-aspect emotion definition in ToDs to devise a multi-task learning objective and a novel emotion-distance weighted loss function. Our framework yields significant improvements for a range of chit-chat ERC models on EmoWOZ, a large-scale dataset for user emotions in ToDs. We further investigate the generalisability of the best resulting model to predict user satisfaction in different ToD datasets. A comparison with supervised baselines shows a strong zero-shot capability, highlighting the potential usage of our framework in wider scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.9.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.9.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--9" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.9" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.9/">Analyzing Differences in Subjective Annotations by Participants and Third-party Annotators in Multimodal Dialogue Corpus</a></strong><br><a href="/people/k/kazunori-komatani/">Kazunori Komatani</a>
|
<a href="/people/r/ryu-takeda/">Ryu Takeda</a>
|
<a href="/people/s/shogo-okada/">Shogo Okada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--9"><div class="card-body p-3 small">Estimating the subjective impressions of human users during a dialogue is necessary when constructing a dialogue system that can respond adaptively to their emotional states. However, such subjective impressions (e.g., how much the user enjoys the dialogue) are inherently ambiguous, and the annotation results provided by multiple annotators do not always agree because they depend on the subjectivity of the annotators. In this paper, we analyzed the annotation results using 13,226 exchanges from 155 participants in a multimodal dialogue corpus called Hazumi that we had constructed, where each exchange was annotated by five third-party annotators. We investigated the agreement between the subjective annotations given by the third-party annotators and the participants themselves, on both per-exchange annotations (i.e., participant’s sentiments) and per-dialogue (-participant) annotations (i.e., questionnaires on rapport and personality traits). We also investigated the conditions under which the annotation results are reliable. Our findings demonstrate that the dispersion of third-party sentiment annotations correlates with agreeableness of the participants, one of the Big Five personality traits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.10.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.10.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--10" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.10" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.10/">Frame-oriented Summarization of Argumentative Discussions</a></strong><br><a href="/people/s/shahbaz-syed/">Shahbaz Syed</a>
|
<a href="/people/t/timon-ziegenbein/">Timon Ziegenbein</a>
|
<a href="/people/p/philipp-heinisch/">Philipp Heinisch</a>
|
<a href="/people/h/henning-wachsmuth/">Henning Wachsmuth</a>
|
<a href="/people/m/martin-potthast/">Martin Potthast</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--10"><div class="card-body p-3 small">Online discussions on controversial topics with many participants frequently include hundreds of arguments that cover different framings of the topic. But these arguments and frames are often spread across the various branches of the discussion tree structure. This makes it difficult for interested participants to follow the discussion in its entirety as well as to introduce new arguments. In this paper, we present a new rank-based approach to extractive summarization of online discussions focusing on argumentation frames that capture the different aspects of a discussion. Our approach includes three retrieval tasks to find arguments in a discussion that are (1) relevant to a frame of interest, (2) relevant to the topic under discussion, and (3) informative to the reader. Based on a joint ranking by these three criteria for a set of user-selected frames, our approach allows readers to quickly access an ongoing discussion. We evaluate our approach using a test set of 100 controversial Reddit ChangeMyView discussions, for which the relevance of a total of 1871 arguments was manually annotated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.11.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.11.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--11" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.11" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.11/">Towards Multilingual Automatic Open-Domain Dialogue Evaluation</a></strong><br><a href="/people/j/john-mendonca/">John Mendonca</a>
|
<a href="/people/a/alon-lavie/">Alon Lavie</a>
|
<a href="/people/i/isabel-trancoso/">Isabel Trancoso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--11"><div class="card-body p-3 small">The main limiting factor in the development of robust multilingual open-domain dialogue evaluation metrics is the lack of multilingual data and the limited availability of open-sourced multilingual dialogue systems. In this work, we propose a workaround for this lack of data by leveraging a strong multilingual pretrained encoder-based Language Model and augmenting existing English dialogue data using Machine Translation. We empirically show that the naive approach of finetuning a pretrained multilingual encoder model with translated data is insufficient to outperform the strong baseline of finetuning a multilingual model with only source data. Instead, the best approach consists in the careful curation of translated data using MT Quality Estimation metrics, excluding low quality translations that hinder its performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.12.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.12.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--12" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.12" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.12/">Dialog Action-Aware Transformer for Dialog Policy Learning</a></strong><br><a href="/people/h/huimin-wang/">Huimin Wang</a>
|
<a href="/people/w/wai-chung-kwan/">Wai Chung Kwan</a>
|
<a href="/people/k/kam-fai-wong/">Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--12"><div class="card-body p-3 small">Recent works usually address Dialog policy learning DPL by training a reinforcement learning (RL) agent to determine the best dialog action. However, existing works on deep RL require a large volume of agent-user interactions to achieve acceptable performance. In this paper, we propose to make full use of the plain text knowledge from the pre-trained language model to accelerate the RL agent’s learning speed. Specifically, we design a dialog action-aware transformer encoder (DaTrans), which integrates a new fine-tuning procedure named masked last action task to encourage DaTrans to be dialog-aware and distill action-specific features. Then, DaTrans is further optimized in an RL setting with ongoing interactions and evolves through exploration in the dialog action space toward maximizing long-term accumulated rewards. The effectiveness and efficiency of the proposed model are demonstrated with both simulator evaluation and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.13.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.13.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--13" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.13" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.13/">The Wizard of Curiosities: Enriching Dialogues with Fun Facts</a></strong><br><a href="/people/f/frederico-vicente/">Frederico Vicente</a>
|
<a href="/people/r/rafael-ferreira/">Rafael Ferreira</a>
|
<a href="/people/d/david-semedo/">David Semedo</a>
|
<a href="/people/j/joao-magalhaes/">Joao Magalhaes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--13"><div class="card-body p-3 small">Introducing curiosities in a conversation is a way to teach something new to the person in a pleasant and enjoyable way. Enriching dialogues with contextualized curiosities can improve the users’ perception of a dialog system and their overall user experience. In this paper, we introduce a set of curated curiosities, targeting dialogues in the cooking and DIY domains. In particular, we use real human-agent conversations collected in the context of the Amazon Alexa TaskBot challenge, a multimodal and multi-turn conversational setting. According to an A/B test with over 1000 conversations, curiosities not only increase user engagement, but provide an average relative rating improvement of 9.7%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.14.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.14.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--14" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.14" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.14/">The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling</a></strong><br><a href="/people/b/brielen-madureira/">Brielen Madureira</a>
|
<a href="/people/p/patrick-kahardipraja/">Patrick Kahardipraja</a>
|
<a href="/people/d/david-schlangen/">David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--14"><div class="card-body p-3 small">Incremental dialogue model components produce a sequence of output prefixes based on incoming input. Mistakes can occur due to local ambiguities or to wrong hypotheses, making the ability to revise past outputs a desirable property that can be governed by a policy. In this work, we formalise and characterise edits and revisions in incremental sequence labelling and propose metrics to evaluate revision policies. We then apply our methodology to profile the incremental behaviour of three Transformer-based encoders in various tasks, paving the road for better revision policies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.15.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.15.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--15" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.15" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.15/">The effect of conversation type on entrainment: Evidence from laughter</a></strong><br><a href="/people/b/bogdan-ludusan/">Bogdan Ludusan</a>
|
<a href="/people/p/petra-wagner/">Petra Wagner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--15"><div class="card-body p-3 small">Entrainment is a phenomenon that occurs across several modalities and at different linguistic levels in conversation. Previous work has shown that its effects may be modulated by conversation extrinsic factors, such as the relation between the interlocutors or the speakers’ traits. The current study investigates the role of conversation type on laughter entrainment. Employing dyadic interaction materials in German, containing two conversation types (free dialogues and task-based interactions), we analyzed three measures of entrainment previously proposed in the literature. The results show that the entrainment effects depend on the type of conversation, with two of the investigated measures being affected by this factor. These findings represent further evidence towards the role of situational aspects as a mediating factor in conversation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.16.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.16.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--16" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.16" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.16/">‘What are you referring to?’ Evaluating the Ability of Multi-Modal Dialogue Models to Process Clarificational Exchanges</a></strong><br><a href="/people/j/javier-chiyah-garcia/">Javier Chiyah-Garcia</a>
|
<a href="/people/a/alessandro-suglia/">Alessandro Suglia</a>
|
<a href="/people/a/arash-eshghi/">Arash Eshghi</a>
|
<a href="/people/h/helen-hastie/">Helen Hastie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--16"><div class="card-body p-3 small">Referential ambiguities arise in dialogue when a referring expression does not uniquely identify the intended referent for the addressee. Addressees usually detect such ambiguities immediately and work with the speaker to repair it using meta-communicative, Clarificational Exchanges (CE): a Clarification Request (CR) and a response. Here, we argue that the ability to generate and respond to CRs imposes specific constraints on the architecture and objective functions of multi-modal, visually grounded dialogue models. We use the SIMMC 2.0 dataset to evaluate the ability of different state-of-the-art model architectures to process CEs, with a metric that probes the contextual updates that arise from them in the model. We find that language-based models are able to encode simple multi-modal semantic information and process some CEs, excelling with those related to the dialogue history, whilst multi-modal models can use additional learning objectives to obtain disentangled object representations, which become crucial to handle complex referential ambiguities across modalities overall.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.17.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.17.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--17" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.17" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.17/"><span class="acl-fixed-case">PGT</span>ask: Introducing the Task of Profile Generation from Dialogues</a></strong><br><a href="/people/r/rui-ribeiro/">Rui Ribeiro</a>
|
<a href="/people/j/joao-paulo-carvalho/">Joao Paulo Carvalho</a>
|
<a href="/people/l/luisa-coheur/">Luisa Coheur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--17"><div class="card-body p-3 small">Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.18.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.18.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--18" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.18" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.18/">Question Generation to Elicit Users’ Food Preferences by Considering the Semantic Content</a></strong><br><a href="/people/j/jie-zeng/">Jie Zeng</a>
|
<a href="/people/y/yukiko-i-nakano/">Yukiko Nakano</a>
|
<a href="/people/t/tatsuya-sakato/">Tatsuya Sakato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--18"><div class="card-body p-3 small">To obtain a better understanding of user preferences in providing tailored services, dialogue systems have to generate semi-structured interviews that require flexible dialogue control while following a topic guide to accomplish the purpose of the interview. Toward this goal, this study proposes a semantics-aware GPT-3 fine-tuning model that generates interviews to acquire users’ food preferences. The model was trained using dialogue history and semantic representation constructed from the communicative function and semantic content of the utterance. Using two baseline models: zero-shot ChatGPT and fine-tuned GPT-3, we conducted a user study for subjective evaluations alongside automatic objective evaluations. In the user study, in impression rating, the outputs of the proposed model were superior to those of baseline models and comparable to real human interviews in terms of eliciting the interviewees’ food preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.19.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.19.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--19" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.19" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.19/">Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System</a></strong><br><a href="/people/l/lingbo-mo/">Lingbo Mo</a>
|
<a href="/people/s/shijie-chen/">Shijie Chen</a>
|
<a href="/people/z/ziru-chen/">Ziru Chen</a>
|
<a href="/people/x/xiang-deng/">Xiang Deng</a>
|
<a href="/people/a/ashley-lewis/">Ashley Lewis</a>
|
<a href="/people/s/sunit-singh/">Sunit Singh</a>
|
<a href="/people/s/samuel-stevens/">Samuel Stevens</a>
|
<a href="/people/c/chang-you-tai/">Chang-You Tai</a>
|
<a href="/people/z/zhen-wang/">Zhen Wang</a>
|
<a href="/people/x/xiang-yue/">Xiang Yue</a>
|
<a href="/people/t/tianshu-zhang/">Tianshu Zhang</a>
|
<a href="/people/y/yu-su/">Yu Su</a>
|
<a href="/people/h/huan-sun/">Huan Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--19"><div class="card-body p-3 small">We introduce TacoBot, a user-centered task-oriented digital assistant designed to guide users through complex real-world tasks with multiple steps. Covering a wide range of cooking and how-to tasks, we aim to deliver a collaborative and engaging dialogue experience. Equipped with language understanding, dialogue management, and response generation components supported by a robust search engine, TacoBot ensures efficient task assistance. To enhance the dialogue experience, we explore a series of data augmentation strategies using LLMs to train advanced neural models continuously. TacoBot builds upon our successful participation in the inaugural Alexa Prize TaskBot Challenge, where our team secured third place among ten competing teams. We offer TacoBot as an open-source framework that serves as a practical example for deploying task-oriented dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.20.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.20.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--20" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.20" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.20/">Leveraging Large Language Models for Automated Dialogue Analysis</a></strong><br><a href="/people/s/sarah-e-finch/">Sarah E. Finch</a>
|
<a href="/people/e/ellie-s-paek/">Ellie S. Paek</a>
|
<a href="/people/j/jinho-d-choi/">Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--20"><div class="card-body p-3 small">Developing high-performing dialogue systems benefits from the automatic identification of undesirable behaviors in system responses. However, detecting such behaviors remains challenging, as it draws on a breadth of general knowledge and understanding of conversational practices. Although recent research has focused on building specialized classifiers for detecting specific dialogue behaviors, the behavior coverage is still incomplete and there is a lack of testing on real-world human-bot interactions. This paper investigates the ability of a state-of-the-art large language model (LLM), ChatGPT-3.5, to perform dialogue behavior detection for nine categories in real human-bot dialogues. We aim to assess whether ChatGPT can match specialized models and approximate human performance, thereby reducing the cost of behavior detection tasks. Our findings reveal that neither specialized models nor ChatGPT have yet achieved satisfactory results for this task, falling short of human performance. Nevertheless, ChatGPT shows promising potential and often outperforms specialized detection models. We conclude with an in-depth examination of the prevalent shortcomings of ChatGPT, offering guidance for future research to enhance LLM capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.21.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.21.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--21" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.21" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.21/">Are Large Language Models All You Need for Task-Oriented Dialogue?</a></strong><br><a href="/people/v/vojtech-hudecek/">Vojtěch Hudeček</a>
|
<a href="/people/o/ondrej-dusek/">Ondrej Dusek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--21"><div class="card-body p-3 small">Instruction-finetuned large language models (LLMs) gained a huge popularity recently, thanks to their ability to interact with users through conversation. In this work, we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that in explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show some ability to guide the dialogue to a successful ending through their generated responses if they are provided with correct slot values. Furthermore, this ability improves with few-shot in-domain examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.22.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.22.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--22" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.22" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.22/">Multi-party Goal Tracking with <span class="acl-fixed-case">LLM</span>s: Comparing Pre-training, Fine-tuning, and Prompt Engineering</a></strong><br><a href="/people/a/angus-addlesee/">Angus Addlesee</a>
|
<a href="/people/w/weronika-sieinska/">Weronika Sieińska</a>
|
<a href="/people/n/nancie-gunson/">Nancie Gunson</a>
|
<a href="/people/d/daniel-hernandez-garcia/">Daniel Hernandez Garcia</a>
|
<a href="/people/c/christian-dondrup/">Christian Dondrup</a>
|
<a href="/people/o/oliver-lemon/">Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--22"><div class="card-body p-3 small">This paper evaluates the extent to which current LLMs can capture task-oriented multi-party conversations (MPCs). We have recorded and transcribed 29 MPCs between patients, their companions, and a social robot in a hospital. We then annotated this corpus for multi-party goal-tracking and intent-slot recognition. People share goals, answer each other’s goals, and provide other people’s goals in MPCs - none of which occur in dyadic interactions. To understand user goals in MPCs, we compared three methods in zero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks to train DialogLM using LED, and employed prompt engineering techniques with GPT-3.5-turbo, to determine which approach can complete this novel task with limited data. GPT-3.5-turbo significantly outperformed the others in a few-shot setting. The ‘reasoning’ style prompt, when given 7% of the corpus as example annotated conversations, was the best performing method. It correctly annotated 62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition MPCs. A ‘story’ style prompt increased model hallucination, which could be detrimental if deployed in safety-critical settings. We conclude that multi-party conversations still challenge state-of-the-art LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.23.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.23.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--23" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.23" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.23/"><span class="acl-fixed-case">C</span>hat<span class="acl-fixed-case">GPT</span> vs. Crowdsourcing vs. Experts: Annotating Open-Domain Conversations with Speech Functions</a></strong><br><a href="/people/l/lidiia-ostyakova/">Lidiia Ostyakova</a>
|
<a href="/people/v/veronika-smilga/">Veronika Smilga</a>
|
<a href="/people/k/kseniia-petukhova/">Kseniia Petukhova</a>
|
<a href="/people/m/maria-molchanova/">Maria Molchanova</a>
|
<a href="/people/d/daniel-kornev/">Daniel Kornev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--23"><div class="card-body p-3 small">This paper deals with the task of annotating open-domain conversations with speech functions. We propose a semi-automated method for annotating dialogs following the topic-oriented, multi-layered taxonomy of speech functions with the use of hierarchical guidelines using Large Language Models. These guidelines comprise simple questions about the topic and speaker change, sentence types, pragmatic aspects of the utterance, and examples that aid untrained annotators in understanding the taxonomy. We compare the results of dialog annotation performed by experts, crowdsourcing workers, and ChatGPT. To improve the performance of ChatGPT, several experiments utilising different prompt engineering techniques were conducted. We demonstrate that in some cases large language models can achieve human-like performance following a multi-step tree-like annotation pipeline on complex discourse annotation, which is usually challenging and costly in terms of time and money when performed by humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.24.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.24.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--24" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.24" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.24/"><span class="acl-fixed-case">D</span>iact<span class="acl-fixed-case">TOD</span>: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems</a></strong><br><a href="/people/q/qingyang-wu/">Qingyang Wu</a>
|
<a href="/people/j/james-gung/">James Gung</a>
|
<a href="/people/r/raphael-shu/">Raphael Shu</a>
|
<a href="/people/y/yi-zhang/">Yi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--24"><div class="card-body p-3 small">Dialogue act annotations are important to improve response generation quality in task-oriented dialogue systems. However, it can be challenging to use dialogue acts to control response generation in a generalizable way because different datasets and tasks may have incompatible annotations. While alternative methods that utilize latent action spaces or reinforcement learning do not require explicit annotations, they may lack interpretability or face difficulties defining task-specific rewards. In this work, we present a novel end-to-end latent dialogue act model (DiactTOD) that represents dialogue acts in a latent space. DiactTOD, when pre-trained on a large corpus, is able to predict and control dialogue acts to generate controllable responses using these latent representations in a zero-shot fashion. Our approach demonstrates state-of-the-art performance across a wide range of experimental settings on the MultiWOZ dataset, including zero-shot, few-shot, and full data fine-tuning with both end-to-end and policy optimization configurations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.25.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.25.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--25" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.25" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.25/">Approximating Online Human Evaluation of Social Chatbots with Prompting</a></strong><br><a href="/people/e/ekaterina-svikhnushina/">Ekaterina Svikhnushina</a>
|
<a href="/people/p/pearl-pu/">Pearl Pu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--25"><div class="card-body p-3 small">With conversational models becoming increasingly available to the general public, developing scalable and robust evaluation metrics is crucial to minimize potential social and psychological risks for the users. Existing evaluation metrics aim to automate offline user evaluation and approximate human judgment of pre-curated dialogs. However, they are limited in their ability to capture subjective perceptions of users who actually interact with the chatbots and might not generalize to real-world settings. To address this limitation, we propose an approach to approximate online human evaluation, leveraging large language models (LLMs) from the GPT-family. We introduce a new Dialog system Evaluation framework based on Prompting (DEP), which enables a fully automatic evaluation pipeline that replicates live user studies and achieves an impressive correlation with human judgment (up to Pearson r=0.95 on a system level). The DEP approach involves collecting synthetic chat logs of evaluated bots with an LLM in the other-play setting, where the LLM is carefully conditioned to follow a specific scenario. We further explore different prompting approaches to produce evaluation scores with the same LLM. The best-performing prompts, which contain few-shot demonstrations and instructions, show outstanding performance on the tested dataset and demonstrate the ability to generalize to other dialog corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.26.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.26.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--26" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.26" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.26/">Dialogue Response Generation Using Completion of Omitted Predicate Arguments Based on Zero Anaphora Resolution</a></strong><br><a href="/people/a/ayaka-ueyama/">Ayaka Ueyama</a>
|
<a href="/people/y/yoshinobu-kano/">Yoshinobu Kano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--26"><div class="card-body p-3 small">Human conversation attempts to build common ground consisting of shared beliefs, knowledge, and perceptions that form the premise for understanding utterances. Recent deep learning-based dialogue systems use human dialogue data to train a mapping from a dialogue history to responses, but common ground not directly expressed in words makes it difficult to generate coherent responses by learning statistical patterns alone. We propose Dialogue Completion using Zero Anaphora Resolution (DCZAR), a framework that explicitly completes omitted information in the dialogue history and generates responses from the completed dialogue history. In this study, we conducted automatic and human evaluations by applying several pretraining methods and datasets in Japanese in various combinations. Experimental results show that the DCZAR framework contributes to the generation of more coherent and engaging responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.27.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.27.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--27" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.27" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.27/">Syndicom: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback</a></strong><br><a href="/people/c/christopher-richardson/">Christopher Richardson</a>
|
<a href="/people/a/anirudh-sundar/">Anirudh Sundar</a>
|
<a href="/people/l/larry-heck/">Larry Heck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--27"><div class="card-body p-3 small">Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce Syndicom - a method for improving commonsense in dialogue response generation. Syndicom consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. Syndicom is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad range of metrics. Syndicom achieves a relative improvement of 53% over ChatGPT on ROUGE-1, and human evaluators prefer Syndicom over ChatGPT 57% of the time. We will publicly release the code and the full dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.28.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.28.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--28" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.28" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.28/">“What do others think?”: Task-Oriented Conversational Modeling with Subjective Knowledge</a></strong><br><a href="/people/c/chao-zhao/">Chao Zhao</a>
|
<a href="/people/s/spandana-gella/">Spandana Gella</a>
|
<a href="/people/s/seokhwan-kim/">Seokhwan Kim</a>
|
<a href="/people/d/di-jin/">Di Jin</a>
|
<a href="/people/d/devamanyu-hazarika/">Devamanyu Hazarika</a>
|
<a href="/people/a/alexandros-papangelis/">Alexandros Papangelis</a>
|
<a href="/people/b/behnam-hedayatnia/">Behnam Hedayatnia</a>
|
<a href="/people/m/mahdi-namazifar/">Mahdi Namazifar</a>
|
<a href="/people/y/yang-liu-icsi/">Yang Liu</a>
|
<a href="/people/d/dilek-hakkani-tur/">Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--28"><div class="card-body p-3 small">Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that assist users in accomplishing specific goals, such as booking a hotel or a restaurant. Traditional TODs rely on domain-specific APIs/DBs or external factual knowledge to generate responses, which cannot accommodate subjective user requests (e.g.,”Is the WIFI reliable?” or “Does the restaurant have a good atmosphere?”). To address this issue, we propose a novel task of subjective-knowledge-based TOD (SK-TOD). We also propose the first corresponding dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets. We hope this task and dataset can promote further research on TOD and subjective content understanding. The code and the dataset are available at https://github.com/alexa/dstc11-track5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.29.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.29.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--29" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.29" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.29/"><span class="acl-fixed-case">UD</span>_<span class="acl-fixed-case">J</span>apanese-<span class="acl-fixed-case">CEJC</span>: Dependency Relation Annotation on Corpus of Everyday <span class="acl-fixed-case">J</span>apanese Conversation</a></strong><br><a href="/people/m/mai-omura/">Mai Omura</a>
|
<a href="/people/h/hiroshi-matsuda/">Hiroshi Matsuda</a>
|
<a href="/people/m/masayuki-asahara/">Masayuki Asahara</a>
|
<a href="/people/a/aya-wakasa/">Aya Wakasa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--29"><div class="card-body p-3 small">In this study, we have developed Universal Dependencies (UD) resources for spoken Japanese in the Corpus of Everyday Japanese Conversation (CEJC). The CEJC is a large corpus of spoken language that encompasses various everyday conversations in Japanese, and includes word delimitation and part-of-speech annotation. We have newly annotated Long Word Unit delimitation and Bunsetsu (Japanese phrase)-based dependencies, including Bunsetsu boundaries, for CEJC. The UD of Japanese resources was constructed in accordance with hand-maintained conversion rules from the CEJC with two types of word delimitation, part-of-speech tags and Bunsetsu-based syntactic dependency relations. Furthermore, we examined various issues pertaining to the construction of UD in the CEJC by comparing it with the written Japanese corpus and evaluating UD parsing accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.30.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.30.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--30" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.30" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.30/">Unravelling Indirect Answers to Wh-Questions: Corpus Construction, Analysis, and Generation</a></strong><br><a href="/people/z/zulipiye-yusupujiang/">Zulipiye Yusupujiang</a>
|
<a href="/people/j/jonathan-ginzburg/">Jonathan Ginzburg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--30"><div class="card-body p-3 small">Indirect answers, crucial in human communication, serve to maintain politeness, avoid conflicts, and align with social customs. Although there has been a substantial number of studies on recognizing and understanding indirect answers to polar questions (often known as yes/no questions), there is a dearth of such work regarding wh-questions. This study takes up the challenge by constructing what is, to our knowledge, the first corpus of indirect answers to wh-questions. We analyze and interpret indirect answers to different wh-questions based on our carefully compiled corpus. In addition, we conducted a pilot study on generating indirect answers to wh-questions by fine-tuning the pre-trained generative language model DialoGPT (Zhang et al., 2020). Our results suggest this is a task that GPT finds difficult.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.31.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.31.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--31" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.31" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.31/">A New Dataset for Causality Identification in Argumentative Texts</a></strong><br><a href="/people/k/khalid-al-khatib/">Khalid Al Khatib</a>
|
<a href="/people/m/michael-voelske/">Michael Voelske</a>
|
<a href="/people/a/anh-le/">Anh Le</a>
|
<a href="/people/s/shahbaz-syed/">Shahbaz Syed</a>
|
<a href="/people/m/martin-potthast/">Martin Potthast</a>
|
<a href="/people/b/benno-stein/">Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--31"><div class="card-body p-3 small">Existing datasets for causality identification in argumentative texts have several limitations, such as the type of input text (e.g., only claims), causality type (e.g., only positive), and the linguistic patterns investigated (e.g., only verb connectives). To resolve these limitations, we build the Webis-Causality-23 dataset, with sophisticated inputs (all units from arguments), a balanced distribution of causality types, and a larger number of linguistic patterns denoting causality. The dataset contains 1485 examples derived by combining the two paradigms of distant supervision and uncertainty sampling to identify diverse, high-quality samples of causality relations, and annotate them in a cost-effective manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.32.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.32.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--32" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.32" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.32/">Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking</a></strong><br><a href="/people/a/angela-ramirez/">Angela Ramirez</a>
|
<a href="/people/k/kartik-agarwal/">Kartik Agarwal</a>
|
<a href="/people/j/juraj-juraska/">Juraj Juraska</a>
|
<a href="/people/u/utkarsh-garg/">Utkarsh Garg</a>
|
<a href="/people/m/marilyn-walker/">Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--32"><div class="card-body p-3 small">Dialogue systems need to produce responses that realize multiple types of dialogue acts (DAs) with high semantic fidelity. In the past, natural language generators (NLGs) for dialogue were trained on large parallel corpora that map from a domain-specific DA and its semantic attributes to an output utterance. Recent work shows that pretrained language models (LLMs) offer new possibilities for controllable NLG using prompt-based learning. Here we develop a novel few-shot overgenerate-and-rank approach that achieves the controlled generation of DAs. We compare eight few-shot prompt styles that include a novel method of generating from textual pseudo-references using a textual style transfer approach. We develop six automatic ranking functions that identify outputs with both the correct DA and high semantic accuracy at generation time. We test our approach on three domains and four LLMs. To our knowledge, this is the first work on NLG for dialogue that automatically ranks outputs using both DA and attribute accuracy. For completeness, we compare our results to fine-tuned few-shot models trained with 5 to 100 instances per DA. Our results show that several prompt settings achieve perfect DA accuracy, and near perfect semantic accuracy (99.81%) and perform better than few-shot fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.33.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.33.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--33" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.33" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.33/">Reference Resolution and New Entities in Exploratory Data Visualization: From Controlled to Unconstrained Interactions with a Conversational Assistant</a></strong><br><a href="/people/a/abari-bhattacharya/">Abari Bhattacharya</a>
|
<a href="/people/a/abhinav-kumar/">Abhinav Kumar</a>
|
<a href="/people/b/barbara-di-eugenio/">Barbara Di Eugenio</a>
|
<a href="/people/r/roderick-tabalba/">Roderick Tabalba</a>
|
<a href="/people/j/jillian-aurisano/">Jillian Aurisano</a>
|
<a href="/people/v/veronica-grosso/">Veronica Grosso</a>
|
<a href="/people/a/andrew-johnson/">Andrew Johnson</a>
|
<a href="/people/j/jason-leigh/">Jason Leigh</a>
|
<a href="/people/m/moira-zellner/">Moira Zellner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--33"><div class="card-body p-3 small">In the context of data visualization, as in other grounded settings, referents are created by the task the agents engage in and are salient because they belong to the shared physical setting. Our focus is on resolving references to visualizations on large displays; crucially, reference resolution is directly involved in the process of creating new entities, namely new visualizations. First, we developed a reference resolution model for a conversational assistant. We trained the assistant on controlled dialogues for data visualizations involving a single user. Second, we ported the conversational assistant including its reference resolution model to a different domain, supporting two users collaborating on a data exploration task. We explore how the new setting affects reference detection and resolution; we compare the performance in the controlled vs unconstrained setting, and discuss the general lessons that we draw from this adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.34.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.34.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--34" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.34" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.34/"><span class="acl-fixed-case">CONVERSER</span>: Few-shot Conversational Dense Retrieval with Synthetic Data Generation</a></strong><br><a href="/people/c/chao-wei-huang/">Chao-Wei Huang</a>
|
<a href="/people/c/chen-yu-hsu/">Chen-Yu Hsu</a>
|
<a href="/people/t/tsu-yuan-hsu/">Tsu-Yuan Hsu</a>
|
<a href="/people/c/chen-an-li/">Chen-An Li</a>
|
<a href="/people/y/yun-nung-chen/">Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--34"><div class="card-body p-3 small">Conversational search provides a natural interface for information retrieval (IR). Recent approaches have demonstrated promising results in applying dense retrieval to conversational IR. However, training dense retrievers requires large amounts of in-domain paired data. This hinders the development of conversational dense retrievers, as abundant in-domain conversations are expensive to collect. In this paper, we propose Converser, a framework for training conversational dense retrievers with at most 6 examples of in-domain dialogues. Specifically, we utilize the in-context learning capability of large language models to generate conversational queries given a passage in the retrieval corpus. Experimental results on conversational retrieval benchmarks OR-QuAC and TREC CAsT 19 show that the proposed Converser achieves comparable performance to fully-supervised models, demonstrating the effectiveness of our proposed framework in few-shot conversational dense retrieval. All source code and generated datasets are available: https://github.com/MiuLab/CONVERSER</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.35.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.35.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--35" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.35" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.35/">Speaker Role Identification in Call Centre Dialogues: Leveraging Opening Sentences and Large Language Models</a></strong><br><a href="/people/m/minh-quoc-nghiem/">Minh-Quoc Nghiem</a>
|
<a href="/people/n/nichola-roberts/">Nichola Roberts</a>
|
<a href="/people/d/dmitry-sityaev/">Dmitry Sityaev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--35"><div class="card-body p-3 small">This paper addresses the task of speaker role identification in call centre dialogues, focusing on distinguishing between the customer and the agent. We propose a text-based approach that utilises the identification of the agent’s opening sentence as a key feature for role classification. The opening sentence is identified using a model trained through active learning. By combining this information with a large language model, we accurately classify the speaker roles. The proposed approach is evaluated on a dataset of call centre dialogues and achieves 93.61% accuracy. This work contributes to the field by providing an effective solution for speaker role identification in call centre settings, with potential applications in interaction analysis and information retrieval.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.36.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.36.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--36" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.36" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.36/">Synthesising Personality with Neural Speech Synthesis</a></strong><br><a href="/people/s/shilin-gao/">Shilin Gao</a>
|
<a href="/people/m/matthew-p-aylett/">Matthew P. Aylett</a>
|
<a href="/people/d/david-a-braude/">David A. Braude</a>
|
<a href="/people/c/catherine-lai/">Catherine Lai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--36"><div class="card-body p-3 small">Matching the personality of conversational agent to the personality of the user can significantly improve the user experience, with many successful examples in text-based chatbots. It is also important for a voice-based system to be able to alter the personality of the speech as perceived by the users. In this pilot study, fifteen voices were rated using Big Five personality traits. Five content-neutral sentences were chosen for the listening tests. The audio data, together with two rated traits (Extroversion and Agreeableness), were used to train a neural speech synthesiser based on one male and one female voices. The effect of altering the personality trait features was evaluated by a second listening test. Both perceived extroversion and agreeableness in the synthetic voices were affected significantly. The controllable range was limited due to a lack of variance in the source audio data. The perceived personality traits correlated with each other and with the naturalness of the speech. Future work can be making a chatbot speak in a voice with a pre-defined or adaptive personality by using personality synthesis in speech together with text-based personality generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.37.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.37.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--37" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.37" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.37/">Prompting, Retrieval, Training: An exploration of different approaches for task-oriented dialogue generation</a></strong><br><a href="/people/g/goncalo-raposo/">Gonçalo Raposo</a>
|
<a href="/people/l/luisa-coheur/">Luisa Coheur</a>
|
<a href="/people/b/bruno-martins/">Bruno Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--37"><div class="card-body p-3 small">Task-oriented dialogue systems need to generate appropriate responses to help fulfill users’ requests. This paper explores different strategies, namely prompting, retrieval, and fine-tuning, for task-oriented dialogue generation. Through a systematic evaluation, we aim to provide valuable insights and guidelines for researchers and practitioners working on developing efficient and effective dialogue systems for real-world applications. Evaluation is performed on the MultiWOZ and Taskmaster-2 datasets, and we test various versions of FLAN-T5, GPT-3.5, and GPT-4 models. Costs associated with running these models are analyzed, and dialogue evaluation is briefly discussed. Our findings suggest that when testing data differs from the training data, fine-tuning may decrease performance, favoring a combination of a more general language model and a prompting mechanism based on retrieved examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.38.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.38.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--38" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.38" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.38/">Bootstrapping a Conversational Guide for Colonoscopy Prep</a></strong><br><a href="/people/p/pulkit-arya/">Pulkit Arya</a>
|
<a href="/people/m/madeleine-bloomquist/">Madeleine Bloomquist</a>
|
<a href="/people/s/subhankar-chakraborty/">Subhankar Chakraborty</a>
|
<a href="/people/a/andrew-perrault/">Andrew Perrault</a>
|
<a href="/people/w/william-schuler/">William Schuler</a>
|
<a href="/people/e/eric-fosler-lussier/">Eric Fosler-Lussier</a>
|
<a href="/people/m/michael-white/">Michael White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--38"><div class="card-body p-3 small">Creating conversational systems for niche domains is a challenging task, further exacerbated by a lack of quality datasets. We explore the construction of safer conversational systems for guiding patients in preparing for colonoscopies. This has required a data generation pipeline to generate a minimum viable dataset to bootstrap a semantic parser, augmented by automatic paraphrasing. Our study suggests large language models (e.g., GPT-3.5 and GPT-4) are a viable alternative to crowd sourced paraphrasing, but conversational systems that rely upon language models’ ability to do temporal reasoning struggle to provide accurate responses. A neural-symbolic system that performs temporal reasoning on an intermediate representation of user queries shows promising results compared to an end-to-end dialogue system, improving the number of correct responses while vastly reducing the number of incorrect or misleading ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.39.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.39.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--39" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.39" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.39/">Applying Item Response Theory to Task-oriented Dialogue Systems for Accurately Determining User’s Task Success Ability</a></strong><br><a href="/people/r/ryu-hirai/">Ryu Hirai</a>
|
<a href="/people/a/ao-guo/">Ao Guo</a>
|
<a href="/people/r/ryuichiro-higashinaka/">Ryuichiro Higashinaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--39"><div class="card-body p-3 small">While task-oriented dialogue systems have improved, not all users can fully accomplish their tasks. Users with limited knowledge about the system may experience dialogue breakdowns or fail to achieve their tasks because they do not know how to interact with the system. For addressing this issue, it would be desirable to construct a system that can estimate the user’s task success ability and adapt to that ability. In this study, we propose a method that estimates this ability by applying item response theory (IRT), commonly used in education for estimating examinee abilities, to task-oriented dialogue systems. Through experiments predicting the probability of a correct answer to each slot by using the estimated task success ability, we found that the proposed method significantly outperformed baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.40.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.40.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--40" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.40" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.40/">An Open-Domain Avatar Chatbot by Exploiting a Large Language Model</a></strong><br><a href="/people/t/takato-yamazaki/">Takato Yamazaki</a>
|
<a href="/people/t/tomoya-mizumoto/">Tomoya Mizumoto</a>
|
<a href="/people/k/katsumasa-yoshikawa/">Katsumasa Yoshikawa</a>
|
<a href="/people/m/masaya-ohagi/">Masaya Ohagi</a>
|
<a href="/people/t/toshiki-kawamoto/">Toshiki Kawamoto</a>
|
<a href="/people/t/toshinori-sato/">Toshinori Sato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--40"><div class="card-body p-3 small">With the ambition to create avatars capable of human-level casual conversation, we developed an open-domain avatar chatbot, situated in a virtual reality environment, that employs a large language model (LLM). Introducing the LLM posed several challenges for multimodal integration, such as developing techniques to align diverse outputs and avatar control, as well as addressing the issue of slow generation speed. To address these challenges, we integrated various external modules into our system. Our system is based on the award-winning model from the Dialogue System Live Competition 5. Through this work, we hope to stimulate discussions within the research community about the potential and challenges of multimodal dialogue systems enhanced with LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.41.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.41.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--41" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.41" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.41/">Learning Multimodal Cues of Children’s Uncertainty</a></strong><br><a href="/people/q/qi-cheng/">Qi Cheng</a>
|
<a href="/people/m/mert-inan/">Mert Inan</a>
|
<a href="/people/r/rahma-mbarki/">Rahma Mbarki</a>
|
<a href="/people/g/grace-grmek/">Grace Grmek</a>
|
<a href="/people/t/theresa-choi/">Theresa Choi</a>
|
<a href="/people/y/yiming-sun/">Yiming Sun</a>
|
<a href="/people/k/kimele-persaud/">Kimele Persaud</a>
|
<a href="/people/j/jenny-wang/">Jenny Wang</a>
|
<a href="/people/m/malihe-alikhani/">Malihe Alikhani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--41"><div class="card-body p-3 small">Understanding uncertainty plays a critical role in achieving common ground (Clark et al., 1983). This is especially important for multimodal AI systems that collaborate with users to solve a problem or guide the user through a challenging concept. In this work, for the first time, we present a dataset annotated in collaboration with developmental and cognitive psychologists for the purpose of studying nonverbal cues of uncertainty. We then present an analysis of the data, studying different roles of uncertainty and its relationship with task difficulty and performance. Lastly, we present a multimodal machine learning model that can predict uncertainty given a real-time video clip of a participant, which we find improves upon a baseline multimodal transformer model. This work informs research on cognitive coordination between human-human and human-AI and has broad implications for gesture understanding and generation. The anonymized version of our data and code will be publicly available upon the completion of the required consent forms and data sheets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.42.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.42.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--42" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.42" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.42/">Grounding Description-Driven Dialogue State Trackers with Knowledge-Seeking Turns</a></strong><br><a href="/people/a/alexandru-coca/">Alexandru Coca</a>
|
<a href="/people/b/bo-hsiang-tseng/">Bo-Hsiang Tseng</a>
|
<a href="/people/j/jinghong-chen/">Jinghong Chen</a>
|
<a href="/people/w/weizhe-lin/">Weizhe Lin</a>
|
<a href="/people/w/weixuan-zhang/">Weixuan Zhang</a>
|
<a href="/people/t/tisha-anders/">Tisha Anders</a>
|
<a href="/people/b/bill-byrne/">Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--42"><div class="card-body p-3 small">Schema-guided dialogue state trackers can generalise to new domains without further training, yet they are sensitive to the writing style of the schemata. Augmenting the training set with human or synthetic schema paraphrases improves the model robustness to these variations but can be either costly or difficult to control. We propose to circumvent these issues by grounding the state tracking model in knowledge-seeking turns collected from the dialogue corpus as well as the schema. Including these turns in prompts during finetuning and inference leads to marked improvements in model robustness, as demonstrated by large average joint goal accuracy and schema sensitivity improvements on SGD and SGD-X.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.43.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.43.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--43" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.43" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.43/">Resolving References in Visually-Grounded Dialogue via Text Generation</a></strong><br><a href="/people/b/bram-willemsen/">Bram Willemsen</a>
|
<a href="/people/l/livia-qian/">Livia Qian</a>
|
<a href="/people/g/gabriel-skantze/">Gabriel Skantze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--43"><div class="card-body p-3 small">Vision-language models (VLMs) have shown to be effective at image retrieval based on simple text queries, but text-image retrieval based on conversational input remains a challenge. Consequently, if we want to use VLMs for reference resolution in visually-grounded dialogue, the discourse processing capabilities of these models need to be augmented. To address this issue, we propose fine-tuning a causal large language model (LLM) to generate definite descriptions that summarize coreferential information found in the linguistic context of references. We then use a pretrained VLM to identify referents based on the generated descriptions, zero-shot. We evaluate our approach on a manually annotated dataset of visually-grounded dialogues and achieve results that, on average, exceed the performance of the baselines we compare against. Furthermore, we find that using referent descriptions based on larger context windows has the potential to yield higher returns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.44.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.44.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--44" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.44" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.44/">Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning</a></strong><br><a href="/people/h/hoang-nguyen/">Hoang Nguyen</a>
|
<a href="/people/c/chenwei-zhang/">Chenwei Zhang</a>
|
<a href="/people/y/ye-liu/">Ye Liu</a>
|
<a href="/people/p/philip-s-yu/">Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--44"><div class="card-body p-3 small">Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved performance on the Slot Filling tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.45.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.45.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--45" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.45" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.45/">The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems</a></strong><br><a href="/people/a/andreas-liesenfeld/">Andreas Liesenfeld</a>
|
<a href="/people/a/alianda-lopez/">Alianda Lopez</a>
|
<a href="/people/m/mark-dingemanse/">Mark Dingemanse</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--45"><div class="card-body p-3 small">Speech recognition systems are a key intermediary in voice-driven human-computer interaction. Although speech recognition works well for pristine monologic audio, real-life use cases in open-ended interactive settings still present many challenges. We argue that timing is mission-critical for dialogue systems, and evaluate 5 major commercial ASR systems for their conversational and multilingual support. We find that word error rates for natural conversational data in 6 languages remain abysmal, and that overlap remains a key challenge (study 1). This impacts especially the recognition of conversational words (study 2), and in turn has dire consequences for downstream intent recognition (study 3). Our findings help to evaluate the current state of conversational ASR, contribute towards multidimensional error analysis and evaluation, and identify phenomena that need most attention on the way to build robust interactive speech technologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.46.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.46.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--46" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.46" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.46/">Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog</a></strong><br><a href="/people/m/miaoran-li/">Miaoran Li</a>
|
<a href="/people/b/baolin-peng/">Baolin Peng</a>
|
<a href="/people/m/michel-galley/">Michel Galley</a>
|
<a href="/people/j/jianfeng-gao/">Jianfeng Gao</a>
|
<a href="/people/z/zhu-drew-zhang/">Zhu (Drew) Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--46"><div class="card-body p-3 small">The construction of dialog systems for various types of conversations, such as task-oriented dialog (TOD) and open-domain dialog (ODD), has been an active area of research. In order to more closely mimic human-like conversations that often involve the fusion of different dialog modes, it is important to develop systems that can effectively handle both TOD and ODD and access different knowledge sources. In this work, we present a new automatic framework to enrich TODs with synthesized ODDs. We also introduce the PivotBot model, which is capable of handling both TOD and ODD modes and can access different knowledge sources to generate informative responses. Evaluation results indicate the superior ability of the proposed model to switch smoothly between TOD and ODD tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.47.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.47.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--47" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.47" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.47/">Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System</a></strong><br><a href="/people/j/jianguo-zhang/">Jianguo Zhang</a>
|
<a href="/people/s/stephen-roller/">Stephen Roller</a>
|
<a href="/people/k/kun-qian/">Kun Qian</a>
|
<a href="/people/z/zhiwei-liu/">Zhiwei Liu</a>
|
<a href="/people/r/rui-meng/">Rui Meng</a>
|
<a href="/people/s/shelby-heinecke/">Shelby Heinecke</a>
|
<a href="/people/h/huan-wang/">Huan Wang</a>
|
<a href="/people/s/silvio-savarese/">Silvio Savarese</a>
|
<a href="/people/c/caiming-xiong/">Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--47"><div class="card-body p-3 small">End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The introduced cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7% compared to strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.48.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.48.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--48" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.48" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.48/">Transformer-based Multi-Party Conversation Generation using Dialogue Discourse Acts Planning</a></strong><br><a href="/people/a/alexander-chernyavskiy/">Alexander Chernyavskiy</a>
|
<a href="/people/d/dmitry-ilvovsky/">Dmitry Ilvovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--48"><div class="card-body p-3 small">Recent transformer-based approaches to multi-party conversation generation may produce syntactically coherent but discursively inconsistent dialogues in some cases. To address this issue, we propose an approach to integrate a dialogue act planning stage into the end-to-end transformer-based generation pipeline. This approach consists of a transformer fine-tuning procedure based on linearized dialogue representations that include special discourse tokens. The obtained results demonstrate that incorporating discourse tokens into training sequences is sufficient to significantly improve dialogue consistency and overall generation quality. The suggested approach performs well, including for automatically annotated data. Apart from that, it is observed that increasing the weight of the discourse planning task in the loss function accelerates learning convergence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.49.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.49.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--49" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.49" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.49/">Incorporating Annotator Uncertainty into Representations of Discourse Relations</a></strong><br><a href="/people/s/s-magali-lopez-cortez/">S. Magalí López Cortez</a>
|
<a href="/people/c/cassandra-l-jacobs/">Cassandra L. Jacobs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--49"><div class="card-body p-3 small">Annotation of discourse relations is a known difficult task, especially for non-expert annotators. In this paper, we investigate novice annotators’ uncertainty on the annotation of discourse relations on spoken conversational data. We find that dialogue context (single turn, pair of turns within speaker, and pair of turns across speakers) is a significant predictor of confidence scores. We compute distributed representations of discourse relations from co-occurrence statistics that incorporate information about confidence scores and dialogue context. We perform a hierarchical clustering analysis using these representations and show that weighting discourse relation representations with information about confidence and dialogue context coherently models our annotators’ uncertainty about discourse relation labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.50.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.50.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--50" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.50" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.50/">Investigating the Representation of Open Domain Dialogue Context for Transformer Models</a></strong><br><a href="/people/v/vishakh-padmakumar/">Vishakh Padmakumar</a>
|
<a href="/people/b/behnam-hedayatnia/">Behnam Hedayatnia</a>
|
<a href="/people/d/di-jin/">Di Jin</a>
|
<a href="/people/p/patrick-l-lange/">Patrick Lange</a>
|
<a href="/people/s/seokhwan-kim/">Seokhwan Kim</a>
|
<a href="/people/n/nanyun-peng/">Nanyun Peng</a>
|
<a href="/people/y/yang-liu-icsi/">Yang Liu</a>
|
<a href="/people/d/dilek-hakkani-tur/">Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--50"><div class="card-body p-3 small">The bulk of work adapting transformer models to open-domain dialogue represents dialogue context as the concatenated set of turns in natural language. However, it is unclear if this is the best approach. In this work, we investigate this question by means of an empirical controlled experiment varying the dialogue context format from text-only formats (all recent utterances, summaries, selected utterances) as well as variants that are more structurally different (triples, AMR). We compare these formats based on fine-tuned model performance on two downstream tasks—knowledge selection and response generation. We find that simply concatenating the utterances works as a strong baseline in most cases, but is outperformed in longer contexts by a hybrid approach of combining a summary of the context with recent utterances. Through empirical analysis, our work highlights the need to examine the format of context representation and offers recommendations on adapting general-purpose language models to dialogue tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.51.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.51.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--51" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.51" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.51/">C3: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues</a></strong><br><a href="/people/h/hung-le/">Hung Le</a>
|
<a href="/people/n/nancy-chen/">Nancy Chen</a>
|
<a href="/people/s/steven-c-h-hoi/">Steven C.H. Hoi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--51"><div class="card-body p-3 small">Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partially accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning (C3) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual samples based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositional output tokens to optimize the representation space in a generation setting. We achieved promising performance gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the benefits of our approach in grounding video and dialogue context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.52.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.52.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--52" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.52" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.52/">No that’s not what <span class="acl-fixed-case">I</span> meant: Handling Third Position Repair in Conversational Question Answering</a></strong><br><a href="/people/v/vevake-balaraman/">Vevake Balaraman</a>
|
<a href="/people/a/arash-eshghi/">Arash Eshghi</a>
|
<a href="/people/i/ioannis-konstas/">Ioannis Konstas</a>
|
<a href="/people/i/ioannis-papaioannou/">Ioannis Papaioannou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--52"><div class="card-body p-3 small">The ability to handle miscommunication is crucial to robust and faithful conversational AI. People usually deal with miscommunication immediately as they detect it, using highly systematic interactional mechanisms called repair. One important type of repair is Third Position Repair (TPR) whereby a speaker is initially misunderstood but then corrects the misunderstanding as it becomes apparent after the addressee’s erroneous response. Here, we collect and publicly release REPAIR-QA, the first large dataset of TPRs in a conversational question answering (QA) setting. The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for execution of TPRs. We demonstrate the usefulness of the data by training and evaluating strong baseline models for executing TPRs. For stand-alone TPR execution, we perform both automatic and human evaluations on a fine-tuned T5 model, as well as OpenAI’s GPT-3 LLMs. Additionally, we extrinsically evaluate the LLMs’ TPR processing capabilities in the downstream conversational QA task. The results indicate poor out-of-the-box performance on TPR’s by the GPT-3 models, which then significantly improves when exposed to REPAIR-QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.53.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.53.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--53" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.53" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.53/">When to generate hedges in peer-tutoring interactions</a></strong><br><a href="/people/a/alafate-abulimiti/">Alafate Abulimiti</a>
|
<a href="/people/c/chloe-clavel/">Chloé Clavel</a>
|
<a href="/people/j/justine-cassell/">Justine Cassell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--53"><div class="card-body p-3 small">This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviors. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models, including MLP and LSTM. The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model’s performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.54.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.54.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--54" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.54" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.54/"><span class="acl-fixed-case">P</span>aper<span class="acl-fixed-case">P</span>ersi<span class="acl-fixed-case">C</span>hat: Scientific Paper Discussion Chatbot using Transformers and Discourse Flow Management</a></strong><br><a href="/people/a/alexander-chernyavskiy/">Alexander Chernyavskiy</a>
|
<a href="/people/m/max-bregeda/">Max Bregeda</a>
|
<a href="/people/m/maria-nikiforova/">Maria Nikiforova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--54"><div class="card-body p-3 small">The rate of scientific publications is increasing exponentially, necessitating a significant investment of time in order to read and comprehend the most important articles. While ancillary services exist to facilitate this process, they are typically closed-model and paid services or have limited capabilities. In this paper, we present <i>PaperPersiChat</i>, an open chatbot-system designed for the discussion of scientific papers. This system supports summarization and question-answering modes within a single end-to-end chatbot pipeline, which is guided by discourse analysis. To expedite the development of similar systems, we also release the gathered dataset, which has no publicly available analogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.55.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.55.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--55" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.55" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.55/"><span class="acl-fixed-case">F</span>ur<span class="acl-fixed-case">C</span>hat: An Embodied Conversational Agent using <span class="acl-fixed-case">LLM</span>s, Combining Open and Closed-Domain Dialogue with Facial Expressions</a></strong><br><a href="/people/n/neeraj-cherakara/">Neeraj Cherakara</a>
|
<a href="/people/f/finny-varghese/">Finny Varghese</a>
|
<a href="/people/s/sheena-shabana/">Sheena Shabana</a>
|
<a href="/people/n/nivan-nelson/">Nivan Nelson</a>
|
<a href="/people/a/abhiram-karukayil/">Abhiram Karukayil</a>
|
<a href="/people/r/rohith-kulothungan/">Rohith Kulothungan</a>
|
<a href="/people/m/mohammed-afil-farhan/">Mohammed Afil Farhan</a>
|
<a href="/people/b/birthe-nesset/">Birthe Nesset</a>
|
<a href="/people/m/meriam-moujahid/">Meriam Moujahid</a>
|
<a href="/people/t/tanvi-dinkar/">Tanvi Dinkar</a>
|
<a href="/people/v/verena-rieser/">Verena Rieser</a>
|
<a href="/people/o/oliver-lemon/">Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--55"><div class="card-body p-3 small">We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.56.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.56.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--56" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.56" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.56/">Towards Breaking the Self-imposed Filter Bubble in Argumentative Dialogues</a></strong><br><a href="/people/a/annalena-aicher/">Annalena Aicher</a>
|
<a href="/people/d/daniel-kornmueller/">Daniel Kornmueller</a>
|
<a href="/people/y/yuki-matsuda/">Yuki Matsuda</a>
|
<a href="/people/s/stefan-ultes/">Stefan Ultes</a>
|
<a href="/people/w/wolfgang-minker/">Wolfgang Minker</a>
|
<a href="/people/k/keiichi-yasumoto/">Keiichi Yasumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--56"><div class="card-body p-3 small">Human users tend to selectively ignore information that contradicts their pre-existing beliefs or opinions in their process of information seeking. These “self-imposed filter bubbles” (SFB) pose a significant challenge for cooperative argumentative dialogue systems aiming to build an unbiased opinion and a better understanding of the topic at hand. To address this issue, we develop a strategy for overcoming users’ SFB within the course of the interaction. By continuously modeling the user’s position in relation to the SFB, we are able to identify the respective arguments which maximize the probability to get outside the SFB and present them to the user. We implemented this approach in an argumentative dialogue system and evaluated in a laboratory user study with 60 participants to show its validity and applicability. The findings suggest that the strategy was successful in breaking users’ SFBs and promoting a more reflective and comprehensive discussion of the topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.57.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.57.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--57" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.57" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.57/">The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue</a></strong><br><a href="/people/g/gabriel-skantze/">Gabriel Skantze</a>
|
<a href="/people/a/a-seza-dogruoz/">A. Seza Doğruöz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--57"><div class="card-body p-3 small">There is a surge in interest in the development of open-domain chatbots, driven by the recent advancements of large language models. The “openness” of the dialogue is expected to be maximized by providing minimal information to the users about the common ground they can expect, including the presumed joint activity. However, evidence suggests that the effect is the opposite. Asking users to “just chat about anything” results in a very narrow form of dialogue, which we refer to as the “open-domain paradox”. In this position paper, we explain this paradox through the theory of common ground as the basis for human-like communication. Furthermore, we question the assumptions behind open-domain chatbots and identify paths forward for enabling common ground in human-computer dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.58.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.58.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--58" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.58" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.58/"><span class="acl-fixed-case">MERCY</span>: Multiple Response Ranking Concurrently in Realistic Open-Domain Conversational Systems</a></strong><br><a href="/people/s/sarik-ghazarian/">Sarik Ghazarian</a>
|
<a href="/people/b/behnam-hedayatnia/">Behnam Hedayatnia</a>
|
<a href="/people/d/di-jin/">Di Jin</a>
|
<a href="/people/s/sijia-liu/">Sijia Liu</a>
|
<a href="/people/n/nanyun-peng/">Nanyun Peng</a>
|
<a href="/people/y/yang-liu-icsi/">Yang Liu</a>
|
<a href="/people/d/dilek-hakkani-tur/">Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--58"><div class="card-body p-3 small">Automatic Evaluation (AE) and Response Selection (RS) models assign quality scores to various candidate responses and rank them in conversational setups. Prior response ranking research compares various models’ performance on synthetically generated test sets. In this work, we investigate the performance of model-based reference-free AE and RS models on our constructed response ranking datasets that mirror real-case scenarios of ranking candidates during inference time. Metrics’ unsatisfying performance can be interpreted as their low generalizability over more pragmatic conversational domains such as human-chatbot dialogs. To alleviate this issue we propose a novel RS model called MERCY that simulates human behavior in selecting the best candidate by taking into account distinct candidates concurrently and learns to rank them. In addition, MERCY leverages natural language feedback as another component to help the ranking task by explaining why each candidate response is relevant/irrelevant to the dialog context. These feedbacks are generated by prompting large language models in a few-shot setup. Our experiments show the better performance of MERCY over baselines for the response ranking task in our curated realistic datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.59.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.59.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--59" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.59" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.59/">Empathetic Response Generation for Distress Support</a></strong><br><a href="/people/a/anuradha-welivita/">Anuradha Welivita</a>
|
<a href="/people/c/chun-hung-yeh/">Chun-Hung Yeh</a>
|
<a href="/people/p/pearl-pu/">Pearl Pu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--59"><div class="card-body p-3 small">AI-driven chatbots are seen as an attractive solution to support people undergoing emotional distress. One of the main components of such a chatbot is the ability to empathize with the user. But a significant limitation in achieving this goal is the lack of a large dialogue dataset containing empathetic support for those undergoing distress. In this work, we curate a large-scale dialogue dataset that contains ≈1.3M peer support dialogues spanning across more than 4K distress-related topics. We analyze the empathetic characteristics of this dataset using statistical and visual means. To demonstrate the utility of this dataset, we train four baseline neural dialogue models that can respond empathetically to distress prompts. Two of the baselines adapt existing architecture and the other two incorporate a framework identifying levels of cognitive and emotional empathy in responses. Automatic and human evaluation of these models validate the utility of the dataset in generating empathetic responses for distress support and show that identifying levels of empathy in peer-support responses facilitates generating responses that are lengthier, richer in empathy, and closer to the ground truth.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.sigdial-1.60.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.sigdial-1.60.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--sigdial-1--60" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.sigdial-1.60" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.sigdial-1.60/">Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation</a></strong><br><a href="/people/y/yahui-fu/">Yahui Fu</a>
|
<a href="/people/k/koji-inoue/">Koji Inoue</a>
|
<a href="/people/c/chenhui-chu/">Chenhui Chu</a>
|
<a href="/people/t/tatsuya-kawahara/">Tatsuya Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--sigdial-1--60"><div class="card-body p-3 small">Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user’s experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user’s perspective, ignoring the system’s perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user’s perspective (user’s desires and reactions) and the system’s perspective (system’s intentions and reactions). We enhance ChatGPT’s ability to reason for the system’s perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.</div></div></div><hr><div id="2023cs4oa-1"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.cs4oa-1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.cs4oa-1/">Proceedings of the 1st Workshop on CounterSpeech for Online Abuse (CS4OA)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.cs4oa-1.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.cs4oa-1.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--cs4oa-1--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.cs4oa-1.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.cs4oa-1.1/">From Generic to Personalized: Investigating Strategies for Generating Targeted Counter Narratives against Hate Speech</a></strong><br><a href="/people/m/mekselina-doganc/">Mekselina Doğanç</a>
|
<a href="/people/i/ilia-markov/">Ilia Markov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--cs4oa-1--1"><div class="card-body p-3 small">The spread of hate speech (HS) in the digital age poses significant challenges, with online platforms becoming breeding grounds for harmful content. While many natural language processing (NLP) studies have focused on identifying hate speech, few have explored the generation of counter narratives (CNs) as means to combat it. Previous studies have shown that computational models often generate CNs that are dull and generic, and therefore do not resonate with hate speech authors. In this paper, we explore the personalization capabilities of computational models for generating more targeted and engaging CNs. This paper investigates various strategies for incorporating author profiling information into GPT-2 and GPT-3.5 models to enhance the personalization of CNs to combat online hate speech. We investigate the effectiveness of incorporating author profiling aspects, more specifically the age and gender information of HS authors, in tailoring CNs specifically targeted at HS spreaders. We discuss the challenges, opportunities, and future directions for incorporating user profiling information into CN interventions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.cs4oa-1.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.cs4oa-1.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--cs4oa-1--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.cs4oa-1.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.cs4oa-1.2/">Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization</a></strong><br><a href="/people/h/helena-bonaldi/">Helena Bonaldi</a>
|
<a href="/people/g/giuseppe-attanasio/">Giuseppe Attanasio</a>
|
<a href="/people/d/debora-nozza/">Debora Nozza</a>
|
<a href="/people/m/marco-guerini/">Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--cs4oa-1--2"><div class="card-body p-3 small">Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.cs4oa-1.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.cs4oa-1.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--cs4oa-1--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.cs4oa-1.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.cs4oa-1.3/">Distilling Implied Bias from Hate Speech for Counter Narrative Selection</a></strong><br><a href="/people/n/nami-akazawa/">Nami Akazawa</a>
|
<a href="/people/s/serra-sinem-tekiroglu/">Serra Sinem Tekiroğlu</a>
|
<a href="/people/m/marco-guerini/">Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--cs4oa-1--3"><div class="card-body p-3 small">Hate speech is a critical problem in our society and social media platforms are often an amplifier for this phenomenon. Recently the use of Counter Narratives (informative and non-aggressive responses) has been proposed as a viable solution to counter hateful content that goes beyond simple detection-removal strategies. In this paper we present a novel approach along this line of research, which utilizes the implied statement (bias) expressed in the hate speech to retrieve an appropriate counter narrative. To this end, we first trained and tested several LMs that, given a hateful post, generate the underlying bias and the target group. Then, for the counter narrative selection task, we experimented with several methodologies that either use or not use the implied bias during the process. Experiments show that using the target group information allows the system to better focus on relevant content and that implied statement for selecting counter narratives is better than the corresponding standard approach that does not use it. To our knowledge, this is the first attempt to build an automatic selection tool that uses hate speech implied bias to drive Counter Narrative selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.cs4oa-1.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.cs4oa-1.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--cs4oa-1--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.cs4oa-1.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.cs4oa-1.4/">Just Collect, Don’t Filter: Noisy Labels Do Not Improve Counterspeech Collection for Languages Without Annotated Resources</a></strong><br><a href="/people/p/pauline-mohle/">Pauline Möhle</a>
|
<a href="/people/m/matthias-orlikowski/">Matthias Orlikowski</a>
|
<a href="/people/p/philipp-cimiano/">Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--cs4oa-1--4"><div class="card-body p-3 small">Counterspeech on social media is rare. Consequently, it is difficult to collect naturally occurring examples, in particular for languages without annotated datasets. In this work, we study methods to increase the relevance of social media samples for counterspeech annotation when we lack annotated resources. We use the example of sourcing German data for counterspeech annotations from Twitter. We monitor tweets from German politicians and activists to collect replies. To select relevant replies we a) find replies that match German abusive keywords or b) label replies for counterspeech using a multilingual classifier fine-tuned on English data. For both approaches and a baseline setting, we annotate a random sample and use bootstrap sampling to estimate the amount of counterspeech. We find that neither the multilingual model nor the keyword approach achieve significantly higher counts of true counterspeech than the baseline. Thus, keyword lists or multi-lingual classifiers are likely not worth the added complexity beyond purposive data collection: Already without additional filtering, we gather a meaningful sample with 7,4% true counterspeech.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.cs4oa-1.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.cs4oa-1.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--cs4oa-1--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.cs4oa-1.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.cs4oa-1.5/">What Makes Good Counterspeech? A Comparison of Generation Approaches and Evaluation Metrics</a></strong><br><a href="/people/y/yi-zheng/">Yi Zheng</a>
|
<a href="/people/b/bjorn-ross/">Björn Ross</a>
|
<a href="/people/w/walid-magdy/">Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--cs4oa-1--5"><div class="card-body p-3 small">Counterspeech has been proposed as a solution to the proliferation of online hate. Research has shown that natural language processing (NLP) approaches could generate such counterspeech automatically, but there are competing ideas for how NLP models might be used for this task and a variety of evaluation metrics whose relationship to one another is unclear. We test three different approaches and collect ratings of the generated counterspeech for 1,740 tweet-participant pairs to systematically compare the counterspeech on three aspects: quality, effectiveness and user preferences. We examine which model performs best at which metric and which aspects of counterspeech predict user preferences. A free-form text generation approach using ChatGPT performs the most consistently well, though its generations are occasionally unspecific and repetitive. In our experiment, participants’ preferences for counterspeech are predicted by the quality of the counterspeech, not its perceived effectiveness. The results can help future research approach counterspeech evaluation more systematically.</div></div></div><hr><div id="2023dstc-1"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.dstc-1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.dstc-1/">Proceedings of The Eleventh Dialog System Technology Challenge</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.1/">Exploring Prompt-based Multi-task Learning for Multimodal Dialog State Tracking and Immersive Multimodal Conversation</a></strong><br><a href="/people/y/yirong-chen/">Yirong Chen</a>
|
<a href="/people/y/ya-li/">Ya Li</a>
|
<a href="/people/t/tao-wang/">Tao Wang</a>
|
<a href="/people/x/xiaofen-xing/">Xiaofen Xing</a>
|
<a href="/people/x/xiangmin-xu/">Xiangmin Xu</a>
|
<a href="/people/q/quan-liu/">Quan Liu</a>
|
<a href="/people/c/cong-liu/">Cong Liu</a>
|
<a href="/people/g/guoping-hu/">Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--1"><div class="card-body p-3 small">With the rise of the metaverse, immersive multimodal conversation has attracted more and more researchers’ attention. Multimodal contexts will become more important for human-computer interaction in the metaverse, especially in shopping domain. Unlike traditional conversation tasks, immersive multimodal conversation has challenges such as multimodal ambiguous candidate identification and multimodal coreference resolution, which makes it more difficult to dialog state tracking and response generation, as described in SIMMC 2.1 challenge, a part of DSTC11. In particular, as the number of objects in the scene increases, the difficulty will increase dramatically. We proposed a prompt-based multi-task learning Encoder-Decoder, in which different subtasks use different prompts to make the model tend to focus on the current subtask. We achieve the winner in ambiguous candidates indentification and runner-up in multimodal coreference resolution (MM-Coref), multimodal dialog state tracking (MM-DST) and assistant response generation. Our code and model are made publicly available at https://github.com/scutcyr/dstc11-simmc2.1-scut-bds-lab.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.2/">Multi-Task Learning for Ambiguous Candidate Identification with Pre-trained Model</a></strong><br><a href="/people/d/daesik-jang/">Daesik Jang</a>
|
<a href="/people/h/hyewon-choi/">Hyewon Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--2"><div class="card-body p-3 small">Recently, research using multimodal datasets containing image and text information has been conducted actively. One of them is the SIMMC2.1 dataset. It is a more complicated dataset than answering a conversation using only text because it should predict an answer after understanding the relationship between images and text. Therefore, there are limitations to answering a conversation only using text-based models such as BERT or GPT-2, so models with both image and language understanding abilities should be considered. We propose a new model that is effective for the ambiguous candidate identification task in DSTC11 SIMMC2.1 Tark. It consists of a simple pipeline model structure, which has two steps. The first step is to check whether there is ambiguity in the current user utterance, and the second step is to extract objects mentioned in the ambiguous utterance of the user. We suggest a new learning framework with a pre-trained image model and text model that is effective for the ambiguous candidate identification task. Experiments show that the proposed method can improve the model performance, and our model achieved 3rd place in sub-task 1 of the SIMMC2.1 track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.3/">Improving Situated Conversational Agents with Step-by-Step Multi-modal Logic Reasoning</a></strong><br><a href="/people/y/yuxing-long/">Yuxing Long</a>
|
<a href="/people/h/huibin-zhang/">Huibin Zhang</a>
|
<a href="/people/b/binyuan-hui/">Binyuan Hui</a>
|
<a href="/people/z/zhenglu-yang/">Zhenglu Yang</a>
|
<a href="/people/c/caixia-yuan/">Caixia Yuan</a>
|
<a href="/people/x/xiaojie-wang/">Xiaojie Wang</a>
|
<a href="/people/f/fei-huang/">Fei Huang</a>
|
<a href="/people/y/yongbin-li/">Yongbin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--3"><div class="card-body p-3 small">To fulfill complex user requirements in a situated conversational scenario, the agent needs to conduct step-by-step multi-modal logic reasoning, which includes locating objects, querying information and searching objects. However, existing methods omit this multi-step procedure and therefore constitutes the risk of shortcuts when making predictions. For example, they may directly copy the information from the dialogue history or simply use the textual description without perform visual reasoning. To address this issue and further boost the system performance, we apply the dual process theory to plug a reasoner into the original transformer based model for step-by-step reasoning. When system 2 completes multi-step reasoning, its output is regarded as final prediction. Our proposed method achieved the 1st rank on the summing scores across all four DSTC-11 SIMMC 2.1 sub-tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.4/">Contrastively Pretrained Vision-Language Transformers and Domain Adaptation Methods for Multimodal <span class="acl-fixed-case">TOD</span> Systems</a></strong><br><a href="/people/y/youngjae-chang/">Youngjae Chang</a>
|
<a href="/people/d/doo-young-kim/">Doo Young Kim</a>
|
<a href="/people/j/jinyoung-kim/">Jinyoung Kim</a>
|
<a href="/people/k/keunha-kim/">Keunha Kim</a>
|
<a href="/people/h/hyunmook-cha/">Hyunmook Cha</a>
|
<a href="/people/s/suyoung-min/">Suyoung Min</a>
|
<a href="/people/y/youngjoong-ko/">Youngjoong Ko</a>
|
<a href="/people/k/kye-hwan-lee/">Kye-Hwan Lee</a>
|
<a href="/people/j/joonwoo-park/">Joonwoo Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--4"><div class="card-body p-3 small">The Situated Interactive MultiModal Conversations (SIMMC2.1) Challenge 2022 is hosted by the Eleventh Dialog System Technology Challenge (DSTC11). This is the third consecutive year multimodal dialog systems have been selected as an official track of the competition, promoted by the continued interest in the research community. The task of SIMMC is to create a shopping assistant agent that can communicate with customers in a virtual store. It requires processing store scenes and product catalogs along with the customer’s request. The task is decomposed into four steps and each becomes a subtask. In this work, we explore the common approaches to modeling multimodality and find the method with the most potential. We also identify a discrepancy in using pretrained language models for dialog tasks and devise a simple domain-adaptation method. Our model came in third place for object coreferencing, dialog state tracking, and response generation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.5/">Multi-Stage Coarse-to-Fine Contrastive Learning for Conversation Intent Induction</a></strong><br><a href="/people/c/caiyuan-chu/">Caiyuan Chu</a>
|
<a href="/people/y/ya-li/">Ya Li</a>
|
<a href="/people/y/yifan-liu/">Yifan Liu</a>
|
<a href="/people/j/jia-chen-gu/">Jia-Chen Gu</a>
|
<a href="/people/q/quan-liu/">Quan Liu</a>
|
<a href="/people/y/yongxin-ge/">Yongxin Ge</a>
|
<a href="/people/g/guoping-hu/">Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--5"><div class="card-body p-3 small">Intent recognition is critical for task-oriented dialogue systems. However, for emerging domains and new services, it is difficult to accurately identify the key intent of a conversation due to time-consuming data annotation and comparatively poor model transferability. Therefore, the automatic induction of dialogue intention is very important for intelligent dialogue systems. This paper presents our solution to Track 2 of Intent Induction from Conversations for Task-Oriented Dialogue at the Eleventh Dialogue System Technology Challenge (DSTC11). The essence of intention clustering lies in distinguishing the representation of different dialogue utterances. The key to automatic intention induction is that, for any given set of new data, the sentence representation obtained by the model can be well distinguished from different labels. Therefore, we propose a multi-stage coarse-to-fine contrastive learning model training scheme including unsupervised contrastive learning pre-training, supervised contrastive learning pre-training, and fine-tuning with joint contrastive learning and clustering to obtain a better dialogue utterance representation model for the clustering task. In the released DSTC11 Track 2 evaluation results, our proposed system ranked first on both of the two subtasks of this Track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.6.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.6.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--6" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.6" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.6/"><span class="acl-fixed-case">DORIC</span> : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing</a></strong><br><a href="/people/j/jihyun-lee/">Jihyun Lee</a>
|
<a href="/people/s/seungyeon-seo/">Seungyeon Seo</a>
|
<a href="/people/y/yunsu-kim/">Yunsu Kim</a>
|
<a href="/people/g/gary-geunbae-lee/">Gary Geunbae Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--6"><div class="card-body p-3 small">We present our work on Track 2 in the Dialog System Technology Challenges 11 (DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot, cross-domain, intent-set induction. In the absence of in-domain training dataset, robust utterance representation that can be used across domains is necessary to induce users’ intentions. To achieve this, we leveraged a multi-domain dialogue dataset to fine-tune the language model and proposed extracting Verb-Object pairs to remove the artifacts of unnecessary information. Furthermore, we devised the method that generates each cluster’s name for the explainability of clustered results. Our approach achieved 3rd place in the precision score and showed superior accuracy and normalized mutual information (NMI) score than the baseline model on various domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.7.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.7.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--7" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.7" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.7/">A Two-Stage Progressive Intent Clustering for Task-Oriented Dialogue</a></strong><br><a href="/people/b/bingzhu-du/">Bingzhu Du</a>
|
<a href="/people/n/nan-su/">Nan Su</a>
|
<a href="/people/y/yuchi-zhang/">Yuchi Zhang</a>
|
<a href="/people/y/yongliang-wang/">Yongliang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--7"><div class="card-body p-3 small">Natural Language Understanding (NLU) is one of the most critical components of task-oriented dialogue, and it is often considered as an intent classification task. To achieve outstanding intent identification performance, system designers often need to hire a large number of domain experts to label the data, which is inefficient and costly. To address this problem, researchers’ attention has gradually shifted to automatic intent clustering methods, which employ low-resource unsupervised approaches to solve classification problems. The classical framework for clustering is deep clustering, which uses deep neural networks (DNNs) to jointly optimize non-clustering loss and clustering loss. However, for new conversational domains or services, utterances required to assign intents are scarce and the performance of DNNs is often dependent on large amounts of data. In addition, although re-clustering with k-means algorithm after training the network usually leads to better results, k-means methods often suffer from poor stability. To address these problems, we propose an effective two-stage progressive approach to refine the clustering. Firstly, we pre-train the network with contrastive loss using all conversations data and then optimize the clustering loss and contrastive loss simultaneously. Secondly, we propose adaptive progressive k-means to alleviate the randomness of vanilla k-means, achieving better performance and smaller deviation. Our method ranks second in DSTC11 Track2 Task 1, a benchmark for intent clustering of task-oriented dialogue, demonstrating the superiority and effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.8.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.8.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--8" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.8" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.8/">Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue</a></strong><br><a href="/people/j/jeiyoon-park/">Jeiyoon Park</a>
|
<a href="/people/y/yoonna-jang/">Yoonna Jang</a>
|
<a href="/people/c/chanhee-lee/">Chanhee Lee</a>
|
<a href="/people/h/heui-seok-lim/">Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--8"><div class="card-body p-3 small">The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.9.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.9.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--9" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.9" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.9/">Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer</a></strong><br><a href="/people/h/hyukhun-koh/">Hyukhun Koh</a>
|
<a href="/people/h/haesung-pyun/">Haesung Pyun</a>
|
<a href="/people/n/nakyeong-yang/">Nakyeong Yang</a>
|
<a href="/people/k/kyomin-jung/">Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--9"><div class="card-body p-3 small">In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multiview model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly. Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.10.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.10.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--10" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.10" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.10/">Adapting Text-based Dialogue State Tracker for Spoken Dialogues</a></strong><br><a href="/people/j/jaeseok-yoon/">Jaeseok Yoon</a>
|
<a href="/people/s/seunghyun-hwang/">Seunghyun Hwang</a>
|
<a href="/people/h/han-ran/">Han Ran</a>
|
<a href="/people/j/jeong-uk-bang/">Jeong-Uk Bang</a>
|
<a href="/people/k/kee-eung-kim/">Kee-Eung Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--10"><div class="card-body p-3 small">Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written cor- pora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, and (3) post-processing for recovering the error of the estimated slot value. Our experiments show that it is important to use an explicit automatic speech recognition error correction module, post-processing, and data augmentation to adapt a text-based dialogue state tracker for spoken dialogue corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.11.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.11.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--11" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.11" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.11/"><span class="acl-fixed-case">C</span>opy<span class="acl-fixed-case">T</span>5: Copy Mechanism and Post-Trained T5 for Speech-Aware Dialogue State Tracking System</a></strong><br><a href="/people/c/cheonyoung-park/">Cheonyoung Park</a>
|
<a href="/people/e/eunji-ha/">Eunji Ha</a>
|
<a href="/people/y/yewon-jeong/">Yewon Jeong</a>
|
<a href="/people/c/chi-young-kim/">Chi-young Kim</a>
|
<a href="/people/h/haeun-yu/">Haeun Yu</a>
|
<a href="/people/j/joo-won-sung/">Joo-won Sung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--11"><div class="card-body p-3 small">In a real-world environment, Dialogue State Tracking (DST) should use speech recognition results to perform tasks. However, most existing DST research has been conducted in text-based environments. This study aims to build a model that efficiently performs Automatic Speech Recognition-based DST. To operate robustly against speech noise, we used CopyT5, which adopted a copy mechanism, and trained the model using augmented data including speech noise. Furthermore, CopyT5 performed post-training using the masked language modeling method with the MultiWOZ dataset in T5 in order to learn the dialogue context better. The copy mechanism also mitigated name entity errors that may occur during DST generation. Experiments confirmed that data augmentation, post-training, and the copy mechanism effectively improve DST performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.12.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.12.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--12" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.12" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.12/"><span class="acl-fixed-case">OLISIA</span>: a Cascade System for Spoken Dialogue State Tracking</a></strong><br><a href="/people/l/leo-jacqmin/">Léo Jacqmin</a>
|
<a href="/people/l/lucas-druart/">Lucas Druart</a>
|
<a href="/people/y/yannick-esteve/">Yannick Estève</a>
|
<a href="/people/b/benoit-favre/">Benoît Favre</a>
|
<a href="/people/l/lina-m-rojas/">Lina M Rojas</a>
|
<a href="/people/v/valentin-vielzeuf/">Valentin Vielzeuf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--12"><div class="card-body p-3 small">Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language. In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations. With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.13.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.13.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--13" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.13" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.13/">Speech-Aware Multi-Domain Dialogue State Generation with <span class="acl-fixed-case">ASR</span> Error Correction Modules</a></strong><br><a href="/people/r/ridong-jiang/">Ridong Jiang</a>
|
<a href="/people/w/wei-shi/">Wei Shi</a>
|
<a href="/people/b/bin-wang/">Bin Wang</a>
|
<a href="/people/c/chen-zhang/">Chen Zhang</a>
|
<a href="/people/y/yan-zhang/">Yan Zhang</a>
|
<a href="/people/c/chunlei-pan/">Chunlei Pan</a>
|
<a href="/people/j/jung-jae-kim/">Jung Jae Kim</a>
|
<a href="/people/h/haizhou-li/">Haizhou Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--13"><div class="card-body p-3 small">Prior research on dialogue state tracking (DST) is mostly based on written dialogue corpora. For spoken dialogues, the DST model trained on the written text should use the results (or hypothesis) of automatic speech recognition (ASR) as input. But ASR hypothesis often includes errors, which leads to significant performance drop for spoken dialogue state tracking. We address the issue by developing the following ASR error correction modules. First, we train a model to convert ASR hypothesis to ground truth user utterance, which can fix frequent patterns of errors. The model takes ASR hypotheses of two ASR models as input and fine-tuned in two stages. The corrected hypothesis is fed into a large scale pre-trained encoder-decoder model (T5) for DST training and inference. Second, if an output slot value from the encoder-decoder model is a name, we compare it with names in a dictionary crawled from Web sites and, if feasible, replace with the crawled name of the shortest edit distance. Third, we fix errors of temporal expressions in ASR hypothesis by using hand-crafted rules. Experiment results on the DSTC 11 speech-aware dataset, which is built on the popular MultiWOZ task (version 2.1), show that our proposed method can effectively mitigate the performance drop when moving from written text to spoken conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.14.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.14.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--14" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.14" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.14/">Three Ways of Using Large Language Models to Evaluate Chat</a></strong><br><a href="/people/o/ondrej-platek/">Ondřej Plátek</a>
|
<a href="/people/v/vojtech-hudecek/">Vojtech Hudecek</a>
|
<a href="/people/p/patricia-schmidtova/">Patricia Schmidtova</a>
|
<a href="/people/m/mateusz-lango/">Mateusz Lango</a>
|
<a href="/people/o/ondrej-dusek/">Ondrej Dusek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--14"><div class="card-body p-3 small">This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.15.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.15.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--15" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.15" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.15/">Parallel Corpora Alignment Framework for Multilingual and Robust Automatic Dialogue Evaluation</a></strong><br><a href="/people/x/xinglin-wang/">Xinglin Wang</a>
|
<a href="/people/j/jiayi-shi/">Jiayi Shi</a>
|
<a href="/people/p/peiwen-yuan/">Peiwen Yuan</a>
|
<a href="/people/k/kan-li/">Kan Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--15"><div class="card-body p-3 small">Open-domain automatic dialogue evaluation plays an important role in dialogue systems. While recent efforts are being put into making learning-based evaluation metrics correlate better with human evaluation, robust metrics for parallel corpora and multiple domains remain unexplored. Parallel corpora refer to corpora that express the same idea in different ways (e.g., translation, paraphrasing and back-translation). In this paper, we propose Parallel Corpora Alignment Framework (PCAF), which improves the consistency and robustness of model evaluation on parallel corpora. Firstly, parallel corpora are aligned in semantic space through parallel-corpora-aligned contrastive learning. Then, parallel-corpora-aligned distillation on multi-dataset is applied to further improve model’s generalization ability across multiple data domains. Our approach ranks second on the final test data of DSTC11 track4 subtask1 (“Multilingual Automatic Evaluation Metrics”, turn-level) and third on the subtask2 (“Robust Automatic Evaluation Metrics”, turn-level), which proves the strong generalization ability and robustness of our proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.16.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.16.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--16" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.16" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.16/">Simple <span class="acl-fixed-case">LLM</span> Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation</a></strong><br><a href="/people/j/john-mendonca/">John Mendonça</a>
|
<a href="/people/p/patricia-pereira/">Patrícia Pereira</a>
|
<a href="/people/h/helena-moniz/">Helena Moniz</a>
|
<a href="/people/j/joao-paulo-carvalho/">Joao Paulo Carvalho</a>
|
<a href="/people/a/alon-lavie/">Alon Lavie</a>
|
<a href="/people/i/isabel-trancoso/">Isabel Trancoso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--16"><div class="card-body p-3 small">Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 “Automatic Evaluation Metrics for Open-Domain Dialogue Systems”, proving the evaluation capabilities of prompted LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.17.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.17.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--17" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.17" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.17/">Towards Optimizing Pre-trained Language Model Ensemble Learning for Task-oriented Dialogue System</a></strong><br><a href="/people/z/zhiyuan-zhu/">Zhiyuan Zhu</a>
|
<a href="/people/y/yusheng-liao/">Yusheng Liao</a>
|
<a href="/people/z/zhe-chen/">Zhe Chen</a>
|
<a href="/people/y/yu-wang/">Yu Wang</a>
|
<a href="/people/y/yunfeng-guan/">Yunfeng Guan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--17"><div class="card-body p-3 small">Task-oriented dialogue systems that employ external knowledge to generate informative responses have become an important field of research. This paper outlines our contribution to Track 5 of the Eleventh Dialog System Technology Challenge (DSTC11), which focuses on constructing high-performing, subjective knowledge-enriched task-oriented dialogue systems. Specifically, we investigate the complementarity of various language models to tackle the diverse knowledge selection task that involves multiple external sources. Based on this investigation, we propose pre- and post-generation model ensemble approaches to mitigate potential biases inherent in using a single model for the knowledge selection task. Finally, we utilize the consensus decoding approach to combine fine-tuned ensemble models and improve the performance of the generation system. Our system ranked 1st in human evaluation, even outperforming human annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.18.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.18.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--18" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.18" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.18/">Enhancing Task-Oriented Dialog System with Subjective Knowledge: A Large Language Model-based Data Augmentation Framework</a></strong><br><a href="/people/h/haein-jung/">Haein Jung</a>
|
<a href="/people/h/heuiyeen-yeen/">Heuiyeen Yeen</a>
|
<a href="/people/j/jeehyun-lee/">Jeehyun Lee</a>
|
<a href="/people/m/minju-kim/">Minju Kim</a>
|
<a href="/people/n/namo-bang/">Namo Bang</a>
|
<a href="/people/m/myoung-wan-koo/">Myoung-Wan Koo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--18"><div class="card-body p-3 small">As Task-Oriented Dialog (TOD) systems have advanced, structured DB systems, which aim to collect relevant knowledge for answering user’s questions, have also progressed. Despite these advancements, these methods face challenges when dealing with subjective questions from users. To overcome this, DSTC11 released a subjective-knowledge-based TOD (SK-TOD) dataset and benchmark. This paper introduces a framework that effectively solves SK-TOD tasks by leveraging a Large Language Model (LLM). We demonstrate the proficient use of LLM for each sub-task, including an adapters-based method and knowledge-grounded data augmentation. Our proposed methods, which utilize LLM as an efficient tool, outperform baseline performance and approaches that directly use LLM as a one-step sub-task solver, showing superior task-specific optimization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.19.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.19.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--19" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.19" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.19/">Semantic data augmentation for meaning maintenance on Task-Oriented Conversation with Large-size Language Model</a></strong><br><a href="/people/j/jaehwan-lee/">Jaehwan Lee</a>
|
<a href="/people/k/kwanyoung-son/">Kwanyoung Son</a>
|
<a href="/people/e/eugene-kim/">Eugene Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--19"><div class="card-body p-3 small">This paper presents our approach to building a generalized model for Track 5 in DSTC11: “Task-oriented Conversational Modeling with Subjective Knowledge” which addresses the challenge of generating responses to users’ utterances based on a variety of factual and subjective knowledge. To tackle this challenge, we first augmented the training data by leveraging contextual word embedding and back translation, thereby increasing the quantity of available data. Then, we utilized a large-size language model to enhance the acceptability of the augmented data and fine-tuned the model using augmented data. Specifically, we applied the DeBERTa-v3-large model for knowledge detection and selection, and the BART-large model for response generation. Our best model achieved the seventh rank in the objective evaluation and the second rank in the final official human evaluation. These outcomes serve as solid evidence that data augmentation and using a large-size model were highly effective for developing a conversational model system that incorporates objective and subjective knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.20.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.20.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--20" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.20" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.20/">Ensemble Method via Ranking Model for Conversational Modeling with Subjective Knowledge</a></strong><br><a href="/people/x/xin-huang/">Xin Huang</a>
|
<a href="/people/k/kye-min-tan/">Kye Min Tan</a>
|
<a href="/people/r/richeng-duan/">Richeng Duan</a>
|
<a href="/people/b/bowei-zou/">Bowei Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--20"><div class="card-body p-3 small">This paper describes our submission to the fifth track of the 11th Dialog System Technology Challenge (DSTC-11), which focuses on “Task-oriented Conversational Modeling with Subjective Knowledge”. We focus on response generation and leverage a ranking strategy to ensemble individual models of BART, Long-T5, and a fine-tuned large language model based on LLaMA. The strategy is supplemented by other techniques like low rank adaptation to maintain efficient utilization of these large models while still achieving optimal performance. The experiments show that the ensemble method outperforms individual models and the baseline method. Our model was ranked 1st place in ROUGE_1, 2nd place in ROUGE_L score and 4th place in human evaluation among a total of 14 participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.21.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.21.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--21" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.21" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.21/">Exploring Back Translation with Typo Noise for Enhanced Inquiry Understanding in Task-Oriented Dialogue</a></strong><br><a href="/people/j/jihyun-lee/">Jihyun Lee</a>
|
<a href="/people/j/junseok-kim/">Junseok Kim</a>
|
<a href="/people/g/gary-geunbae-lee/">Gary Geunbae Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--21"><div class="card-body p-3 small">This paper presents our approach to the DSTC11 Track 5 selection task, which focuses on retrieving appropriate natural language knowledge sources for task-oriented dialogue. We propose typologically diverse back-translation method with typo noise, which could generate various structured user inquries. Through our noised back translation, we augmented inquiries by combining three different typologies of language sources with five different typo noise injections. Our experiments demonstrate that typological variety and typo noise aids the model in generalizing to diverse user inquiries in dialogue. In the competition, where 14 teams participated, our approach achieved the 5th rank for exact matching metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.22.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.22.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--22" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.22" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.22/">Leveraging Few-Shot Data Augmentation and Waterfall Prompting for Response Generation</a></strong><br><a href="/people/l/lea-krause/">Lea Krause</a>
|
<a href="/people/s/selene-baez-santamaria/">Selene Báez Santamaría</a>
|
<a href="/people/m/michiel-van-der-meer/">Michiel van der Meer</a>
|
<a href="/people/u/urja-khurana/">Urja Khurana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--22"><div class="card-body p-3 small">This paper discusses our approaches for task-oriented conversational modelling using subjective knowledge, with a particular emphasis on response generation. Our methodology was shaped by an extensive data analysis that evaluated key factors such as response length, sentiment, and dialogue acts present in the provided dataset. We used few-shot learning to augment the data with newly generated subjective knowledge items and present three approaches for DSTC11: (1) task-specific model exploration, (2) incorporation of the most frequent question into all generated responses, and (3) a waterfall prompting technique using a combination of both GPT-3 and ChatGPT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.23.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.23.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--23" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.23" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.23/">Leveraging Ensemble Techniques and Metadata for Subjective Knowledge-grounded Conversational Systems</a></strong><br><a href="/people/s/seongho-joo/">Seongho Joo</a>
|
<a href="/people/k/kang-il-lee/">Kang-il Lee</a>
|
<a href="/people/k/kyungmin-min/">Kyungmin Min</a>
|
<a href="/people/j/joongbo-shin/">Joongbo Shin</a>
|
<a href="/people/j/janghoon-han/">Janghoon Han</a>
|
<a href="/people/s/seungpil-won/">Seungpil Won</a>
|
<a href="/people/k/kyomin-jung/">Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--23"><div class="card-body p-3 small">The goal of DSTC11 track 5 is to build task-oriented dialogue systems that can effectively utilize external knowledge sources such as FAQs and reviews. This year’s challenge differs from previous ones as it includes subjective knowledge snippets and requires multiple snippets for a single turn. We propose a pipeline system for the challenge focusing on entity tracking, knowledge selection and response generation. Specifically, we devise a novel heuristic to ensemble the outputs from the rule-based method and neural model for entity tracking and knowledge selection. We also leverage metadata information in the knowledge source to handle fine-grained user queries. Our approach achieved the first place in objective evaluation and the third place in human evaluation of DSTC11 track 5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.24.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.24.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--24" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.24" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.24/">A Difference-aware Ensemble Method for Task-oriented Dialogue with Subjective Knowledge</a></strong><br><a href="/people/c/changxin-ke/">Changxin Ke</a>
|
<a href="/people/c/churui-sun/">Churui Sun</a>
|
<a href="/people/l/longxuan-ma/">Longxuan Ma</a>
|
<a href="/people/w/weinan-zhang/">Wei-Nan Zhang</a>
|
<a href="/people/t/ting-liu/">Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--24"><div class="card-body p-3 small">We participate in the 11th Dialog System Technology Challenges (DSTC) track-5 called Task-oriented Conversational Modeling with Subjective Knowledge. Introducing subjective knowledge into task-oriented dialogue (TOD) can help the DS to understand variables of subjective user needs and to suit more dialogue scenarios. Track-5 includes several sub-tasks: 1) knowledge-seeking turn detection; 2) knowledge entity tracking; 3) knowledge entry selection; and 4) use of the selected knowledge entries for response generation. Besides the challenges of each sub-tasks own, there are two challenges across different sub-tasks. The first is that there are multiple valid knowledge entries for each knowledge-seeking turn, the accuracy of the knowledge entry selection is important for the quality of response generation. The second challenge is how to address the unseen dialogue/entities/entries in the validation and the test set. In this paper, we propose a difference-aware ensemble method to address these sub-tasks and the two challenges mentioned above. Our method helps to obtain more robust results and performs well on unseen instances. Among all the submissions for the test set, our method ranks 1st on the knowledge-seeking turn detection task and achieves 3rd on the overall automatic evaluation score. Our code and data will be released on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.25.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.25.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--25" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.25" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.25/"><span class="acl-fixed-case">DSTC</span>-11: Speech Aware Task-Oriented Dialog Modeling Track</a></strong><br><a href="/people/h/hagen-soltau/">Hagen Soltau</a>
|
<a href="/people/i/izhak-shafran/">Izhak Shafran</a>
|
<a href="/people/m/mingqiu-wang/">Mingqiu Wang</a>
|
<a href="/people/a/abhinav-rastogi/">Abhinav Rastogi</a>
|
<a href="/people/w/wei-han/">Wei Han</a>
|
<a href="/people/y/yuan-cao/">Yuan Cao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--25"><div class="card-body p-3 small">Most research on task oriented dialog modeling is based on written text input. However, users interact with practical dialog systems often using speech as input. Typically, systems convert speech into text using an Automatic Speech Recognition (ASR) system, introducing errors. Furthermore, these systems do not address the differences in written and spoken language. The research on this topic is stymied by the lack of a public corpus. Motivated by these considerations, our goal in hosting the speech-aware dialog state tracking challenge was to create a public corpus or task which can be used to investigate the performance gap between the written and spoken forms of input, develop models that could alleviate this gap, and establish whether Text-to-Speech-based (TTS) systems is a reasonable surrogate to the more-labor intensive human data collection. We created three spoken versions of the popular written-domain MultiWoz task – (a) TTS-Verbatim: written user inputs were converted into speech waveforms using a TTS system, (b) Human-Verbatim: humans spoke the user inputs verbatim, and (c) Human-paraphrased: humans paraphrased the user inputs. Additionally, we provided different forms of ASR output to encourage wider participation from teams that may not have access to state-of-the-art ASR systems. These included ASR transcripts, word time stamps, and latent representations of the audio (audio encoder outputs). In this paper, we describe the corpus, report results from participating teams, provide preliminary analyses of their results, and summarize the current state-of-the-art in this domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.26.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.26.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--26" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.26" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.26/">Overview of Situated and Interactive Multimodal Conversations (<span class="acl-fixed-case">SIMMC</span>) 2.1 Track at <span class="acl-fixed-case">DSTC</span> 11</a></strong><br><a href="/people/s/satwik-kottur/">Satwik Kottur</a>
|
<a href="/people/s/seungwhan-moon/">Seungwhan Moon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--26"><div class="card-body p-3 small">With ever increasing interest in task-oriented dialog systems, the recent work on Situated and Interactive Multimodal Conversations (SIMMC 2.0) aims to develop personal assistants that interact with users, grounded in an immersive and co-observed setting of photo-realistic scenes. The dataset contains <span class="tex-math">11k</span> task-oriented dialogs set in an interactive shopping scenario, spanning more than <span class="tex-math">117k</span> utterances. In order to push research towards this next generation virtual assistants, the SIMMC 2.1 challenge was conducted at the Eleventh Dialog System Technology Challenge (DSTC) which had entries from across the world competing to achieve the state-of-the-art performance in the SIMMC 2.1 task. In this report, we present and compare 13 SIMMC 2.1 model entries from 5 trams across the world to understand the current progress made across the last three years (starting with SIMMC 1.0 and 2.0 challenges) for multimodal task-oriented dialog systems. We hope that our analysis throws light on components that showed promise in addition to identifying the gaps for future research towards this grand goal of an immersive multimodal conversational agent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.27.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.27.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--27" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.27" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.27/">Intent Induction from Conversations for Task-Oriented Dialogue Track at <span class="acl-fixed-case">DSTC</span> 11</a></strong><br><a href="/people/j/james-gung/">James Gung</a>
|
<a href="/people/r/raphael-shu/">Raphael Shu</a>
|
<a href="/people/e/emily-moeng/">Emily Moeng</a>
|
<a href="/people/w/wesley-rose/">Wesley Rose</a>
|
<a href="/people/s/salvatore-romeo/">Salvatore Romeo</a>
|
<a href="/people/a/arshit-gupta/">Arshit Gupta</a>
|
<a href="/people/y/yassine-benajiba/">Yassine Benajiba</a>
|
<a href="/people/s/saab-mansour/">Saab Mansour</a>
|
<a href="/people/y/yi-zhang/">Yi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--27"><div class="card-body p-3 small">With increasing demand for and adoption of virtual assistants, recent work has investigated ways to accelerate bot schema design through the automatic induction of intents or the induction of slots and dialogue states. However, a lack of dedicated benchmarks and standardized evaluation has made progress difficult to track and comparisons between systems difficult to make. This challenge track, held as part of the Eleventh Dialog Systems Technology Challenge, introduces a benchmark that aims to evaluate methods for the automatic induction of customer intents in a realistic setting of customer service interactions between human agents and customers. We propose two subtasks for progressively tackling the automatic induction of intents and corresponding evaluation methodologies. We then present three datasets suitable for evaluating the tasks and propose simple baselines. Finally, we summarize the submissions and results of the challenge track, for which we received submissions from 34 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.28.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.28.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--28" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.28" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.28/">Overview of Robust and Multilingual Automatic Evaluation Metricsfor Open-Domain Dialogue Systems at <span class="acl-fixed-case">DSTC</span> 11 Track 4</a></strong><br><a href="/people/m/mario-rodriguez-cantelar/">Mario Rodríguez-Cantelar</a>
|
<a href="/people/c/chen-zhang/">Chen Zhang</a>
|
<a href="/people/c/chengguang-tang/">Chengguang Tang</a>
|
<a href="/people/k/ke-shi/">Ke Shi</a>
|
<a href="/people/s/sarik-ghazarian/">Sarik Ghazarian</a>
|
<a href="/people/j/joao-sedoc/">João Sedoc</a>
|
<a href="/people/l/luis-fernando-dharo/">Luis Fernando D’Haro</a>
|
<a href="/people/a/alexander-rudnicky/">Alexander I. Rudnicky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--28"><div class="card-body p-3 small">The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics’ correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.dstc-1.29.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.dstc-1.29.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--dstc-1--29" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.dstc-1.29" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.dstc-1.29/">Task-Oriented Conversational Modeling with Subjective Knowledge Track in <span class="acl-fixed-case">DSTC</span>11</a></strong><br><a href="/people/s/seokhwan-kim/">Seokhwan Kim</a>
|
<a href="/people/s/spandana-gella/">Spandana Gella</a>
|
<a href="/people/c/chao-zhao/">Chao Zhao</a>
|
<a href="/people/d/di-jin/">Di Jin</a>
|
<a href="/people/a/alexandros-papangelis/">Alexandros Papangelis</a>
|
<a href="/people/b/behnam-hedayatnia/">Behnam Hedayatnia</a>
|
<a href="/people/y/yang-liu/">Yang Liu</a>
|
<a href="/people/d/dilek-z-hakkani-tur/">Dilek Z Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--dstc-1--29"><div class="card-body p-3 small">Conventional Task-oriented Dialogue (TOD) Systems rely on domain-specific APIs/DBs or external factual knowledge to create responses. In DSTC11 track 5, we aims to provide a new challenging task to accommodate subjective user requests (e.g.,”Is the WIFI reliable?” or “Does the restaurant have a good atmosphere?” into TOD. We release a benchmark dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses that are grounded in subjective knowledge sources. The challenge track received a total of 48 entries from 14 participating teams.</div></div></div><hr><div id="2023icard-1"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.icard-1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.icard-1/">Proceedings of the First Workshop on Connecting Multiple Disciplines to AI Techniques in Interaction-centric Autism Research and Diagnosis (ICARD 2023)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.icard-1.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.icard-1.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.icard-1.1/">Global and local prosodic entrainment in a task-oriented interaction in autistic and neurotypical children</a></strong><br><a href="/people/j/joanna-kruyt/">Joanna Kruyt</a>
|
<a href="/people/k/katarina-polonyiova/">Katarína Polónyiová</a>
|
<a href="/people/d/daniela-ostatnikova/">Daniela Ostatníková</a>
|
<a href="/people/s/stefan-benus/">Štefan Beňuš</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.icard-1.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.icard-1.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.icard-1.2/">Receptive Language Development Diagnosis and Tracking in Conversational Interactions with <span class="acl-fixed-case">QT</span>robot for Autism</a></strong><br><a href="/people/a/aida-nazari/">Aida Nazari</a>
|
<a href="/people/s/sviatlana-hohn/">Sviatlana Höhn</a>
|
<a href="/people/a/ali-paikan/">Ali Paikan</a>
|
<a href="/people/p/pouyan-ziafati/">Pouyan Ziafati</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.icard-1.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.icard-1.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.icard-1.3/">Computational Analysis of Backchannel Usage and Overlap Length in Autistic Children</a></strong><br><a href="/people/g/grace-o-lawley/">Grace O. Lawley</a>
|
<a href="/people/p/peter-a-heeman/">Peter A. Heeman</a>
|
<a href="/people/s/steven-bedrick/">Steven Bedrick</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.icard-1.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.icard-1.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.icard-1.4/">Interactional coordination between conversation partners with autism using non-verbal cues in dialogues</a></strong><br><a href="/people/t/tahiya-chowdhury/">Tahiya Chowdhury</a>
|
<a href="/people/v/veronica-romero/">Veronica Romero</a>
|
<a href="/people/a/amanda-stent/">Amanda Stent</a></span></p></div><hr><div id="2023mmnlg-1"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.mmnlg-1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.mmnlg-1/">Proceedings of the Workshop on Multimodal, Multilingual Natural Language Generation and Multilingual WebNLG Challenge (MM-NLG 2023)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.1/">Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting</a></strong><br><a href="/people/l/lea-krause/">Lea Krause</a>
|
<a href="/people/w/wondimagegnhue-tufa/">Wondimagegnhue Tufa</a>
|
<a href="/people/s/selene-baez-santamaria/">Selene Baez Santamaria</a>
|
<a href="/people/a/angel-daza/">Angel Daza</a>
|
<a href="/people/u/urja-khurana/">Urja Khurana</a>
|
<a href="/people/p/piek-vossen/">Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--1"><div class="card-body p-3 small">While the fluency and coherence of Large Language Models (LLMs) in text generation have seen significant improvements, their competency in generating appropriate expressions of uncertainty remains limited.Using a multilingual closed-book QA task and GPT-3.5, we explore how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings. Our results reveal strong performance in high-resource languages but a marked decline in performance in lower-resource languages. Across all, we observe an exaggerated expression of confidence in the model, which does not align with the correctness or likelihood of its responses. Our findings highlight the need for further research into accurate calibration of LLMs especially in a multilingual setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.2/">Visual Question Generation in <span class="acl-fixed-case">B</span>engali</a></strong><br><a href="/people/m/mahmud-hasan/">Mahmud Hasan</a>
|
<a href="/people/l/labiba-islam/">Labiba Islam</a>
|
<a href="/people/j/jannatul-ruma/">Jannatul Ruma</a>
|
<a href="/people/t/tasmiah-mayeesha/">Tasmiah Mayeesha</a>
|
<a href="/people/r/rashedur-rahman/">Rashedur Rahman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--2"><div class="card-body p-3 small">The task of Visual Question Generation (VQG) is to generate human-like questions relevant to the given image. As VQG is an emerging research field, existing works tend to focus only on resource-rich language such as English due to the availability of datasets. In this paper, we propose the first Bengali Visual Question Generation task and develop a novel transformer-based encoder-decoder architecture that generates questions in Bengali when given an image. We propose multiple variants of models - (i) image-only: baseline model of generating questions from images without additional information, (ii) image-category and image-answer-category: guided VQG where we condition the model to generate questions based on the answer and the category of expected question. These models are trained and evaluated on the translated VQAv2.0 dataset. Our quantitative and qualitative results establish the first state of the art models for VQG task in Bengali and demonstrate that our models are capable of generating grammatically correct and relevant questions. Our quantitative results show that our image-cat model achieves a BLUE-1 score of 33.12 and BLEU-3 score of 7.56 which is the highest of the other two variants. We also perform a human evaluation to assess the quality of the generation tasks. Human evaluation suggests that image-cat model is capable of generating goal-driven and attribute-specific questions and also stays relevant to the corresponding image.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.3/">Keeping an Eye on Context: Attention Allocation over Input Partitions in Referring Expression Generation</a></strong><br><a href="/people/s/simeon-schuz/">Simeon Schüz</a>
|
<a href="/people/s/sina-zarriess/">Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--3"><div class="card-body p-3 small">In Referring Expression Generation, model inputs are often composed of different representations, including the visual properties of the intended referent, its relative position and size, and the visual context. Yet, the extent to which this information influences the generation process of black-box neural models is largely unclear. We investigate the relative weighting of target, location, and context information in the attention components of a Transformer-based generation model. Our results show a general target bias, which, however, depends on the content of the generated expressions, pointing to interesting directions for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.4/">Are Language-and-Vision Transformers Sensitive to Discourse? A Case Study of <span class="acl-fixed-case">V</span>i<span class="acl-fixed-case">LBERT</span></a></strong><br><a href="/people/e/ekaterina-voloshina/">Ekaterina Voloshina</a>
|
<a href="/people/n/nikolai-ilinykh/">Nikolai Ilinykh</a>
|
<a href="/people/s/simon-dobnik/">Simon Dobnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--4"><div class="card-body p-3 small">Language-and-vision models have shown good performance in tasks such as image-caption matching and caption generation. However, it is challenging for such models to generate pragmatically correct captions, which adequately reflect what is happening in one image or several images. It is crucial to evaluate this behaviour to understand underlying reasons behind it. Here we explore to what extent contextual language-and-vision models are sensitive to different discourse, both textual and visual. In particular, we employ one of the multi-modal transformers (ViLBERT) and test if it can match descriptions and images, differentiating them from distractors of different degree of similarity that are sampled from different visual and textual contexts. We place our evaluation in the multi-sentence and multi-image setup, where images and sentences are expected to form a single narrative structure. We show that the model can distinguish different situations but it is not sensitive to differences within one narrative structure. We also show that performance depends on the task itself, for example, what modality remains unchanged in non-matching pairs or how similar non-matching pairs are to original pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.5/">Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs</a></strong><br><a href="/people/a/agnes-axelsson/">Agnes Axelsson</a>
|
<a href="/people/g/gabriel-skantze/">Gabriel Skantze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--5"><div class="card-body p-3 small">In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task, even with relatively little training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model’s understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.6.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.6.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--6" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.6" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.6/">The 2023 <span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span> Shared Task on Low Resource Languages. Overview and Evaluation Results (<span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span> 2023)</a></strong><br><a href="/people/l/liam-cripwell/">Liam Cripwell</a>
|
<a href="/people/a/anja-belz/">Anya Belz</a>
|
<a href="/people/c/claire-gardent/">Claire Gardent</a>
|
<a href="/people/a/albert-gatt/">Albert Gatt</a>
|
<a href="/people/c/claudia-borg/">Claudia Borg</a>
|
<a href="/people/m/marthese-borg/">Marthese Borg</a>
|
<a href="/people/j/john-judge/">John Judge</a>
|
<a href="/people/m/michela-lorandi/">Michela Lorandi</a>
|
<a href="/people/a/anna-nikiforovskaya/">Anna Nikiforovskaya</a>
|
<a href="/people/w/william-soto-martinez/">William Soto Martinez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--6"><div class="card-body p-3 small">The WebNLG task consists of mapping a knowledge graph to a text verbalising the con- tent of that graph. The 2017 WebNLG edi- tion required participating systems to gener- ate English text from a set of DBpedia triples, while the 2020 WebNLG+ challenge addition- ally included generation into Russian and se- mantic parsing of English and Russian texts. In contrast, WebNLG 2023 focuses on four under-resourced languages which are severely under-represented in research on text genera- tion, namely Breton, Irish, Maltese and Welsh. In addition, WebNLG 2023 once again includes Russian. In this paper, we present the organi- sation of the shared task (data, timeline, eval- uation), briefly describe the participating sys- tems and summarise results for participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.7.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.7.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--7" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.7" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.7/"><span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span>-Interno: Utilizing <span class="acl-fixed-case">FRED</span>-T5 to address the <span class="acl-fixed-case">RDF</span>-to-text problem (<span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span> 2023)</a></strong><br><a href="/people/m/maxim-kazakov/">Maxim Kazakov</a>
|
<a href="/people/j/julia-preobrazhenskaya/">Julia Preobrazhenskaya</a>
|
<a href="/people/i/ivan-bulychev/">Ivan Bulychev</a>
|
<a href="/people/a/aleksandr-shain/">Aleksandr Shain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--7"><div class="card-body p-3 small">We present our solution for the Russian RDF002 to-text generation task of the WebNLG Challenge 2023. We use the pretrained large language model named FRED-T5 (Zmitrovich et al., 2023) to finetune on the train dataset. Also, we propose several types of prompt and run experiments to analyze their effectiveness. Our submission achieves 0.373 TER on the test dataset, taking the first place according to the results of the automatic evaluation and outperforming the best result of the previous challenge by 0.025. The code of our solution is available at the following link: https://github.com/Ivan30003/webnlg_interno</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.8.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.8.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--8" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.8" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.8/">Better Translation + Split and Generate for Multilingual <span class="acl-fixed-case">RDF</span>-to-Text (<span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span> 2023)</a></strong><br><a href="/people/n/nalin-kumar/">Nalin Kumar</a>
|
<a href="/people/s/saad-obaid-ul-islam/">Saad Obaid Ul Islam</a>
|
<a href="/people/o/ondrej-dusek/">Ondrej Dusek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--8"><div class="card-body p-3 small">This paper presents system descriptions of our submitted outputs for WebNLG Challenge 2023. We use mT5 in multi-task and multilingual settings to generate more fluent and reliable verbalizations of the given RDF triples. Furthermore, we introduce a partial decoding technique to produce more elaborate yet simplified outputs. Additionally, we demonstrate the significance of employing better translation systems in creating training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.9.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.9.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--9" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.9" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.9/">Data-to-text Generation for Severely Under-Resourced Languages with <span class="acl-fixed-case">GPT</span>-3.5: A Bit of Help Needed from <span class="acl-fixed-case">G</span>oogle <span class="acl-fixed-case">T</span>ranslate (<span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span> 2023)</a></strong><br><a href="/people/m/michela-lorandi/">Michela Lorandi</a>
|
<a href="/people/a/anja-belz/">Anya Belz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--9"><div class="card-body p-3 small">LLMs are great at tasks involving English which dominates in their training data. We explore their ability to address tasks involving languages that are severely under-represented in their training data. More specifically, we do this in the context of data-to-text generation for Irish, Maltese, Welsh and Breton. During the prompt-engineering phase we tested GPT-3.5 and~4 with a range of prompt types and formats on a small sample of example input/output pairs. We then fully evaluated the two most promising prompts in two scenarios: (i) direct generation into the under-resourced languages, and (ii) generation into English followed by translation into the under-resourced languages. We find that few-shot prompting works better for direct generation into under-resourced languages, but that the difference disappears when pivoting via English. The few-shot + translation system variants were submitted to the WebNLG 2023 shared task where they outperformed all other systems by substantial margins in all languages on all automatic metrics. We conclude that good performance can be achieved with state-of-the-art LLMs out-of-the box for under-resourced languages. However, best results (for Welsh) of BLEU 25.12, ChrF++ 0.55, and TER 0.64 are well below the lowest ranked English system at WebNLG’20 with BLEU 0.391, ChrF++ 0.579, and TER 0.564.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.10.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.10.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--10" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.10" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.10/"><span class="acl-fixed-case">DCU</span>/<span class="acl-fixed-case">TCD</span>-<span class="acl-fixed-case">FORG</span>e at <span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span>’23: <span class="acl-fixed-case">I</span>rish rules! (<span class="acl-fixed-case">W</span>eg<span class="acl-fixed-case">NLG</span> 2023)</a></strong><br><a href="/people/s/simon-mille/">Simon Mille</a>
|
<a href="/people/e/elaine-ui-dhonnchadha/">Elaine Uí Dhonnchadha</a>
|
<a href="/people/s/stamatia-dasiopoulou/">Stamatia Dasiopoulou</a>
|
<a href="/people/l/lauren-cassidy/">Lauren Cassidy</a>
|
<a href="/people/b/brian-davis/">Brian Davis</a>
|
<a href="/people/a/anja-belz/">Anya Belz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--10"><div class="card-body p-3 small">In this paper, we describe the submission of Dublin City University (DCU) and Trinity College Dublin (TCD) for the WebNLG 2023 shared task. We present a fully rule-based pipeline for generating Irish texts from DBpedia triple sets which comprises 4 components: triple lexicalisation, generation of noninflected Irish text, inflection generation, and post-processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.mmnlg-1.11.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.mmnlg-1.11.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--mmnlg-1--11" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.mmnlg-1.11" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.mmnlg-1.11/"><span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span> Challenge 2023: Domain Adaptive Machine Translation for Low-Resource Multilingual <span class="acl-fixed-case">RDF</span>-to-Text Generation (<span class="acl-fixed-case">W</span>eb<span class="acl-fixed-case">NLG</span> 2023)</a></strong><br><a href="/people/k/kancharla-aditya-hari/">Kancharla Aditya Hari</a>
|
<a href="/people/b/bhavyajeet-singh/">Bhavyajeet Singh</a>
|
<a href="/people/a/anubhav-sharma/">Anubhav Sharma</a>
|
<a href="/people/v/vasudeva-varma/">Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--mmnlg-1--11"><div class="card-body p-3 small">This paper presents our submission to the WebNLG Challenge 2023 for generating text in several low-resource languages from RDF-triples. Our submission focuses on using machine translation for generating texts in Irish, Maltese, Welsh and Russian. While a simple and straightfoward approach, recent works have shown that using monolingual models for inference for multilingual tasks with the help of machine translation (translate-test) can out-perform multilingual models and training multilingual models on machine-translated data (translate-train) through careful tuning of the MT component. Our results show that this approach demonstrates competitive performance for this task even with limited data.</div></div></div><hr><div id="2023tllm-1"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.tllm-1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.tllm-1/">Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants!</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.tllm-1.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.tllm-1.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--tllm-1--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.tllm-1.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.tllm-1.1/"><span class="acl-fixed-case">CST</span>5: Data Augmentation for Code-Switched Semantic Parsing</a></strong><br><a href="/people/a/anmol-agarwal/">Anmol Agarwal</a>
|
<a href="/people/j/jigar-gupta/">Jigar Gupta</a>
|
<a href="/people/r/rahul-goel/">Rahul Goel</a>
|
<a href="/people/s/shyam-upadhyay/">Shyam Upadhyay</a>
|
<a href="/people/p/pankaj-joshi/">Pankaj Joshi</a>
|
<a href="/people/r/rengarajan-aravamudhan/">Rengarajan Aravamudhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--tllm-1--1"><div class="card-body p-3 small">Extending semantic parsers to code-switched input has been a challenging problem, primarily due to a lack of supervised training data. In this work, we introduce CST5, a new data augmentation technique that fine-tunes a T5 model using a small seed set (≈100 utterances) to generate code-switched utterances from English utterances. We show that CST5 generates high quality code-switched data, both intrinsically (per human evaluation) and extrinsically by comparing baseline models which are trained without data augmentation to models which are trained with augmented data. Empirically we observe that using CST5, one can achieve the same semantic parsing performance by using up to 20x less labeled data. To aid further research in this area, we are also releasing (a) Hinglish-TOP, the largest human annotated code-switched semantic parsing dataset to date, containing 10k human annotated Hindi-English (Hinglish) code-switched utterances, and (b) Over 170K CST5 generated code-switched utterances from the TOPv2 dataset. Human evaluation shows that both the human annotated data as well as the CST5 generated data is of good quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.tllm-1.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.tllm-1.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--tllm-1--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.tllm-1.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.tllm-1.2/"><span class="acl-fixed-case">P</span>anda<span class="acl-fixed-case">GPT</span>: One Model To Instruction-Follow Them All</a></strong><br><a href="/people/y/yixuan-su/">Yixuan Su</a>
|
<a href="/people/t/tian-lan/">Tian Lan</a>
|
<a href="/people/h/huayang-li/">Huayang Li</a>
|
<a href="/people/j/jialu-xu/">Jialu Xu</a>
|
<a href="/people/y/yan-wang/">Yan Wang</a>
|
<a href="/people/d/deng-cai/">Deng Cai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--tllm-1--2"><div class="card-body p-3 small">We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an initial step toward building AGI that can perceive and understand inputs in different modalities holistically, as we humans do.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.tllm-1.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.tllm-1.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--tllm-1--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.tllm-1.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.tllm-1.3/">Emotion-Conditioned Text Generation through Automatic Prompt Optimization</a></strong><br><a href="/people/y/yarik-menchaca-resendiz/">Yarik Menchaca Resendiz</a>
|
<a href="/people/r/roman-klinger/">Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--tllm-1--3"><div class="card-body p-3 small">Conditional natural language generation methods often require either expensive fine-tuning or training a large language model from scratch. Both are unlikely to lead to good results without a substantial amount of data and computational resources. Prompt learning without changing the parameters of a large language model presents a promising alternative. It is a cost-effective approach, while still achieving competitive results. While this procedure is now established for zero- and few-shot text classification and structured prediction, it has received limited attention in conditional text generation. We present the first automatic prompt optimization approach for emotion-conditioned text generation with instruction-fine-tuned models. Our method uses an iterative optimization procedure that changes the prompt by adding, removing, or replacing tokens. As objective function, we only require a text classifier that measures the realization of the conditional variable in the generated text. We evaluate the method on emotion-conditioned text generation with a focus on event reports and compare it to manually designed prompts that also act as the seed for the optimization procedure. The optimized prompts achieve 0.75 macro-average F1 to fulfill the emotion condition in contrast to manually designed seed prompts with only 0.22 macro-average F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.tllm-1.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.tllm-1.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--tllm-1--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.tllm-1.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.tllm-1.4/">Mitigating Harms of <span class="acl-fixed-case">LLM</span>s via Knowledge Distillation for a Virtual Museum Tour Guide</a></strong><br><a href="/people/a/ashley-lewis/">Ashley Lewis</a>
|
<a href="/people/m/michael-white/">Michael White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--tllm-1--4"><div class="card-body p-3 small">LLMs are known to be very powerful, exhibiting both great benefits and great risk. We seek to leverage the benefits, in particular the ability to be fluent, conversational dialogue agents, while minimizing the risks, such as hallucination and toxic content. In this work we use knowledge distillation to create a virtual museum tour guide dialogue agent, employing ChatGPT as a teacher model for a smaller student model, T5-large. We find the T5 model shows competitive performance, significantly reduces instances of hallucination, and shows promise for reducing toxic content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.tllm-1.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.tllm-1.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--tllm-1--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.tllm-1.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.tllm-1.5/">Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues</a></strong><br><a href="/people/n/norbert-braunschweiler/">Norbert Braunschweiler</a>
|
<a href="/people/r/rama-doddipatla/">Rama Doddipatla</a>
|
<a href="/people/s/simon-keizer/">Simon Keizer</a>
|
<a href="/people/s/svetlana-stoyanchev/">Svetlana Stoyanchev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--tllm-1--5"><div class="card-body p-3 small">In this paper, we investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded in multiple documents providing relevant information. We generate dialogue completion responses by prompting a ChatGPT model, using two methods: Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT model pre-training while LlamaIndex also extracts relevant information from documents. Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two ChatGPT variants outputs, and human responses. While both ChatGPT variants are more likely to include information not present in the relevant segments, possibly including a presence of hallucinations, they are rated higher than both the shared task winning system and human responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.tllm-1.6.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.tllm-1.6.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--tllm-1--6" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.tllm-1.6" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.tllm-1.6/">Enhancing Pipeline-Based Conversational Agents with Large Language Models</a></strong><br><a href="/people/m/mina-foosherian/">Mina Foosherian</a>
|
<a href="/people/h/hendrik-purwins/">Hendrik Purwins</a>
|
<a href="/people/p/purna-rathnayake/">Purna Rathnayake</a>
|
<a href="/people/t/touhidul-alam/">Touhidul Alam</a>
|
<a href="/people/r/rui-teimao/">Rui Teimao</a>
|
<a href="/people/k/klaus-dieter-thoben/">Klaus-Dieter Thoben</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--tllm-1--6"><div class="card-body p-3 small">The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example. Companies may be hesitant to replace their pipeline-based agents with LLMs entirely due to privacy concerns and the need for deep integration within their existing ecosystems. A hybrid approach in which LLMs’ are integrated into the pipeline-based agents allows them to save time and costs of building and running agents by capitalizing on the capabilities of LLMs while retaining the integration and privacy safeguards of their existing systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.tllm-1.7.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.tllm-1.7.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--tllm-1--7" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.tllm-1.7" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.tllm-1.7/">Style Locality for Controllable Generation with k<span class="acl-fixed-case">NN</span> Language Models</a></strong><br><a href="/people/g/gilles-nawezi/">Gilles Nawezi</a>
|
<a href="/people/l/lucie-flek/">Lucie Flek</a>
|
<a href="/people/c/charles-welch/">Charles Welch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--tllm-1--7"><div class="card-body p-3 small">Recent language models have been improved by the addition of external memory. Nearest neighbor language models retrieve similar contexts to assist in word prediction. The addition of locality levels allows a model to learn how to weight neighbors based on their relative location to the current text in source documents, and have been shown to further improve model performance. Nearest neighbor models have been explored for controllable generation but have not examined the use of locality levels. We present a novel approach for this purpose and evaluate it using automatic and human evaluation on politeness, formality, supportiveness, and toxicity textual data. We find that our model is successfully able to control style and provides a better fluency-style trade-off than previous work</div></div></div><hr><div id="2023yrrsds-1"><small><a href="#" class="text-muted"><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href="/volumes/2023.yrrsds-1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class="align-middle" href="/volumes/2023.yrrsds-1/">Proceedings of the 19th Annual Meeting of the Young Reseachers' Roundtable on Spoken Dialogue Systems</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.0.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.0.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.0/">Proceedings of the 19th Annual Meeting of the Young Reseachers' Roundtable on Spoken Dialogue Systems</a></strong><br><a href="/people/v/vojtech-hudecek/">Vojtech Hudecek</a>
|
<a href="/people/p/patricia-schmidtova/">Patricia Schmidtova</a>
|
<a href="/people/t/tanvi-dinkar/">Tanvi Dinkar</a>
|
<a href="/people/j/javier-chiyah-garcia/">Javier Chiyah-Garcia</a>
|
<a href="/people/w/weronika-sieinska/">Weronika Sieinska</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.1.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.1.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--1" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.1" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.1/">Processing Referential Ambiguities in Situated Dialogue Systems</a></strong><br><a href="/people/j/javier-chiyah-garcia/">Javier Chiyah-Garcia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--1"><div class="card-body p-3 small">Position paper for YRRSDS 2023</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.2.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.2.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--2" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.2" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.2/">Safety and Robustness in Conversational <span class="acl-fixed-case">AI</span></a></strong><br><a href="/people/t/tanvi-dinkar/">Tanvi Dinkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--2"><div class="card-body p-3 small">In this position paper, I will present the research interests in my PostDoc on safety and robustness specific to conversational AI, including then relevant overlap from my PhD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.3.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.3.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--3" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.3" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.3/">Incremental Speech Processing for Voice Assistant Accessibility</a></strong><br><a href="/people/a/angus-addlesee/">Angus Addlesee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--3"><div class="card-body p-3 small">Speech production is nuanced and unique to every individual, but today’s Spoken Dialogue Systems (SDSs) are trained to use general speech patterns to successfully improve performance on various evaluation metrics. However, these patterns do not apply to certain user groups - often the very people that can benefit the most from SDSs. For example, people with dementia produce more disfluent speech than the general population. The healthcare domain is now a popular setting for spoken dialogue and human-robot interaction research. This trend is similar when observing company behaviour. Charities promote industry voice assistants, the creators are getting HIPAA compliance, and their features sometimes target vulnerable user groups. It is therefore critical to adapt SDSs to be more accessible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.4.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.4.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--4" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.4" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.4/">Advancing Spoken Dialog Systems for Manufacturing: From Conceptual Architecture and Taxonomy to Real Case Applications and Future Directions</a></strong><br><a href="/people/s/silvia-colabianchi/">Silvia Colabianchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--4"><div class="card-body p-3 small">This research encompasses a comprehensive exploration of Spoken Dialogue Systems (SDSs) in the manufacturing sector. It begins by establishing a conceptual architecture and taxonomy to guide the design and selection of SDS elements. Real case applications, including worker safety and cybersecurity support, validate the research findings and highlight areas for improvement. Looking ahead, the study delves into the potential of Large Language Models (LLMs) and multi-modal applications. Emphasizing the importance of extreme personalization, the study highlights the need to cater to the diverse qualifications and preferences of workers. Additionally, it investigates the integration of SDSs with other sensory modalities, such as images, videos, and augmented or virtual reality scenarios, to enhance the user experience and productivity. The research also addresses crucial considerations related to knowledge base optimization. It examines semantic variations of words across different application contexts, the continuous updating of procedures and data, and the adaptability of SDSs to diverse dialects and linguistic abilities, particularly in low-schooling personnel scenarios. Privacy, industrial protection, and ethical concerns in the era of LLMs and external players like OpenAI are given due attention. The study explores the boundaries of knowledge that conversational systems should possess, advocating for transparency, explainability, and responsible data handling practices.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.5.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.5.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--5" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.5" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.5/">Conversational Grounding in Multimodal Dialog Systems</a></strong><br><a href="/people/b/biswesh-mohapatra/">Biswesh Mohapatra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--5"><div class="card-body p-3 small">The process of “conversational grounding” is an interactive process that has been studied extensively in cognitive science, whereby participants in a conversation check to make sure their interlocutors understand what is being referred to. This interactive process uses multiple modes of communication to establish the information between the participants. This could include information provided through eye-gaze, head movements, intonation in speech, along with the content of the speech. While the process is essential to successful communication between humans and between humans and machines, work needs to be done on testing and building the capabilities of the current dialogue system in managing conversational grounding, especially in multimodal medium of communication. Recent work such as Benotti and Blackburn have shown the importance of conversational grounding in dialog systems and how current systems fail in them. This is essential for the advancement of Embodied Conversational Agents and Social Robots. Thus my PhD project aims to test, understand and improve the functioning of current dialog models with respect to Conversational Grounding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.6.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.6.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--6" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.6" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.6/"><span class="acl-fixed-case">SQL</span> Comment Generation and Additional Research Interests</a></strong><br><a href="/people/a/alyssa-allen/">Alyssa Allen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--6"><div class="card-body p-3 small">My research interests focus on natural language generation (NLG) regarding how to make system outputs more intuitive and comprehensible for the human-user and conversational entrainment and alignment from the perspective of how dialogue systems could or should personalize its responses to the human user. As it relates to NLG, my current work focuses on training a system to auto-generate comments for SQL queries produced by a Text-to-SQL parser. The goal is to make the connection between technical SQL language and the user’s question more transparent. My linguistic training lies primarily at the intersection of computational and socio-linguistics. As such, my curiosities in conversational entrainment and alignment focus on the extent to which conversational agents can or should adjust their language based on human characteristics such as age, race, or gender.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.7.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.7.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--7" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.7" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.7/">On Referring Language Use in Visually Grounded Dialogue</a></strong><br><a href="/people/b/bram-willemsen/">Bram Willemsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--7"><div class="card-body p-3 small">Position paper for YRRSDS 2023</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.8.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.8.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--8" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.8" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.8/">Challenges and Approaches in Designing Social <span class="acl-fixed-case">SDS</span> in the <span class="acl-fixed-case">LLM</span> Era</a></strong><br><a href="/people/k/koji-inoue/">Koji Inoue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--8"><div class="card-body p-3 small">Large language models (LLMs) have brought about a significant transformation in spoken dialogue systems (SDSs). It is anticipated that these systems will be implemented into diverse robotic applications and employed in a variety of social settings. The author presents research interest with the aim of realizing social SDSs from multiple perspectives, including task design, turn-taking mechanisms, and evaluation methodologies. Additionally, future research in social SDSs should delve into a deeper understanding of user mental states and a relationship with society via multi-party conversations. Finally, the author suggests topics for discussion regarding the future directions of SDS researchers in the LLM era.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.9.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.9.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--9" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.9" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.9/">Breakdowns and Repairs. Detecting Patterns that Lead to Breakdowns in Customer Service Messages</a></strong><br><a href="/people/a/anouck-braggaar/">Anouck Braggaar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--9"><div class="card-body p-3 small">Many companies use dialogue systems for their customer service, and although there has been a rise in the usage of these systems (Costello and LoDolce, 2022), many of these systems still face challenges in comprehending and properly responding to the customer (Følstadet al., 2021). In our project we aim to figure out how to develop and improve these conversational agents. Part of this project (detailed in this paper) will focus on the detection of breakdown patterns and the possible solutions (repairs) to mitigate negative results of these errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.10.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.10.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--10" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.10" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.10/">Towards More Natural Dialogues: Integrating Open-Domain Dialogue Skills into Task-Oriented Agents</a></strong><br><a href="/people/a/armand-stricker/">Armand Stricker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--10"><div class="card-body p-3 small">Position paper on the intersection between chitchat and task-oriented dialogues (TODs), with a focus on integrating capabilities typically associated with chitchat systems into task-oriented agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.11.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.11.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--11" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.11" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.11/">The Future of Designing Spoken Dialogue Systems and Analyzing Written Conversations</a></strong><br><a href="/people/l/livia-qian/">Livia Qian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--11"><div class="card-body p-3 small">This is my position paper for YRRSDS 2023. In it, I write about the details of my research interests as well as past, current and future projects, talk about the status of spoken dialogue system research, include a short bio, and suggest topics for discussion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.12.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.12.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--12" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.12" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.12/">Exploring the Synergy of Deep Learning and Anthropomorphism in Multimodal Dialogue Systems</a></strong><br><a href="/people/i/iwona-christop/">Iwona Christop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--12"><div class="card-body p-3 small">This position paper is an overview of author’s main research interests and work considering deep learning techniques in audio classification, sign languages, and multimodality in dialogue systems. Author also shares her opinion on current and future research considering dialogue agents, and suggests topics for discussion panels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.13.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.13.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--13" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.13" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.13/">A Perspective on Anchoring and Dialogue History Propagation for Smoother Interactions with Spoken Task-Oriented Dialogue Systems</a></strong><br><a href="/people/l/lucas-druart/">Lucas Druart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--13"><div class="card-body p-3 small">Task-Oriented Dialogue (TOD) systems provide interactive assistance to a user in order to accomplish a specific task such as making a reservation at a restaurant or booking a room in a hotel. Speech presents itself as a natural interface for TOD systems. A typical approach to implement them is to use a modular architecture (Gao et al., 2018). A core component of such dialogue systems is Spoken Language Understanding (SLU) whose goal is to extract the relevant information from the user’s utterances. While spoken dialogue was the focus of earlier work (Williams et al., 2013; Henderson et al., 2014), recent work has focused on text inputs with no regard for the specificities of spoken language (Wu et al., 2019; Heck et al., 2020; Feng et al., 2021). However, this approach fails to account for the differences between written and spoken language (Faruqui and Hakkani-Tür, 2022) such as disfluencies. My research focuses on Spoken Language Understanding in the context of Task-Oriented Dialogue. More specifically I am interested in the two following research directions: • Annotation schema for spoken TODs, • Integration of dialogue history for contextually coherent predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.14.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.14.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--14" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.14" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.14/">More Human-Like Interaction in Spoken Dialogue Systems: Global Context for Natural Language Understanding and Multimodal Solutions</a></strong><br><a href="/people/k/kacper-dudzic/">Kacper Dudzic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--14"><div class="card-body p-3 small">My position paper for the YRRSDS 2023 workshop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.15.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.15.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--15" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.15" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.15/">Designing and Evaluating <span class="acl-fixed-case">LLM</span>-based Conversational Agents for Behaviour Change</a></strong><br><a href="/people/s/selina-meyer/">Selina Meyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--15"><div class="card-body p-3 small">My PhD focuses on conversational agents for behaviour change, with a focus on the feasibility of applying Large Language Models (LLMs) such as GPT-4 in this context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.16.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.16.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--16" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.16" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.16/">Stylized Dialog Response Generation</a></strong><br><a href="/people/s/sourabrata-mukherjee/">Sourabrata Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--16"><div class="card-body p-3 small">My primary research focus lies in the domain of Text Style Transfer (TST), a fascinating area within Natural Language Processing (NLP). TST involves the transfor- mation of text into a desired style while approximately preserving its underlying content. In my research, I am also driven by the goal of incorporating TST techniques into NLP systems, particularly within the realm of dia- logue systems. I am intrigued by the concept of Stylized Dialog Response Generation, which aims to enhance the versatility and adaptability of dialog systems in generat- ing text responses with specific style attributes. By ad- vancing our understanding of TST and its integration into dialogue systems, my research seeks to contribute to the broader field of human-computer interaction. Through the development of robust and versatile dialogue systems with enhanced style transfer capabilities, we can facili- tate more engaging and personalized conversational experiences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.17.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.17.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--17" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.17" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.17/">Take the Most out of Text Data Augmentation Strategies For Intent Clustering And Induction Based on <span class="acl-fixed-case">DSTC</span> 11 Track 2</a></strong><br><a href="/people/m/mikolaj-krzyminski/">Mikołaj Krzymiński</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--17"><div class="card-body p-3 small">A brief introduction to author’s keyinterests and research topics which are: multimodal dialogue systems and impact of data augmentation to NLU performance. In addition to that the author shares his biography and view on the future of dialogue assistants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.18.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.18.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--18" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.18" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.18/">Advancing Dialogue Systems: Measuring User Satisfaction and Embracing Multimodality</a></strong><br><a href="/people/a/adrian-charkiewicz/">Adrian Charkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--18"><div class="card-body p-3 small">This submission discusses my research interests in two areas: measuring user satisfaction in goal-oriented dialogue systems and exploring the potential of multi-modal interactions. For goal-oriented dialogue systems, I focus on evaluating and enhancing user satisfaction throughout the interaction process, aiming to propose innovative strategies and address the limitations of existing evaluation techniques. Additionally, I explore the benefits of multi-modal dialogue systems, highlighting their ability to provide more natural and immersive conversations by incorporating various communication modes such as speech, text, gestures, and visuals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.19.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.19.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--19" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.19" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.19/">Information Extraction and Program Synthesis from Goal-Oriented Dialogue</a></strong><br><a href="/people/s/sopan-khosla/">Sopan Khosla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--19"><div class="card-body p-3 small">My research interests broadly lie in the area of Information Extraction from Spoken Dialogue, with a spacial focus on state modeling, anaphora resolution, program synthesis &amp; planning, and intent classification in goal-oriented conversations. My aim is to create embedded dialogue systems that can interact with humans in a collaborative setup to solve tasks in a digital/non-digital environment. Most of the goal-oriented conversations usually involve experts and a laypersons. The aim for the expert is to consider all the information provided by the layperson, identify the underlying set of issues or intents, and prescribe solutions. While human experts are very good at extracting such information, AI agents (that build up most of the automatic dialog systems today) not so much. Most of the existing assistants (or chatbots) only consider individual utterances and do not ground them in the context of the dialogue. My work in this direction has focused on making these systems more effective at extracting the most relevant information from the dialogue to help the human user reach their end-goal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.20.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.20.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--20" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.20" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.20/">Modelling Emotions in Task-Oriented Dialogue</a></strong><br><a href="/people/s/shutong-feng/">Shutong Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--20"><div class="card-body p-3 small">My research interests lie in the area of modelling natural and human-like conversations, with a special focus on emotions in task-oriented dialogue (ToD) systems. ToD systems need to produce semantically and grammatically correct responses to fulfil the user’s goal. Being able to perceive and express emotions pushes them one more step towards achieving human-likeness. To begin with, I constructed a dataset with meaningful emotion labels as well as a wide coverage of emotions and linguistic features in ToDs. Then, I improved emotion recognition in conversations (ERC) in the task-oriented domain by exploiting key characteristics of ToDs. Currently, I am working towards enhancing ToD systems with emotions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.21.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.21.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--21" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.21" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.21/">Incrementally Enriching the Common Ground: A Research Path</a></strong><br><a href="/people/b/brielen-madureira/">Brielen Madureira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--21"><div class="card-body p-3 small">I am broadly interested in evaluation of dialogue systems, in all its many facets: The data they are trained on, their ability to perform a task successfully, their skills with respect to various dialogue phenomena, their resemblance to human cognitive processes, and their ethical and societal impact. More specifically, my research topics focus on understanding the possibilities and limits of current multimodal neural network-based models to incrementally encode information for natural language understanding in general and also for building common ground and asking for clarification. Besides, I am interested in dialogue games as a means to elicit and collect dialogue data and to evaluate the abilities of dialogue models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.22.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.22.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--22" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.22" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.22/">Commonsense Enabled Conversational Model and System-Initiated transitions in Unified <span class="acl-fixed-case">SDS</span>s</a></strong><br><a href="/people/y/ye-liu/">Ye Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--22"><div class="card-body p-3 small">My research work centers on how to enable a human-like interaction through generating contextual, emotional or proactive responses, both in task-oriented and in chitchat spoken dialogue systems (SDSs), because natural lan- guage generation (NLG) is an indispensable component in SDSs and can directly affect the user interactive expe- rience of the entire dialogue system. In addition to NLG, I am also interested in natural language understanding (NLU), as it plays a crucial role in SDSs and is a prerequisite for dialogue systems to generate replies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.23.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.23.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--23" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.23" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.23/">Causality Reasoning for Empathy-Enriched and Personality-Conditioned Spoken Dialogue System</a></strong><br><a href="/people/y/yahui-fu/">Yahui Fu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--23"><div class="card-body p-3 small">The author’s objective centers around developing a spoken dialogue system (SDS) that can emulate the cognitive and conversational qualities of a human friend. Key attributes such as empathy, knowledge/causality reasoning, and personality are integral components of human interaction. The proposed approach involves the creation of an <b>Empathy-enriched SDS</b>, capable of comprehending human emotions and circumstances, thus providing companionship and assistance akin to a trusted friend. Additionally, the <b>Causality-reasoning for SDS</b> aims to ground the system in commonsense knowledge and equip it with the ability to reason about causalities, such as predicting user desires/reactions and system intentions/reactions, thereby enhancing the system’s intelligence and human-like behavior. Finally, the concept of a <b>Personality-conditioned SDS</b> involves enabling systems to exhibit distinct personalities, further enhancing the naturalness of human-robot interaction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href="https://aclanthology.org/2023.yrrsds-1.24.pdf" data-toggle="tooltip" data-placement="top" title="" data-original-title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href="/2023.yrrsds-1.24.bib" data-toggle="tooltip" data-placement="top" title="" data-original-title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href="#abstract-2023--yrrsds-1--24" data-toggle="collapse" aria-expanded="false" aria-controls="abstract-2023.yrrsds-1.24" title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class="d-block"><strong><a class="align-middle" href="/2023.yrrsds-1.24/">Tutorials and User Adaptation in Task Oriented Dialogue</a></strong><br><a href="/people/r/ryu-hirai/">Ryu Hirai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id="abstract-2023--yrrsds-1--24"><div class="card-body p-3 small">This position paper describes my research interests, spoken dialogue system research, and suggested topics for discussion.</div></div></div><hr></section></div><footer class="bg-gradient-light py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5"><div class="container"><p class="text-muted small px-1"><span class="float-right mt-2 ml-2"><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png"></a></span>
ACL materials are Copyright ©&nbsp;1963–2024 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p><p class="text-muted small px-1">The ACL Anthology is managed and built by the <a href="/info/credits/">ACL Anthology team</a> of volunteers.</p><p class="text-muted small px-1"><i>Site last built on 24 May 2024 at 02:51 UTC with <a href="https://github.com/acl-org/acl-anthology/tree/dbd0ba828245c0c28b8ddad7df7a900255137c5c">commit dbd0ba8</a>.</i></p></div></footer><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>