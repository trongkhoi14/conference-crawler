<!DOCTYPE html><!--[if IE 8]><html class="no-js lt-ie9" lang="en" dir="ltr"> <![endif]--><!--[if gt IE 8]><!--><html class="no-js js" lang="en" dir="ltr"><!--<![endif]--><head>
  <script type="text/javascript" charset="UTF-8" async="" src="https://consent.cookiebot.com/694f6fb4-ca29-459f-a9b6-c1deccf2eaca/cc.js?renew=false&amp;referer=www.usenix.org&amp;dnt=false&amp;init=false"></script><script src="https://www.googletagmanager.com/gtm.js?id=GTM-WQSPGJT" async=""></script><script type="text/javascript" id="Cookiebot" src="https://consent.cookiebot.com/uc.js" data-cbid="694f6fb4-ca29-459f-a9b6-c1deccf2eaca" async="async" class="jquery-once-1-processed"></script>
<meta charset="utf-8"><script type="text/javascript">(window.NREUM||(NREUM={})).init={ajax:{deny_list:["bam.nr-data.net"]}};(window.NREUM||(NREUM={})).loader_config={licenseKey:"d823139095",applicationID:"509444"};;/*! For license information please see nr-loader-rum-1.261.1.min.js.LICENSE.txt */
(()=>{var e,t,r={2983:(e,t,r)=>{"use strict";r.d(t,{D0:()=>m,gD:()=>y,Vp:()=>s,fr:()=>S,jD:()=>I,hR:()=>x,xN:()=>b,x1:()=>c,aN:()=>R,V:()=>j});var n=r(384),i=r(7864);const o={beacon:n.NT.beacon,errorBeacon:n.NT.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0},a={};function s(e){if(!e)throw new Error("All info objects require an agent identifier!");if(!a[e])throw new Error("Info for ".concat(e," was never set"));return a[e]}function c(e,t){if(!e)throw new Error("All info objects require an agent identifier!");a[e]=(0,i.a)(t,o);const r=(0,n.nY)(e);r&&(r.info=a[e])}var u=r(993);const l=e=>{if(!e||"string"!=typeof e)return!1;try{document.createDocumentFragment().querySelector(e)}catch{return!1}return!0};var d=r(2614),g=r(944);const f="[data-nr-mask]",p=()=>{const e={mask_selector:"*",block_selector:"[data-nr-block]",mask_input_options:{color:!1,date:!1,"datetime-local":!1,email:!1,month:!1,number:!1,range:!1,search:!1,tel:!1,text:!1,time:!1,url:!1,week:!1,textarea:!1,select:!1,password:!0}};return{ajax:{deny_list:void 0,block_internal:!0,enabled:!0,harvestTimeSeconds:10,autoStart:!0},distributed_tracing:{enabled:void 0,exclude_newrelic_header:void 0,cors_use_newrelic_header:void 0,cors_use_tracecontext_headers:void 0,allowed_origins:void 0},feature_flags:[],harvest:{tooManyRequestsDelay:60},jserrors:{enabled:!0,harvestTimeSeconds:10,autoStart:!0},logging:{enabled:!0,harvestTimeSeconds:10,autoStart:!0,level:u.p_.INFO},metrics:{enabled:!0,autoStart:!0},obfuscate:void 0,page_action:{enabled:!0,harvestTimeSeconds:30,autoStart:!0},page_view_event:{enabled:!0,autoStart:!0},page_view_timing:{enabled:!0,harvestTimeSeconds:30,long_task:!1,autoStart:!0},privacy:{cookies_enabled:!0},proxy:{assets:void 0,beacon:void 0},session:{expiresMs:d.wk,inactiveMs:d.BB},session_replay:{autoStart:!0,enabled:!1,harvestTimeSeconds:60,preload:!1,sampling_rate:10,error_sampling_rate:100,collect_fonts:!1,inline_images:!1,inline_stylesheet:!0,mask_all_inputs:!0,get mask_text_selector(){return e.mask_selector},set mask_text_selector(t){l(t)?e.mask_selector="".concat(t,",").concat(f):""===t||null===t?e.mask_selector=f:(0,g.R)("An invalid session_replay.mask_selector was provided. '*' will be used.",t)},get block_class(){return"nr-block"},get ignore_class(){return"nr-ignore"},get mask_text_class(){return"nr-mask"},get block_selector(){return e.block_selector},set block_selector(t){l(t)?e.block_selector+=",".concat(t):""!==t&&(0,g.R)("An invalid session_replay.block_selector was provided and will not be used",t)},get mask_input_options(){return e.mask_input_options},set mask_input_options(t){t&&"object"==typeof t?e.mask_input_options={...t,password:!0}:(0,g.R)("An invalid session_replay.mask_input_option was provided and will not be used",t)}},session_trace:{enabled:!0,harvestTimeSeconds:10,autoStart:!0},soft_navigations:{enabled:!0,harvestTimeSeconds:10,autoStart:!0},spa:{enabled:!0,harvestTimeSeconds:10,autoStart:!0},ssl:void 0}},h={},v="All configuration objects require an agent identifier!";function m(e){if(!e)throw new Error(v);if(!h[e])throw new Error("Configuration for ".concat(e," was never set"));return h[e]}function b(e,t){if(!e)throw new Error(v);h[e]=(0,i.a)(t,p());const r=(0,n.nY)(e);r&&(r.init=h[e])}function y(e,t){if(!e)throw new Error(v);var r=m(e);if(r){for(var n=t.split("."),i=0;i<n.length-1;i++)if("object"!=typeof(r=r[n[i]]))return;r=r[n[n.length-1]]}return r}const w={accountID:void 0,trustKey:void 0,agentID:void 0,licenseKey:void 0,applicationID:void 0,xpid:void 0},A={};function R(e,t){if(!e)throw new Error("All loader-config objects require an agent identifier!");A[e]=(0,i.a)(t,w);const r=(0,n.nY)(e);r&&(r.loader_config=A[e])}const x=(0,n.dV)().o;var _=r(6154),E=r(9324);const N={buildEnv:E.F3,distMethod:E.Xs,version:E.xv,originTime:_.WN},k={customTransaction:void 0,disabled:!1,isolatedBacklog:!1,loaderType:void 0,maxBytes:3e4,onerror:void 0,origin:""+_.gm.location,ptid:void 0,releaseIds:{},appMetadata:{},session:void 0,denyList:void 0,harvestCount:0,timeKeeper:void 0},T={};function S(e){if(!e)throw new Error("All runtime objects require an agent identifier!");if(!T[e])throw new Error("Runtime for ".concat(e," was never set"));return T[e]}function j(e,t){if(!e)throw new Error("All runtime objects require an agent identifier!");T[e]={...(0,i.a)(t,k),...N};const r=(0,n.nY)(e);r&&(r.runtime=T[e])}function I(e){return function(e){try{const t=s(e);return!!t.licenseKey&&!!t.errorBeacon&&!!t.applicationID}catch(e){return!1}}(e)}},7864:(e,t,r)=>{"use strict";r.d(t,{a:()=>i});var n=r(944);function i(e,t){try{if(!e||"object"!=typeof e)return(0,n.R)("Setting a Configurable requires an object as input");if(!t||"object"!=typeof t)return(0,n.R)("Setting a Configurable requires a model to set its initial properties");const r=Object.create(Object.getPrototypeOf(t),Object.getOwnPropertyDescriptors(t)),o=0===Object.keys(r).length?e:r;for(let a in o)if(void 0!==e[a])try{if(null===e[a]){r[a]=null;continue}Array.isArray(e[a])&&Array.isArray(t[a])?r[a]=Array.from(new Set([...e[a],...t[a]])):"object"==typeof e[a]&&"object"==typeof t[a]?r[a]=i(e[a],t[a]):r[a]=e[a]}catch(e){(0,n.R)("An error occurred while setting a property of a Configurable",e)}return r}catch(e){(0,n.R)("An error occured while setting a Configurable",e)}}},9324:(e,t,r)=>{"use strict";r.d(t,{F3:()=>i,Xs:()=>o,xv:()=>n});const n="1.261.1",i="PROD",o="CDN"},6154:(e,t,r)=>{"use strict";r.d(t,{OF:()=>c,RI:()=>i,Vr:()=>d,WN:()=>g,bv:()=>o,gm:()=>a,lT:()=>l,mw:()=>s,sb:()=>u});var n=r(1863);const i="undefined"!=typeof window&&!!window.document,o="undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self.navigator instanceof WorkerNavigator||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis.navigator instanceof WorkerNavigator),a=i?window:"undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis),s=Boolean("hidden"===a?.document?.visibilityState),c=/iPad|iPhone|iPod/.test(a.navigator?.userAgent),u=c&&"undefined"==typeof SharedWorker,l=((()=>{const e=a.navigator?.userAgent?.match(/Firefox[/\s](\d+\.\d+)/);Array.isArray(e)&&e.length>=2&&e[1]})(),Boolean(i&&window.document.documentMode)),d=!!a.navigator?.sendBeacon,g=Date.now()-(0,n.t)()},4777:(e,t,r)=>{"use strict";r.d(t,{J:()=>o});var n=r(944);const i={agentIdentifier:"",ee:void 0};class o{constructor(e){try{if("object"!=typeof e)return(0,n.R)("shared context requires an object as input");this.sharedContext={},Object.assign(this.sharedContext,i),Object.entries(e).forEach((e=>{let[t,r]=e;Object.keys(i).includes(t)&&(this.sharedContext[t]=r)}))}catch(e){(0,n.R)("An error occurred while setting SharedContext",e)}}}},1687:(e,t,r)=>{"use strict";r.d(t,{Ak:()=>c,Ze:()=>d,x3:()=>u});var n=r(7836),i=r(1478),o=r(3606),a=r(860);const s={};function c(e,t){const r={staged:!1,priority:a.P[t]||0};l(e),s[e].get(t)||s[e].set(t,r)}function u(e,t){l(e),s[e].get(t)&&s[e].delete(t),s[e].size&&g(e)}function l(e){if(!e)throw new Error("agentIdentifier required");s[e]||(s[e]=new Map)}function d(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"",t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"feature",r=arguments.length>2&&void 0!==arguments[2]&&arguments[2];if(l(e),!e||!s[e].get(t)||r)return f(e,t);s[e].get(t).staged=!0,g(e)}function g(e){const t=Array.from(s[e]);t.every((e=>{let[t,r]=e;return r.staged}))&&(t.sort(((e,t)=>e[1].priority-t[1].priority)),t.forEach((t=>{let[r]=t;s[e].delete(r),f(e,r)})))}function f(e,t){const r=e?n.ee.get(e):n.ee,a=o.i.handlers;if(r.backlog&&a){var s=r.backlog[t],c=a[t];if(c){for(var u=0;s&&u<s.length;++u)p(s[u],c);(0,i.$)(c,(function(e,t){(0,i.$)(t,(function(t,r){r[0].on(e,r[1])}))}))}r.isolatedBacklog||delete a[t],r.backlog[t]=null,r.emit("drain-"+t,[])}}function p(e,t){var r=e[1];(0,i.$)(t[r],(function(t,r){var n=e[0];if(r[0]===n){var i=r[1],o=e[3],a=e[2];i.apply(o,a)}}))}},7836:(e,t,r)=>{"use strict";r.d(t,{P:()=>c,ee:()=>u});var n=r(384),i=r(8990),o=r(2983),a=r(2646),s=r(5607);const c="nr@context:".concat(s.W),u=function e(t,r){var n={},s={},l={},d=!1;try{d=16===r.length&&(0,o.fr)(r).isolatedBacklog}catch(e){}var g={on:p,addEventListener:p,removeEventListener:function(e,t){var r=n[e];if(!r)return;for(var i=0;i<r.length;i++)r[i]===t&&r.splice(i,1)},emit:function(e,r,n,i,o){!1!==o&&(o=!0);if(u.aborted&&!i)return;t&&o&&t.emit(e,r,n);for(var a=f(n),c=h(e),l=c.length,d=0;d<l;d++)c[d].apply(a,r);var p=m()[s[e]];p&&p.push([g,e,r,a]);return a},get:v,listeners:h,context:f,buffer:function(e,t){const r=m();if(t=t||"feature",g.aborted)return;Object.entries(e||{}).forEach((e=>{let[n,i]=e;s[i]=t,t in r||(r[t]=[])}))},abort:function(){g._aborted=!0,Object.keys(g.backlog).forEach((e=>{delete g.backlog[e]}))},isBuffering:function(e){return!!m()[s[e]]},debugId:r,backlog:d?{}:t&&"object"==typeof t.backlog?t.backlog:{},isolatedBacklog:d};return Object.defineProperty(g,"aborted",{get:()=>{let e=g._aborted||!1;return e||(t&&(e=t.aborted),e)}}),g;function f(e){return e&&e instanceof a.y?e:e?(0,i.I)(e,c,(()=>new a.y(c))):new a.y(c)}function p(e,t){n[e]=h(e).concat(t)}function h(e){return n[e]||[]}function v(t){return l[t]=l[t]||e(g,t)}function m(){return g.backlog}}(void 0,"globalEE"),l=(0,n.Zm)();l.ee||(l.ee=u)},2646:(e,t,r)=>{"use strict";r.d(t,{y:()=>n});class n{constructor(e){this.contextId=e}}},9908:(e,t,r)=>{"use strict";r.d(t,{d:()=>n,p:()=>i});var n=r(7836).ee.get("handle");function i(e,t,r,i,o){o?(o.buffer([e],i),o.emit(e,t,r)):(n.buffer([e],i),n.emit(e,t,r))}},3606:(e,t,r)=>{"use strict";r.d(t,{i:()=>o});var n=r(9908);o.on=a;var i=o.handlers={};function o(e,t,r,o){a(o||n.d,i,e,t,r)}function a(e,t,r,i,o){o||(o="feature"),e||(e=n.d);var a=t[o]=t[o]||{};(a[r]=a[r]||[]).push([e,i])}},3878:(e,t,r)=>{"use strict";r.d(t,{DD:()=>c,jT:()=>a,sp:()=>s});var n=r(6154);let i=!1,o=!1;try{const e={get passive(){return i=!0,!1},get signal(){return o=!0,!1}};n.gm.addEventListener("test",null,e),n.gm.removeEventListener("test",null,e)}catch(e){}function a(e,t){return i||o?{capture:!!e,passive:i,signal:t}:!!e}function s(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3?arguments[3]:void 0;window.addEventListener(e,t,a(r,n))}function c(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3?arguments[3]:void 0;document.addEventListener(e,t,a(r,n))}},5607:(e,t,r)=>{"use strict";r.d(t,{W:()=>n});const n=(0,r(9566).bz)()},9566:(e,t,r)=>{"use strict";r.d(t,{LA:()=>s,bz:()=>a});var n=r(6154);const i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";function o(e,t){return e?15&e[t]:16*Math.random()|0}function a(){const e=n.gm?.crypto||n.gm?.msCrypto;let t,r=0;return e&&e.getRandomValues&&(t=e.getRandomValues(new Uint8Array(30))),i.split("").map((e=>"x"===e?o(t,r++).toString(16):"y"===e?(3&o()|8).toString(16):e)).join("")}function s(e){const t=n.gm?.crypto||n.gm?.msCrypto;let r,i=0;t&&t.getRandomValues&&(r=t.getRandomValues(new Uint8Array(e)));const a=[];for(var s=0;s<e;s++)a.push(o(r,i++).toString(16));return a.join("")}},2614:(e,t,r)=>{"use strict";r.d(t,{BB:()=>a,H3:()=>n,g:()=>u,iL:()=>c,tS:()=>s,uh:()=>i,wk:()=>o});const n="NRBA",i="SESSION",o=144e5,a=18e5,s={STARTED:"session-started",PAUSE:"session-pause",RESET:"session-reset",RESUME:"session-resume",UPDATE:"session-update"},c={SAME_TAB:"same-tab",CROSS_TAB:"cross-tab"},u={OFF:0,FULL:1,ERROR:2}},1863:(e,t,r)=>{"use strict";function n(){return Math.floor(performance.now())}r.d(t,{t:()=>n})},944:(e,t,r)=>{"use strict";function n(e,t){"function"==typeof console.warn&&(console.warn("New Relic: ".concat(e)),t&&console.warn(t))}r.d(t,{R:()=>n})},5284:(e,t,r)=>{"use strict";r.d(t,{t:()=>c,B:()=>s});var n=r(7836),i=r(6154);const o="newrelic";const a=new Set,s={};function c(e,t){const r=n.ee.get(t);s[t]??={},e&&"object"==typeof e&&(a.has(t)||(r.emit("rumresp",[e]),s[t]=e,a.add(t),function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};try{i.gm.dispatchEvent(new CustomEvent(o,{detail:e}))}catch(e){}}({loaded:!0})))}},8990:(e,t,r)=>{"use strict";r.d(t,{I:()=>i});var n=Object.prototype.hasOwnProperty;function i(e,t,r){if(n.call(e,t))return e[t];var i=r();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:i,writable:!0,enumerable:!1}),i}catch(e){}return e[t]=i,i}},6389:(e,t,r)=>{"use strict";function n(e){var t=this;let r=arguments.length>1&&void 0!==arguments[1]?arguments[1]:500,n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const i=n?.leading||!1;let o;return function(){for(var n=arguments.length,a=new Array(n),s=0;s<n;s++)a[s]=arguments[s];i&&void 0===o&&(e.apply(t,a),o=setTimeout((()=>{o=clearTimeout(o)}),r)),i||(clearTimeout(o),o=setTimeout((()=>{e.apply(t,a)}),r))}}function i(e){var t=this;let r=!1;return function(){if(!r){r=!0;for(var n=arguments.length,i=new Array(n),o=0;o<n;o++)i[o]=arguments[o];e.apply(t,i)}}}r.d(t,{J:()=>i,s:()=>n})},1478:(e,t,r)=>{"use strict";r.d(t,{$:()=>n});const n=(e,t)=>Object.entries(e||{}).map((e=>{let[r,n]=e;return t(r,n)}))},3304:(e,t,r)=>{"use strict";r.d(t,{A:()=>o});var n=r(7836);const i=()=>{const e=new WeakSet;return(t,r)=>{if("object"==typeof r&&null!==r){if(e.has(r))return;e.add(r)}return r}};function o(e){try{return JSON.stringify(e,i())}catch(e){try{n.ee.emit("internal-error",[e])}catch(e){}}}},5289:(e,t,r)=>{"use strict";r.d(t,{GG:()=>o,sB:()=>a});var n=r(3878);function i(){return"undefined"==typeof document||"complete"===document.readyState}function o(e,t){if(i())return e();(0,n.sp)("load",e,t)}function a(e){if(i())return e();(0,n.DD)("DOMContentLoaded",e)}},384:(e,t,r)=>{"use strict";r.d(t,{NT:()=>o,US:()=>l,Zm:()=>a,bQ:()=>c,dV:()=>s,nY:()=>u,pV:()=>d});var n=r(6154),i=r(1863);const o={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net"};function a(){return n.gm.NREUM||(n.gm.NREUM={}),void 0===n.gm.newrelic&&(n.gm.newrelic=n.gm.NREUM),n.gm.NREUM}function s(){let e=a();return e.o||(e.o={ST:n.gm.setTimeout,SI:n.gm.setImmediate,CT:n.gm.clearTimeout,XHR:n.gm.XMLHttpRequest,REQ:n.gm.Request,EV:n.gm.Event,PR:n.gm.Promise,MO:n.gm.MutationObserver,FETCH:n.gm.fetch}),e}function c(e,t){let r=a();r.initializedAgents??={},t.initializedAt={ms:(0,i.t)(),date:new Date},r.initializedAgents[e]=t}function u(e){let t=a();return t.initializedAgents?.[e]}function l(e,t){a()[e]=t}function d(){return function(){let e=a();const t=e.info||{};e.info={beacon:o.beacon,errorBeacon:o.errorBeacon,...t}}(),function(){let e=a();const t=e.init||{};e.init={...t}}(),s(),function(){let e=a();const t=e.loader_config||{};e.loader_config={...t}}(),a()}},2843:(e,t,r)=>{"use strict";r.d(t,{u:()=>i});var n=r(3878);function i(e){let t=arguments.length>1&&void 0!==arguments[1]&&arguments[1],r=arguments.length>2?arguments[2]:void 0,i=arguments.length>3?arguments[3]:void 0;(0,n.DD)("visibilitychange",(function(){if(t)return void("hidden"===document.visibilityState&&e());e(document.visibilityState)}),r,i)}},3434:(e,t,r)=>{"use strict";r.d(t,{YM:()=>c});var n=r(7836),i=r(5607);const o="nr@original:".concat(i.W);var a=Object.prototype.hasOwnProperty,s=!1;function c(e,t){return e||(e=n.ee),r.inPlace=function(e,t,n,i,o){n||(n="");const a="-"===n.charAt(0);for(let s=0;s<t.length;s++){const c=t[s],u=e[c];l(u)||(e[c]=r(u,a?c+n:n,i,c,o))}},r.flag=o,r;function r(t,r,n,s,c){return l(t)?t:(r||(r=""),nrWrapper[o]=t,function(e,t,r){if(Object.defineProperty&&Object.keys)try{return Object.keys(e).forEach((function(r){Object.defineProperty(t,r,{get:function(){return e[r]},set:function(t){return e[r]=t,t}})})),t}catch(e){u([e],r)}for(var n in e)a.call(e,n)&&(t[n]=e[n])}(t,nrWrapper,e),nrWrapper);function nrWrapper(){var o,a,l,d;try{a=this,o=[...arguments],l="function"==typeof n?n(o,a):n||{}}catch(t){u([t,"",[o,a,s],l],e)}i(r+"start",[o,a,s],l,c);try{return d=t.apply(a,o)}catch(e){throw i(r+"err",[o,a,e],l,c),e}finally{i(r+"end",[o,a,d],l,c)}}}function i(r,n,i,o){if(!s||t){var a=s;s=!0;try{e.emit(r,n,i,t,o)}catch(t){u([t,r,n,i],e)}s=a}}}function u(e,t){t||(t=n.ee);try{t.emit("internal-error",e)}catch(e){}}function l(e){return!(e&&"function"==typeof e&&e.apply&&!e[o])}},993:(e,t,r)=>{"use strict";r.d(t,{ET:()=>o,It:()=>a,YY:()=>c,bu:()=>s,p_:()=>i,zk:()=>u});var n=r(860);const i={ERROR:"ERROR",WARN:"WARN",INFO:"INFO",DEBUG:"DEBUG",TRACE:"TRACE"},o="log",a=(n.K.logging,1e6),s="failed to wrap logger: ",c="invalid log level: ",u="ignored log: "},3969:(e,t,r)=>{"use strict";r.d(t,{TZ:()=>n,XG:()=>s,rs:()=>i,xV:()=>a,z_:()=>o});const n=r(860).K.metrics,i="sm",o="cm",a="storeSupportabilityMetrics",s="storeEventMetrics"},6630:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K.pageViewEvent},782:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K.pageViewTiming},6344:(e,t,r)=>{"use strict";r.d(t,{G4:()=>i});var n=r(2614);r(860).K.sessionReplay;const i={RECORD:"recordReplay",PAUSE:"pauseReplay",REPLAY_RUNNING:"replayRunning",ERROR_DURING_REPLAY:"errorDuringReplay"};n.g.ERROR,n.g.FULL,n.g.OFF},4234:(e,t,r)=>{"use strict";r.d(t,{W:()=>i});var n=r(7836);class i{constructor(e,t,r){this.agentIdentifier=e,this.aggregator=t,this.ee=n.ee.get(e),this.featureName=r,this.blocked=!1}}},2266:(e,t,r)=>{"use strict";r.d(t,{j:()=>j});var n=r(860),i=r(2983),o=r(9908),a=r(7836),s=r(1687),c=r(5289),u=r(6154),l=r(944),d=r(3969),g=r(384),f=r(6344);const p=["setErrorHandler","finished","addToTrace","addRelease","addPageAction","setCurrentRouteName","setPageViewName","setCustomAttribute","interaction","noticeError","setUserId","setApplicationVersion","start",f.G4.RECORD,f.G4.PAUSE,"log","wrapLogger"],h=["setErrorHandler","finished","addToTrace","addRelease"];var v=r(1863),m=r(2614),b=r(993),y=r(3304);function w(e){return"string"==typeof e&&Object.values(b.p_).some((t=>t.toUpperCase()===e.toUpperCase()))}var A=r(2646),R=r(3434);function x(e,t,r,n){const i=function(e){return(e||a.ee).get("logger")}(e),o=(0,R.YM)(i),s=new A.y(a.P);return s.level=n.level,s.customAttributes=n.customAttributes,o.inPlace(t,[r],"wrap-logger-",s),i}function _(){const e=(0,g.pV)();p.forEach((t=>{e[t]=function(){for(var r=arguments.length,n=new Array(r),i=0;i<r;i++)n[i]=arguments[i];return function(t){for(var r=arguments.length,n=new Array(r>1?r-1:0),i=1;i<r;i++)n[i-1]=arguments[i];let o=[];return Object.values(e.initializedAgents).forEach((e=>{e.exposed&&e.api[t]&&o.push(e.api[t](...n))})),o.length>1?o:o[0]}(t,...n)}}))}const E={};function N(e,t){let g=arguments.length>2&&void 0!==arguments[2]&&arguments[2];t||(0,s.Ak)(e,"api");const p={};var A=a.ee.get(e),R=A.get("tracer");E[e]=m.g.OFF,A.on(f.G4.REPLAY_RUNNING,(t=>{E[e]=t}));var _="api-",N=_+"ixn-";function k(t,r,n,o){const a=(0,i.Vp)(e);return null===r?delete a.jsAttributes[t]:(0,i.x1)(e,{...a,jsAttributes:{...a.jsAttributes,[t]:r}}),j(_,n,!0,o||null===r?"session":void 0)(t,r)}function T(){}p.log=function(e){let{customAttributes:t={},level:r=b.p_.INFO}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return t&&"object"==typeof t||(t={}),"string"==typeof e&&e?w(r)?e.length>b.It?(0,l.R)(b.zk+"> "+b.It+" bytes: ",e.slice(0,25)+"..."):void function(e,t){let r=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:b.p_.INFO;try{if("string"!=typeof t){const e=(0,y.A)(t);t=e&&"{}"!==e?e:String(t)}}catch(e){return void(0,l.R)("could not cast log message to string",t)}(0,o.p)(d.xV,["API/logging/".concat(i.toLowerCase(),"/called")],void 0,n.K.metrics,e),(0,o.p)(b.ET,[(0,v.t)(),t,r,i],void 0,n.K.logging,e)}(A,e,t,r.toUpperCase()):(0,l.R)(b.YY+r,b.p_):(0,l.R)(b.zk+"invalid message")},p.wrapLogger=function(e,t){let{customAttributes:r={},level:n=b.p_.INFO}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return r&&"object"==typeof r||(r={}),"object"==typeof e&&e&&"string"==typeof t&&t&&"function"==typeof e[t]&&"object"==typeof r?w(n)?void x(A,e,t,{customAttributes:r,level:n.toUpperCase()}):(0,l.R)(b.bu+b.YY+n,b.p_):(0,l.R)(b.bu+"invalid argument(s)")},h.forEach((e=>{p[e]=j(_,e,!0,"api")})),p.addPageAction=j(_,"addPageAction",!0,n.K.pageAction),p.setPageViewName=function(t,r){if("string"==typeof t)return"/"!==t.charAt(0)&&(t="/"+t),(0,i.fr)(e).customTransaction=(r||"http://custom.transaction")+t,j(_,"setPageViewName",!0)()},p.setCustomAttribute=function(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2];if("string"==typeof e){if(["string","number","boolean"].includes(typeof t)||null===t)return k(e,t,"setCustomAttribute",r);(0,l.R)("Failed to execute setCustomAttribute.\nNon-null value must be a string, number or boolean type, but a type of <".concat(typeof t,"> was provided."))}else(0,l.R)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e,"> was provided."))},p.setUserId=function(e){if("string"==typeof e||null===e)return k("enduser.id",e,"setUserId",!0);(0,l.R)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e,"> was provided."))},p.setApplicationVersion=function(e){if("string"==typeof e||null===e)return k("application.version",e,"setApplicationVersion",!1);(0,l.R)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e,">."))},p.start=()=>{try{(0,o.p)(d.xV,["API/start/called"],void 0,n.K.metrics,A),A.emit("manual-start-all")}catch(e){(0,l.R)("An unexpected issue occurred",e)}},p[f.G4.RECORD]=function(){(0,o.p)(d.xV,["API/recordReplay/called"],void 0,n.K.metrics,A),(0,o.p)(f.G4.RECORD,[],void 0,n.K.sessionReplay,A)},p[f.G4.PAUSE]=function(){(0,o.p)(d.xV,["API/pauseReplay/called"],void 0,n.K.metrics,A),(0,o.p)(f.G4.PAUSE,[],void 0,n.K.sessionReplay,A)},p.interaction=function(e){return(new T).get("object"==typeof e?e:{})};const S=T.prototype={createTracer:function(e,t){var r={},i=this,a="function"==typeof t;return(0,o.p)(d.xV,["API/createTracer/called"],void 0,n.K.metrics,A),g||(0,o.p)(N+"tracer",[(0,v.t)(),e,r],i,n.K.spa,A),function(){if(R.emit((a?"":"no-")+"fn-start",[(0,v.t)(),i,a],r),a)try{return t.apply(this,arguments)}catch(e){const t="string"==typeof e?new Error(e):e;throw R.emit("fn-err",[arguments,this,t],r),t}finally{R.emit("fn-end",[(0,v.t)()],r)}}}};function j(e,t,r,i){return function(){return(0,o.p)(d.xV,["API/"+t+"/called"],void 0,n.K.metrics,A),i&&(0,o.p)(e+t,[(0,v.t)(),...arguments],r?null:this,i,A),r?void 0:this}}function I(){r.e(296).then(r.bind(r,8778)).then((t=>{let{setAPI:r}=t;r(e),(0,s.Ze)(e,"api")})).catch((e=>{(0,l.R)("Downloading runtime APIs failed...",e),A.abort()}))}return["actionText","setName","setAttribute","save","ignore","onEnd","getContext","end","get"].forEach((e=>{S[e]=j(N,e,void 0,g?n.K.softNav:n.K.spa)})),p.setCurrentRouteName=g?j(N,"routeName",void 0,n.K.softNav):j(_,"routeName",!0,n.K.spa),p.noticeError=function(t,r){"string"==typeof t&&(t=new Error(t)),(0,o.p)(d.xV,["API/noticeError/called"],void 0,n.K.metrics,A),(0,o.p)("err",[t,(0,v.t)(),!1,r,!!E[e]],void 0,n.K.jserrors,A)},u.RI?(0,c.GG)((()=>I()),!0):I(),p}var k=r(5284);const T=e=>{const t=e.startsWith("http");e+="/",r.p=t?e:"https://"+e};let S=!1;function j(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},r=arguments.length>2?arguments[2]:void 0,n=arguments.length>3?arguments[3]:void 0,{init:o,info:a,loader_config:s,runtime:c={loaderType:r},exposed:l=!0}=t;const d=(0,g.pV)();a||(o=d.init,a=d.info,s=d.loader_config),(0,i.xN)(e.agentIdentifier,o||{}),(0,i.aN)(e.agentIdentifier,s||{}),a.jsAttributes??={},u.bv&&(a.jsAttributes.isWorker=!0),(0,i.x1)(e.agentIdentifier,a);const f=(0,i.D0)(e.agentIdentifier),p=[a.beacon,a.errorBeacon];S||(f.proxy.assets&&(T(f.proxy.assets),p.push(f.proxy.assets)),f.proxy.beacon&&p.push(f.proxy.beacon),_(),(0,g.US)("activatedFeatures",k.B),e.runSoftNavOverSpa&&=!0===f.soft_navigations.enabled&&f.feature_flags.includes("soft_nav")),c.denyList=[...f.ajax.deny_list||[],...f.ajax.block_internal?p:[]],c.ptid=e.agentIdentifier,(0,i.V)(e.agentIdentifier,c),void 0===e.api&&(e.api=N(e.agentIdentifier,n,e.runSoftNavOverSpa)),void 0===e.exposed&&(e.exposed=l),S=!0}},8374:(e,t,r)=>{r.nc=(()=>{try{return document?.currentScript?.nonce}catch(e){}return""})()},860:(e,t,r)=>{"use strict";r.d(t,{K:()=>n,P:()=>i});const n={ajax:"ajax",jserrors:"jserrors",logging:"logging",metrics:"metrics",pageAction:"page_action",pageViewEvent:"page_view_event",pageViewTiming:"page_view_timing",sessionReplay:"session_replay",sessionTrace:"session_trace",softNav:"soft_navigations",spa:"spa"},i={[n.pageViewEvent]:1,[n.pageViewTiming]:2,[n.metrics]:3,[n.jserrors]:4,[n.spa]:5,[n.ajax]:6,[n.sessionTrace]:7,[n.pageAction]:8,[n.softNav]:9,[n.sessionReplay]:10,[n.logging]:11}}},n={};function i(e){var t=n[e];if(void 0!==t)return t.exports;var o=n[e]={exports:{}};return r[e](o,o.exports,i),o.exports}i.m=r,i.d=(e,t)=>{for(var r in t)i.o(t,r)&&!i.o(e,r)&&Object.defineProperty(e,r,{enumerable:!0,get:t[r]})},i.f={},i.e=e=>Promise.all(Object.keys(i.f).reduce(((t,r)=>(i.f[r](e,t),t)),[])),i.u=e=>"nr-rum-1.261.1.min.js",i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),e={},t="NRBA-1.261.1.PROD:",i.l=(r,n,o,a)=>{if(e[r])e[r].push(n);else{var s,c;if(void 0!==o)for(var u=document.getElementsByTagName("script"),l=0;l<u.length;l++){var d=u[l];if(d.getAttribute("src")==r||d.getAttribute("data-webpack")==t+o){s=d;break}}if(!s){c=!0;var g={296:"sha512-UWYaho4X6eh6nJg6dRdp4swAyAzOdR2hxfaFrW6vqW9Xs5aWAh9ZBGThDETozoMUdDbUrkXNefP/eW2aWAo8BQ=="};(s=document.createElement("script")).charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.setAttribute("data-webpack",t+o),s.src=r,0!==s.src.indexOf(window.location.origin+"/")&&(s.crossOrigin="anonymous"),g[a]&&(s.integrity=g[a])}e[r]=[n];var f=(t,n)=>{s.onerror=s.onload=null,clearTimeout(p);var i=e[r];if(delete e[r],s.parentNode&&s.parentNode.removeChild(s),i&&i.forEach((e=>e(n))),t)return t(n)},p=setTimeout(f.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=f.bind(null,s.onerror),s.onload=f.bind(null,s.onload),c&&document.head.appendChild(s)}},i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.p="https://js-agent.newrelic.com/",(()=>{var e={840:0,374:0};i.f.j=(t,r)=>{var n=i.o(e,t)?e[t]:void 0;if(0!==n)if(n)r.push(n[2]);else{var o=new Promise(((r,i)=>n=e[t]=[r,i]));r.push(n[2]=o);var a=i.p+i.u(t),s=new Error;i.l(a,(r=>{if(i.o(e,t)&&(0!==(n=e[t])&&(e[t]=void 0),n)){var o=r&&("load"===r.type?"missing":r.type),a=r&&r.target&&r.target.src;s.message="Loading chunk "+t+" failed.\n("+o+": "+a+")",s.name="ChunkLoadError",s.type=o,s.request=a,n[1](s)}}),"chunk-"+t,t)}};var t=(t,r)=>{var n,o,[a,s,c]=r,u=0;if(a.some((t=>0!==e[t]))){for(n in s)i.o(s,n)&&(i.m[n]=s[n]);if(c)c(i)}for(t&&t(r);u<a.length;u++)o=a[u],i.o(e,o)&&e[o]&&e[o][0](),e[o]=0},r=self["webpackChunk:NRBA-1.261.1.PROD"]=self["webpackChunk:NRBA-1.261.1.PROD"]||[];r.forEach(t.bind(null,0)),r.push=t.bind(null,r.push.bind(r))})(),(()=>{"use strict";i(8374);var e=i(944),t=i(6344),r=i(9566),n=i(7836);class o{agentIdentifier;constructor(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:(0,r.LA)(16);this.agentIdentifier=e,this.ee=n.ee.get(e)}#e(t){for(var r=arguments.length,n=new Array(r>1?r-1:0),i=1;i<r;i++)n[i-1]=arguments[i];if("function"==typeof this.api?.[t])return this.api[t](...n);(0,e.R)("Call to agent api ".concat(t," failed. The API is not currently initialized."))}addPageAction(e,t){return this.#e("addPageAction",e,t)}setPageViewName(e,t){return this.#e("setPageViewName",e,t)}setCustomAttribute(e,t,r){return this.#e("setCustomAttribute",e,t,r)}noticeError(e,t){return this.#e("noticeError",e,t)}setUserId(e){return this.#e("setUserId",e)}setApplicationVersion(e){return this.#e("setApplicationVersion",e)}setErrorHandler(e){return this.#e("setErrorHandler",e)}finished(e){return this.#e("finished",e)}addRelease(e,t){return this.#e("addRelease",e,t)}start(e){return this.#e("start",e)}recordReplay(){return this.#e(t.G4.RECORD)}pauseReplay(){return this.#e(t.G4.PAUSE)}addToTrace(e){return this.#e("addToTrace",e)}setCurrentRouteName(e){return this.#e("setCurrentRouteName",e)}interaction(){return this.#e("interaction")}log(e,t){return this.#e("logInfo",e,t)}wrapLogger(e,t,r){return this.#e("wrapLogger",e,t,r)}}var a=i(860),s=i(2983);const c=Object.values(a.K);function u(e){const t={};return c.forEach((r=>{t[r]=function(e,t){return!0===(0,s.gD)(t,"".concat(e,".enabled"))}(r,e)})),t}var l=i(2266);var d=i(1687),g=i(4234),f=i(5289),p=i(6154);const h=e=>p.RI&&!0===(0,s.gD)(e,"privacy.cookies_enabled");function v(e){return!!s.hR.MO&&h(e)&&!0===(0,s.gD)(e,"session_trace.enabled")}var m=i(6389);class b extends g.W{constructor(e,t,r){let n=!(arguments.length>3&&void 0!==arguments[3])||arguments[3];super(e,t,r),this.auto=n,this.abortHandler=void 0,this.featAggregate=void 0,this.onAggregateImported=void 0,!1===(0,s.gD)(this.agentIdentifier,"".concat(this.featureName,".autoStart"))&&(this.auto=!1),this.auto?(0,d.Ak)(e,r):this.ee.on("manual-start-all",(0,m.J)((()=>{(0,d.Ak)(this.agentIdentifier,this.featureName),this.auto=!0,this.importAggregator()})))}importAggregator(){let t,r=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};if(this.featAggregate||!this.auto)return;this.onAggregateImported=new Promise((e=>{t=e}));const n=async()=>{let n;try{if(h(this.agentIdentifier)){const{setupAgentSession:e}=await i.e(296).then(i.bind(i,2987));n=e(this.agentIdentifier)}}catch(t){(0,e.R)("A problem occurred when starting up session manager. This page will not start or extend any session.",t),this.ee.emit("internal-error",[t]),this.featureName===a.K.sessionReplay&&this.abortHandler?.()}try{if(!this.#t(this.featureName,n))return(0,d.Ze)(this.agentIdentifier,this.featureName),void t(!1);const{lazyFeatureLoader:e}=await i.e(296).then(i.bind(i,6103)),{Aggregate:o}=await e(this.featureName,"aggregate");this.featAggregate=new o(this.agentIdentifier,this.aggregator,r),t(!0)}catch(r){(0,e.R)("Downloading and initializing ".concat(this.featureName," failed..."),r),this.abortHandler?.(),(0,d.Ze)(this.agentIdentifier,this.featureName,!0),t(!1),this.ee&&this.ee.abort()}};p.RI?(0,f.GG)((()=>n()),!0):n()}#t(e,t){return e===a.K.sessionReplay?(r=this.agentIdentifier,n=t,!(!v(r)||!n?.isNew&&!n?.state.sessionReplayMode)):!(e===a.K.sessionTrace&&!t);var r,n}}var y=i(6630);class w extends b{static featureName=y.T;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,y.T,r),this.importAggregator()}}var A=i(4777),R=i(1478);class x extends A.J{constructor(e){super(e),this.aggregatedData={}}store(e,t,r,n,i){var o=this.getBucket(e,t,r,i);return o.metrics=function(e,t){t||(t={count:0});return t.count+=1,(0,R.$)(e,(function(e,r){t[e]=_(r,t[e])})),t}(n,o.metrics),o}merge(e,t,r,n,i){var o=this.getBucket(e,t,n,i);if(o.metrics){var a=o.metrics;a.count+=r.count,(0,R.$)(r,(function(e,t){if("count"!==e){var n=a[e],i=r[e];i&&!i.c?a[e]=_(i.t,n):a[e]=function(e,t){if(!t)return e;t.c||(t=E(t.t));return t.min=Math.min(e.min,t.min),t.max=Math.max(e.max,t.max),t.t+=e.t,t.sos+=e.sos,t.c+=e.c,t}(i,a[e])}}))}else o.metrics=r}storeMetric(e,t,r,n){var i=this.getBucket(e,t,r);return i.stats=_(n,i.stats),i}getBucket(e,t,r,n){this.aggregatedData[e]||(this.aggregatedData[e]={});var i=this.aggregatedData[e][t];return i||(i=this.aggregatedData[e][t]={params:r||{}},n&&(i.custom=n)),i}get(e,t){return t?this.aggregatedData[e]&&this.aggregatedData[e][t]:this.aggregatedData[e]}take(e){for(var t={},r="",n=!1,i=0;i<e.length;i++)t[r=e[i]]=Object.values(this.aggregatedData[r]||{}),t[r].length&&(n=!0),delete this.aggregatedData[r];return n?t:null}}function _(e,t){return null==e?function(e){e?e.c++:e={c:1};return e}(t):t?(t.c||(t=E(t.t)),t.c+=1,t.t+=e,t.sos+=e*e,e>t.max&&(t.max=e),e<t.min&&(t.min=e),t):{t:e}}function E(e){return{t:e,min:e,max:e,sos:e*e,c:1}}var N=i(384),k=i(3304);var T=i(9908),S=i(2843),j=i(3878),I=i(782),O=i(1863);class P extends b{static featureName=I.T;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,I.T,r),p.RI&&((0,S.u)((()=>(0,T.p)("docHidden",[(0,O.t)()],void 0,I.T,this.ee)),!0),(0,j.sp)("pagehide",(()=>(0,T.p)("winPagehide",[(0,O.t)()],void 0,I.T,this.ee))),this.importAggregator())}}var D=i(3969);class C extends b{static featureName=D.TZ;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,D.TZ,r),this.importAggregator()}}new class extends o{constructor(t,r){super(r),p.gm?(this.sharedAggregator=new x({agentIdentifier:this.agentIdentifier}),this.features={},(0,N.bQ)(this.agentIdentifier,this),this.desiredFeatures=new Set(t.features||[]),this.desiredFeatures.add(w),this.runSoftNavOverSpa=[...this.desiredFeatures].some((e=>e.featureName===a.K.softNav)),(0,l.j)(this,t,t.loaderType||"agent"),this.run()):(0,e.R)("Failed to initialize the agent. Could not determine the runtime environment.")}get config(){return{info:this.info,init:this.init,loader_config:this.loader_config,runtime:this.runtime}}run(){try{const t=u(this.agentIdentifier),r=[...this.desiredFeatures];r.sort(((e,t)=>a.P[e.featureName]-a.P[t.featureName])),r.forEach((r=>{if(!t[r.featureName]&&r.featureName!==a.K.pageViewEvent)return;if(this.runSoftNavOverSpa&&r.featureName===a.K.spa)return;if(!this.runSoftNavOverSpa&&r.featureName===a.K.softNav)return;const n=function(e){switch(e){case a.K.ajax:return[a.K.jserrors];case a.K.sessionTrace:return[a.K.ajax,a.K.pageViewEvent];case a.K.sessionReplay:return[a.K.sessionTrace];case a.K.pageViewTiming:return[a.K.pageViewEvent];default:return[]}}(r.featureName);n.every((e=>e in this.features))||(0,e.R)("".concat(r.featureName," is enabled but one or more dependent features has not been initialized (").concat((0,k.A)(n),"). This may cause unintended consequences or missing data...")),this.features[r.featureName]=new r(this.agentIdentifier,this.sharedAggregator)}))}catch(t){(0,e.R)("Failed to initialize all enabled instrument classes (agent aborted) -",t);for(const e in this.features)this.features[e].abortHandler?.();const r=(0,N.Zm)();delete r.initializedAgents[this.agentIdentifier]?.api,delete r.initializedAgents[this.agentIdentifier]?.features,delete this.sharedAggregator;return r.ee.get(this.agentIdentifier).abort(),!1}}}({features:[w,P,C],loaderType:"lite"})})()})();</script>
<meta name="citation_title" content="Scaling {AI} Sustainably: An Uncharted Territory">
<link rel="shortcut icon" href="https://www.usenix.org/sites/default/files/waves_favicon.ico" type="image/vnd.microsoft.icon">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="citation_author" content="Carole-Jean Wu">
<meta name="citation_author_institution" content="Meta">
<meta name="description" content="The 2024 USENIX Annual Technical Conference will take place July 10–12, 2024, at the Hyatt Regency Santa Clara, in Santa Clara, CA, USA. USENIX ATC '24 will bring together leading systems researchers for cutting-edge systems research and the opportunity to gain insight into a wealth of must-know topics.">
<meta name="citation_author_email" content="carolejeanwu@meta.com">
<meta name="citation_publication_date" content="2024">
<meta name="rating" content="general">
<meta name="generator" content="Drupal 7 (http://drupal.org)">
<link rel="canonical" href="https://www.usenix.org/conference/atc24/technical-sessions">
<link rel="shortlink" href="https://www.usenix.org/node/298665">
<meta property="og:type" content="website">
<meta property="og:site_name" content="USENIX">
<meta property="og:title" content="USENIX ATC '24 Technical Sessions">
<meta property="og:url" content="https://www.usenix.org/conference/atc24/technical-sessions">
<meta property="og:description" content="The 2024 USENIX Annual Technical Conference will take place July 10–12, 2024, at the Hyatt Regency Santa Clara, in Santa Clara, CA, USA. USENIX ATC '24 will bring together leading systems researchers for cutting-edge systems research and the opportunity to gain insight into a wealth of must-know topics.">
<meta property="og:updated_time" content="2024-06-26T17:22:37-07:00">
<meta property="og:image" content="https://www.usenix.org/sites/default/files/atc24_banner_social_1200x630.png">
<meta property="og:image" content="https://www.usenix.org/sites/default/files/usenix_og_1200x630_2.png">
<meta property="og:image:url" content="https://www.usenix.org/sites/default/files/usenix_og_1200x630_2.png">
<meta property="og:image:secure_url" content="https://www.usenix.org/sites/default/files/atc24_banner_social_1200x630.png">
<meta property="og:image:type" content="image/png">
<meta name="twitter:image:width" content="1200">
<meta name="twitter:image:height" content="630">
<meta property="article:published_time" content="2024-05-09T15:31:46-07:00">
<meta property="article:modified_time" content="2024-06-26T17:22:37-07:00">
  <title>USENIX ATC '24 Technical Sessions | USENIX</title>
  <link type="text/css" rel="stylesheet" href="https://www.usenix.org/sites/default/files/css/css_ywqYrtSodM0qVMzAkOSnqZ_iBnAVpkSeD_yPtUN9tBk.css" media="all">
<link type="text/css" rel="stylesheet" href="https://www.usenix.org/sites/default/files/css/css_nUFTrBzuSS1e6iNFoYIyAptja28IikBBh8IfX_l3-Jw.css" media="all">
<link type="text/css" rel="stylesheet" href="https://www.usenix.org/sites/default/files/css/css_KMWXOV-pH6GjOMUFkn6VdZ1T0fURpw9oOC6s4EWTOkg.css" media="all">
<link type="text/css" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.4.0/css/font-awesome.min.css" media="all">
<link type="text/css" rel="stylesheet" href="https://www.usenix.org/sites/default/files/css/css_sDw2Vcm7vjgeMVWnCCoM-gPJl0XsgNBNyLD_tJHG4Kk.css" media="all">
<link type="text/css" rel="stylesheet" href="https://www.usenix.org/sites/default/files/css/css_amIuBZcKwy0thRMOwn-bQ5ZaDMDkwkNxhi2Pgyn6AmQ.css" media="all">
  <script type="text/javascript" src="https://www.usenix.org/sites/default/files/js/js_fyV0VVkC6Q3xduxGurKMTFIU2dMmArUrbAdZORL-9WQ.js"></script><style type="text/css" id="CookieConsentStateDisplayStyles">.cookieconsent-optin-preferences,.cookieconsent-optin-statistics,.cookieconsent-optin-marketing,.cookieconsent-optin{display:none;}.cookieconsent-optout-preferences,.cookieconsent-optout-statistics,.cookieconsent-optout-marketing,.cookieconsent-optout{display:block;display:initial;}</style>
<script type="text/javascript" src="https://www.usenix.org/sites/default/files/js/js_s7yA-hwRxnKty__ED6DuqmTMKG39xvpRyrtyCrbWH4M.js"></script>
<script type="text/javascript" src="https://www.usenix.org/sites/default/files/js/js_ji3W2YMVDPqfrJn3DRbbC-3pD_gTOz5f4UdYE6j-T9Y.js"></script>
<script type="text/javascript" src="https://www.usenix.org/sites/default/files/js/js_zcrgiJuhRoVX_W-YstkFzG-vLdX5p8zkRuBNOtAPnSs.js"></script>
<script type="text/javascript" src="https://js.stripe.com/v3"></script>
<script type="text/javascript" src="https://www.usenix.org/sites/default/files/js/js_9WNfBZMllca6UCxx0bNQeMnszVTbr1v8ZQwaaQtR9yE.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","setHasJsCookie":0,"ajaxPageState":{"theme":"neat_conference","theme_token":"n5fItbplQ34nC2X-OkvBebpcfFaL4QTtvXzk8jq2vNE","js":{"https:\/\/www.usenix.org\/sites\/default\/files\/google_tag\/usenix\/google_tag.script.js":1,"sites\/all\/modules\/jquery_update\/replace\/jquery\/1.8\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/beautytips\/js\/jquery.bt.min.js":1,"sites\/all\/modules\/beautytips\/js\/beautytips.min.js":1,"sites\/all\/modules\/jquery_update\/replace\/ui\/external\/jquery.cookie.js":1,"sites\/all\/modules\/behavior_weights\/behavior_weights.js":1,"sites\/all\/modules\/tb_megamenu\/js\/tb-megamenu-frontend.js":1,"sites\/all\/modules\/tb_megamenu\/js\/tb-megamenu-touch.js":1,"sites\/all\/modules\/cookiebot\/js\/cookiebot.js":1,"sites\/all\/modules\/field_group\/field_group.js":1,"sites\/all\/modules\/usenix\/usenix_conference\/js\/tech-schedule.js":1,"https:\/\/js.stripe.com\/v3":1,"sites\/all\/themes\/custom\/neat_conference\/js\/bibtex.js":1,"sites\/all\/themes\/custom\/neat_conference\/js\/mobile.js":1,"sites\/all\/themes\/custom\/neat_conference\/js\/jquery.slicknav.min.js":1,"sites\/all\/themes\/custom\/neat_conference\/js\/tech-schedule.js":1,"sites\/all\/themes\/custom\/neat_conference\/js\/training-program.js":1,"sites\/all\/themes\/custom\/neat_conference\/js\/organizers.js":1,"sites\/all\/themes\/custom\/neat_conference\/js\/protected-files.js":1,"sites\/all\/themes\/custom\/neat_conference\/js\/cookiebot.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.theme.css":1,"modules\/comment\/comment.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/poll\/poll.css":1,"modules\/search\/search.css":1,"sites\/all\/modules\/usenix\/usenix_conference\/css\/timezone-picker.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/workflow\/workflow_admin_ui\/workflow_admin_ui.css":1,"sites\/all\/modules\/views\/css\/views.css":1,"sites\/all\/modules\/cookiebot\/css\/cookiebot.css":1,"sites\/all\/modules\/media\/modules\/media_wysiwyg\/css\/media_wysiwyg.base.css":1,"sites\/all\/modules\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/geshifilter\/geshifilter.css":1,"sites\/all\/modules\/biblio\/biblio.css":1,"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/font-awesome\/4.4.0\/css\/font-awesome.min.css":1,"sites\/all\/modules\/tb_megamenu\/css\/bootstrap.css":1,"sites\/all\/modules\/tb_megamenu\/css\/base.css":1,"sites\/all\/modules\/tb_megamenu\/css\/default.css":1,"sites\/all\/modules\/tb_megamenu\/css\/compatibility.css":1,"sites\/all\/modules\/field_collection\/field_collection.theme.css":1,"sites\/all\/modules\/addtoany\/addtoany.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/neat.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-atc.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-atc_colo.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-atc_colo_dark.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-enigma.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-enigma_temp.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-fast.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-lisa_lean.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-lisa.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-lisa_blue.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-lisa_red.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-lisa_green.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-nsdi.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-osdi.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-security.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-security_colo.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-soups.css":1,"sites\/all\/themes\/custom\/neat_conference\/css\/schemes\/scheme-srecon.css":1,"sites\/all\/themes\/custom\/neat_conference\/fonts\/fontawesome\/css\/all.min.css":1}},"beautytipStyles":{"default":{"fill":"#F4F4F4","strokeStyle":"#666666","spikeLength":20,"spikeGirth":10,"width":350,"overlap":0,"centerPointY":1,"cornerRadius":0,"cssStyles":{"fontFamily":"\u0026quot;Lucida Grande\u0026quot;,Helvetica,Arial,Verdana,sans-serif","fontSize":"12px","padding":"10px 14px"},"shadow":"1","shadowColor":"rgba(0,0,0,.5)","shadowBlur":8,"shadowOffsetX":4,"shadowOffsetY":4},"plain":[],"netflix":{"positions":["right","left"],"fill":"#FFF","padding":5,"shadow":true,"shadowBlur":12,"strokeStyle":"#B9090B","spikeLength":50,"spikeGirth":60,"cornerRadius":10,"centerPointY":0.1,"overlap":-8,"cssStyles":{"fontSize":"12px","fontFamily":"arial,helvetica,sans-serif"}},"facebook":{"fill":"#F7F7F7","padding":8,"strokeStyle":"#B7B7B7","cornerRadius":0,"cssStyles":{"fontFamily":"\u0022lucida grande\u0022,tahoma,verdana,arial,sans-serif","fontSize":"11px"}},"transparent":{"fill":"rgba(0, 0, 0, .8)","padding":20,"strokeStyle":"#CC0","strokeWidth":3,"spikeLength":40,"spikeGirth":40,"cornerRadius":40,"cssStyles":{"color":"#FFF","fontWeight":"bold"}},"big-green":{"fill":"#00FF4E","padding":20,"strokeWidth":0,"spikeLength":40,"spikeGirth":40,"cornerRadius":15,"cssStyles":{"fontFamily":"\u0022lucida grande\u0022,tahoma,verdana,arial,sans-serif","fontSize":"14px"}},"google-maps":{"positions":["top","bottom"],"fill":"#FFF","padding":15,"strokeStyle":"#ABABAB","strokeWidth":1,"spikeLength":65,"spikeGirth":40,"cornerRadius":25,"centerPointX":0.9,"cssStyles":[]},"hulu":{"fill":"#F4F4F4","strokeStyle":"#666666","spikeLength":20,"spikeGirth":10,"width":350,"overlap":0,"centerPointY":1,"cornerRadius":0,"cssStyles":{"fontFamily":"\u0022Lucida Grande\u0022,Helvetica,Arial,Verdana,sans-serif","fontSize":"12px","padding":"10px 14px"},"shadow":true,"shadowColor":"rgba(0,0,0,.5)","shadowBlur":8,"shadowOffsetX":4,"shadowOffsetY":4}},"beautytips":{".beautytips":{"cssSelect":".beautytips","style":"default"}},"jcarousel":{"ajaxPath":"\/jcarousel\/ajax\/views"},"cookiebot":{"message_placeholder_cookieconsent_optout_marketing_show":false,"message_placeholder_cookieconsent_optout_marketing":"\u003Cdiv class=\u0022cookiebot cookieconsent-optout-marketing\u0022\u003E\r\n\t\u003Cdiv class=\u0022cookieconsent-optout-marketing__inner\u0022\u003E\r\n\t\tPlease \u003Ca href=\u0022!cookiebot_renew\u0022 class=\u0022cookieconsent-optout-marketing__cookiebot-renew\u0022\u003Eaccept marketing-cookies\u003C\/a\u003E to view this embedded content from \u003Ca href=\u0022!cookiebot_from_src_url\u0022 target=\u0022_blank\u0022 class=\u0022cookieconsent-optout-marketing__from-src-url\u0022\u003E!cookiebot_from_src_url\u003C\/a\u003E\t\u003C\/div\u003E\r\n\u003C\/div\u003E\r\n"},"field_group":{"html-element":"schedule"},"ogContext":{"groupType":"node","gid":"279014"}});
//--><!]]>
</script>
</head>
<body class="html not-front not-logged-in no-sidebars page-node page-node- page-node-298665 node-type-schedule context-conference context-conference og-context og-context-node og-context-node-279014 scheme-atc user-is-non-member tech-schedule-sticky-header-processed">
  <noscript aria-hidden="true"><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WQSPGJT" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <div role="document" class="page">
  <header id="site-header" class="site-header"><div class="slicknav_menu"><a href="#" aria-haspopup="true" tabindex="0" class="slicknav_btn slicknav_collapsed"><span class="slicknav_menutxt"></span><span class="slicknav_icon slicknav_no-text"><span class="slicknav_icon-bar"></span><span class="slicknav_icon-bar"></span><span class="slicknav_icon-bar"></span></span></a><ul class="slicknav_nav slicknav_hidden" aria-hidden="true" role="menu" style="display: none;">
    <li data-id="46267" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24#registration" class="dropdown-toggle" role="menuitem" tabindex="-1">
                Attend                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="4" data-hidewcol="0" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="47018" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/registration-information" role="menuitem" tabindex="-1">
                Registration Information                      </a>
          </li>

  <li data-id="47019" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/registration-discounts" role="menuitem" tabindex="-1">
                Registration Discounts                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47021" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/grants" role="menuitem" tabindex="-1">
                Grant Opportunities                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47024" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/venue-hotel-and-travel" role="menuitem" tabindex="-1">
                Venue, Hotel, and Travel                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="47027" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-1 mega dropdown active active-trail">
          <a href="/conference/atc24/glance" class="dropdown-toggle" role="menuitem" tabindex="-1">
                Program                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="4" data-hidewcol="0" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="47030" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/glance" role="menuitem" tabindex="-1">
                Program At a Glance                      </a>
          </li>

  <li data-id="47032" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega active active-trail">
          <a href="/conference/atc24/technical-sessions" role="menuitem" tabindex="-1">
                Technical Sessions                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47033" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/activities" role="menuitem" tabindex="-1">
                Activities                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="46841" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24/call-for-papers" class="dropdown-toggle" role="menuitem" tabindex="-1">
                Participate                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="4" data-hidewcol="0" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="46842" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/call-for-papers" role="menuitem" tabindex="-1">
                Call for Papers                      </a>
          </li>

  <li data-id="46843" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/submission-instructions" role="menuitem" tabindex="-1">
                Submission Instructions                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47017" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/call-for-artifacts" role="menuitem" tabindex="-1">
                Call for Artifacts                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47014" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/instructions-presenters" role="menuitem" tabindex="-1">
                Instructions for Presenters                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="46268" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24#sponsorship" class="dropdown-toggle" role="menuitem" tabindex="-1">
                Sponsors                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="" data-hidewcol="0" class="tb-megamenu-column span  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47034" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/exhibitor-services" role="menuitem" tabindex="-1">
                Exhibitor Services                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="46269" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24#marquee" class="dropdown-toggle" role="menuitem" tabindex="-1">
                About                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="6" data-hidewcol="0" class="tb-megamenu-column span6  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="46844" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24#organizers" role="menuitem" tabindex="-1">
                Conference Organizers                      </a>
          </li>

  <li data-id="46845" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="https://www.usenix.org/conferences/byname/131" role="menuitem" tabindex="-1">
                Past Conferences                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="6" data-hidewcol="" class="tb-megamenu-column span6  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-3">
    <li data-id="46846" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conferences/values-policies" role="menuitem" tabindex="-1">
                Conference Policies                      </a>
          </li>

  <li data-id="46847" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conferences/cod" role="menuitem" tabindex="-1">
                Code of Conduct                      </a>
          </li>

  <li data-id="46848" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24#about" role="menuitem" tabindex="-1">
                Questions                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>
<li class=""><a href="/user?destination=conference/atc24/technical-sessions">Sign In</a></li></ul></div>
    <div class="usenix-login-bar">
          <section class="block block-usenix-login-bar usenix-login-bar-block">

  
<div class="block-content">  <a href="/" id="site-logo" class="site-logo"><svg id="back-to-usenix" class="back-to-usenix" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 162.57 52.34"><title>usenix_logo_notag_white</title><path class="cls-1" d="M51,5.76h2.15V18.7h-1.5l-.46-1.09A9.63,9.63,0,0,1,46.58,19a4.7,4.7,0,0,1-1.85-.33,3.26,3.26,0,0,1-1.22-.83,3.75,3.75,0,0,1-.69-1.26,6.54,6.54,0,0,1-.34-1.46c0-.46-.07-1-.07-1.59V5.76h2.15v7.88q0,3.39,2.95,3.39A7.67,7.67,0,0,0,51,16Z" transform="translate(-3.49 -4.33)"></path><path class="cls-1" d="M68.56,5.51h.52l.47,0,.39,0,.38.05.31.06.33.08.28.07.31.09.29.09L71.3,7.68a11.79,11.79,0,0,0-3.09-.48,5.5,5.5,0,0,0-1.37.16,3,3,0,0,0-.92.38,2,2,0,0,0-.52.52,1.7,1.7,0,0,0-.26.52,1.73,1.73,0,0,0-.06.43,1.19,1.19,0,0,0,.56,1,4.59,4.59,0,0,0,1.39.68q.83.26,1.8.62t1.8.76A3.81,3.81,0,0,1,72,13.33,2.65,2.65,0,0,1,72.57,15a4,4,0,0,1-.14,1,3.47,3.47,0,0,1-.52,1.05,3.69,3.69,0,0,1-1,1,5.5,5.5,0,0,1-1.63.66,9.47,9.47,0,0,1-2.34.26,11.68,11.68,0,0,1-4.4-1l.6-1.62a21.61,21.61,0,0,0,2.21.71,7.12,7.12,0,0,0,1.68.21q3.2,0,3.19-2a1.13,1.13,0,0,0-.31-.79,2.3,2.3,0,0,0-.84-.55c-.35-.14-.74-.29-1.18-.45S67,13.16,66.54,13a8.33,8.33,0,0,1-1.35-.52A7.73,7.73,0,0,1,64,11.81a2.83,2.83,0,0,1-.84-1,3,3,0,0,1-.31-1.39,3.22,3.22,0,0,1,.15-1,3.45,3.45,0,0,1,.56-1,4.28,4.28,0,0,1,1-1,5.38,5.38,0,0,1,1.64-.68A9.19,9.19,0,0,1,68.56,5.51Z" transform="translate(-3.49 -4.33)"></path><path class="cls-1" d="M87.43,5.51a5.79,5.79,0,0,1,1.7.23,4.37,4.37,0,0,1,1.25.58,3.19,3.19,0,0,1,.86.92,5.56,5.56,0,0,1,.55,1.1,6.45,6.45,0,0,1,.29,1.28c.07.5.12.93.13,1.3s0,.8,0,1.3v1H83.34a3.35,3.35,0,0,0,.32,1.41,4.2,4.2,0,0,0,2.44,2.23,5.77,5.77,0,0,0,2.1.37,10.37,10.37,0,0,0,2.95-.47l.38,1.47a12.71,12.71,0,0,1-4.14.72,7.83,7.83,0,0,1-2.53-.38,5.19,5.19,0,0,1-1.78-1A5,5,0,0,1,82,16.14a6.46,6.46,0,0,1-.6-1.7,9.39,9.39,0,0,1-.16-1.79A9.42,9.42,0,0,1,81.56,10a6.44,6.44,0,0,1,1-2,6.13,6.13,0,0,1,1.41-1.38,5.66,5.66,0,0,1,1.69-.82A6.53,6.53,0,0,1,87.43,5.51ZM87,7.37A3.18,3.18,0,0,0,85,8a3.67,3.67,0,0,0-1.21,1.58,5.34,5.34,0,0,0-.38,2h6.76Q90.15,7.37,87,7.37Z" transform="translate(-3.49 -4.33)"></path><path class="cls-1" d="M101.66,5.76h1.51l.47,1.1a9.71,9.71,0,0,1,4.56-1.36,4.68,4.68,0,0,1,1.85.34,3.2,3.2,0,0,1,1.22.84,3.7,3.7,0,0,1,.69,1.27,6.78,6.78,0,0,1,.33,1.46,15,15,0,0,1,.07,1.58V18.7H110.2V10.84q0-3.4-2.93-3.4a7.66,7.66,0,0,0-3.46,1V18.7h-2.15Z" transform="translate(-3.49 -4.33)"></path><path class="cls-1" d="M122.42,5.76h2.15V18.7h-2.15Z" transform="translate(-3.49 -4.33)"></path><path class="cls-1" d="M133.14,5.76h2.49l3.65,5.13,3.52-5.13h2.29l-4.77,6.6,4.52,6.34h-2.49l-3.47-4.86-3.3,4.86H133.3l4.53-6.35Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M69.67,37.24c-.15-.39-.29-.77-.42-1.15l-.4-1.15H64.77L64,37.24H62.64q.52-1.43,1-2.64t.89-2.31q.44-1.09.86-2.08t.9-2h1.16q.47,1,.9,2t.86,2.08q.44,1.09.89,2.31t1,2.64ZM68.49,33.9q-.42-1.13-.83-2.19t-.85-2q-.45,1-.86,2t-.81,2.19Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M76.78,36.35q2,0,2-1.35a1.36,1.36,0,0,0-.18-.71,1.72,1.72,0,0,0-.47-.51,3.31,3.31,0,0,0-.68-.37l-.81-.31a8.43,8.43,0,0,1-.94-.38,3.31,3.31,0,0,1-.77-.51,2.13,2.13,0,0,1-.51-.7A2.55,2.55,0,0,1,75,28.69,3.4,3.4,0,0,1,77.24,28a5.86,5.86,0,0,1,1.49.18,3.38,3.38,0,0,1,1,.38l-.4,1a3.57,3.57,0,0,0-.81-.33,4.4,4.4,0,0,0-1.25-.16,2.8,2.8,0,0,0-.68.08,1.74,1.74,0,0,0-.55.23,1.16,1.16,0,0,0-.37.4,1.12,1.12,0,0,0-.14.57,1.2,1.2,0,0,0,.14.61,1.4,1.4,0,0,0,.4.44,3.48,3.48,0,0,0,.6.34q.34.16.76.31.58.23,1.07.47a3.5,3.5,0,0,1,.85.56,2.24,2.24,0,0,1,.55.77,2.71,2.71,0,0,1,.2,1.09,2.12,2.12,0,0,1-.86,1.82,4.05,4.05,0,0,1-2.44.64,6.11,6.11,0,0,1-1-.07,7.26,7.26,0,0,1-.8-.17,4.42,4.42,0,0,1-.6-.21L74,36.8l.38-1a5.45,5.45,0,0,0,.91.38A4.62,4.62,0,0,0,76.78,36.35Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M85.91,36.35q2,0,2-1.35a1.36,1.36,0,0,0-.17-.71,1.73,1.73,0,0,0-.47-.51,3.36,3.36,0,0,0-.68-.37l-.81-.31a8.37,8.37,0,0,1-.94-.38,3.29,3.29,0,0,1-.77-.51,2.14,2.14,0,0,1-.51-.7,2.55,2.55,0,0,1,.62-2.82A3.4,3.4,0,0,1,86.37,28a5.86,5.86,0,0,1,1.49.18,3.36,3.36,0,0,1,1,.38l-.4,1a3.59,3.59,0,0,0-.81-.33,4.41,4.41,0,0,0-1.25-.16,2.79,2.79,0,0,0-.68.08,1.75,1.75,0,0,0-.55.23,1.18,1.18,0,0,0-.37.4,1.13,1.13,0,0,0-.14.57,1.2,1.2,0,0,0,.14.61,1.39,1.39,0,0,0,.4.44,3.47,3.47,0,0,0,.6.34l.76.31q.58.23,1.07.47a3.52,3.52,0,0,1,.84.56,2.22,2.22,0,0,1,.55.77,2.72,2.72,0,0,1,.2,1.09,2.12,2.12,0,0,1-.87,1.82,4.05,4.05,0,0,1-2.44.64,6.1,6.1,0,0,1-1-.07,7.19,7.19,0,0,1-.8-.17,4.47,4.47,0,0,1-.6-.21l-.4-.19.38-1a5.48,5.48,0,0,0,.91.38A4.63,4.63,0,0,0,85.91,36.35Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M92.58,32.73a5.58,5.58,0,0,1,.34-2,4.23,4.23,0,0,1,.94-1.48,3.87,3.87,0,0,1,1.38-.89,4.66,4.66,0,0,1,1.67-.3,4.52,4.52,0,0,1,1.64.3,3.83,3.83,0,0,1,1.36.89,4.29,4.29,0,0,1,.93,1.48,6.25,6.25,0,0,1,0,4.08,4.3,4.3,0,0,1-.93,1.47,3.83,3.83,0,0,1-1.36.89,4.53,4.53,0,0,1-1.64.3,4.66,4.66,0,0,1-1.67-.3,3.87,3.87,0,0,1-1.38-.89,4.24,4.24,0,0,1-.94-1.47A5.58,5.58,0,0,1,92.58,32.73Zm1.33,0a5,5,0,0,0,.21,1.49,3.32,3.32,0,0,0,.59,1.14,2.59,2.59,0,0,0,.94.72,3,3,0,0,0,1.24.25,2.92,2.92,0,0,0,1.24-.25,2.61,2.61,0,0,0,.93-.72,3.36,3.36,0,0,0,.59-1.14,5.43,5.43,0,0,0,0-3A3.34,3.34,0,0,0,99,30.11a2.6,2.6,0,0,0-.93-.72,2.91,2.91,0,0,0-1.24-.25,2.94,2.94,0,0,0-1.24.25,2.57,2.57,0,0,0-.94.72,3.31,3.31,0,0,0-.59,1.14A5,5,0,0,0,93.9,32.73Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M109.14,37.44a4.59,4.59,0,0,1-1.73-.31,3.65,3.65,0,0,1-1.34-.91,4.13,4.13,0,0,1-.86-1.47,6.08,6.08,0,0,1-.3-2,5.47,5.47,0,0,1,.34-2,4.38,4.38,0,0,1,.93-1.48,3.94,3.94,0,0,1,1.37-.91,4.44,4.44,0,0,1,1.66-.31,5.82,5.82,0,0,1,1,.08,6.12,6.12,0,0,1,.78.18,3.76,3.76,0,0,1,.54.21l.29.16-.36,1a2,2,0,0,0-.34-.17l-.5-.18a4.53,4.53,0,0,0-.61-.15,3.82,3.82,0,0,0-.67-.06,3.22,3.22,0,0,0-1.29.25,2.65,2.65,0,0,0-1,.71,3.23,3.23,0,0,0-.62,1.14,4.87,4.87,0,0,0-.21,1.5,5.4,5.4,0,0,0,.19,1.47,3.24,3.24,0,0,0,.57,1.14,2.54,2.54,0,0,0,.94.73,3.13,3.13,0,0,0,1.32.26,4.84,4.84,0,0,0,1.44-.18,6.24,6.24,0,0,0,.86-.32l.33,1a1.81,1.81,0,0,1-.33.16,4.21,4.21,0,0,1-.58.19,7.28,7.28,0,0,1-.82.16A7.45,7.45,0,0,1,109.14,37.44Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M115.54,28.24h1.26v9h-1.26Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M127.26,37.24c-.15-.39-.29-.77-.42-1.15l-.4-1.15h-4.08l-.82,2.3h-1.31q.52-1.43,1-2.64t.89-2.31c.29-.73.58-1.42.86-2.08s.59-1.32.9-2H125q.47,1,.9,2t.87,2.08q.44,1.09.89,2.31t1,2.64Zm-1.18-3.34q-.41-1.13-.83-2.19t-.85-2q-.45,1-.87,2t-.81,2.19Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M137.26,28.24v1.09h-2.83v7.92h-1.26V29.33h-2.83V28.24Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M140.8,28.24h1.26v9H140.8Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M146.14,32.73a5.6,5.6,0,0,1,.34-2,4.21,4.21,0,0,1,.94-1.48,3.86,3.86,0,0,1,1.38-.89,4.66,4.66,0,0,1,1.67-.3,4.53,4.53,0,0,1,1.65.3,3.84,3.84,0,0,1,1.36.89,4.26,4.26,0,0,1,.93,1.48,6.21,6.21,0,0,1,0,4.08,4.27,4.27,0,0,1-.93,1.47,3.85,3.85,0,0,1-1.36.89,4.53,4.53,0,0,1-1.65.3,4.67,4.67,0,0,1-1.67-.3,3.86,3.86,0,0,1-1.38-.89,4.22,4.22,0,0,1-.94-1.47A5.6,5.6,0,0,1,146.14,32.73Zm1.33,0a5,5,0,0,0,.21,1.49,3.37,3.37,0,0,0,.59,1.14,2.62,2.62,0,0,0,.94.72,3,3,0,0,0,1.24.25,2.91,2.91,0,0,0,1.23-.25,2.6,2.6,0,0,0,.93-.72,3.32,3.32,0,0,0,.59-1.14,5.43,5.43,0,0,0,0-3,3.31,3.31,0,0,0-.59-1.14,2.58,2.58,0,0,0-.93-.72,2.9,2.9,0,0,0-1.23-.25,3,3,0,0,0-1.24.25,2.61,2.61,0,0,0-.94.72,3.36,3.36,0,0,0-.59,1.14A5,5,0,0,0,147.47,32.73Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M165,37.24l-.66-1.08q-.37-.6-.8-1.24l-.89-1.31c-.31-.44-.61-.87-.92-1.28s-.6-.8-.88-1.16-.55-.68-.79-1v7h-1.24v-9h1q.61.65,1.31,1.51t1.38,1.78q.68.92,1.29,1.8t1,1.57V28.24h1.24v9Z" transform="translate(-3.49 -4.33)"></path><g class="cls-3"><g class="cls-3"><path class="cls-2" d="M13.17,36.8a48.25,48.25,0,0,0,4.95,2.13,32.82,32.82,0,0,1,1.2-4.83c-2.27-1-2.67-1.25-4.88-2.42A35.56,35.56,0,0,0,13.17,36.8ZM32.35,8A29.28,29.28,0,0,0,27,4.33a36.84,36.84,0,0,0-4.35,3A41.79,41.79,0,0,1,28,11,36.51,36.51,0,0,1,32.35,8ZM23.71,15c-.82-.59-1.93-1.36-2.77-1.93A49.91,49.91,0,0,0,17,18L14.4,16.29a43.54,43.54,0,0,1,8.23-9,20.77,20.77,0,0,0-4.54-1.88,42.13,42.13,0,0,0-4,3.79c.58.25,1.33.6,1.89.87a47.94,47.94,0,0,0-3.8,4.74l-1.73-1.08C7.95,17.43.19,30.2,5.07,41.21l0,0h0l.13.23c2.24,3.88,5.84,5.09,9.65,4.82l.32,0a27,27,0,0,1-.1-7.92,38.56,38.56,0,0,1-4.15-2.1,38.28,38.28,0,0,1,1.64-6.54,47,47,0,0,1-4-2.76c.38-1,.93-2.37,1.35-3.38,1.68,1.22,2.08,1.48,4,2.73-.43,1-1,2.38-1.36,3.41,1.56.94,1.85,1.1,4.1,2.3a46.4,46.4,0,0,1,2.93-6.54c-1.67-1-1.89-1.16-4-2.51.56-1,1.34-2.26,1.94-3.21l3.95,2.6a43,43,0,0,1,8.65-9.65c-.62-.53-1.48-1.23-2.12-1.75A45.3,45.3,0,0,0,23.71,15Z" transform="translate(-3.49 -4.33)"></path><path class="cls-2" d="M50.57,32.9s-2.4-2.08-9.83-5.94c-2.38-.92-3.08.42-4.47,3.07C29.22,43.56,10.85,54.52,4.06,42.1c.49.88,1.16,2,1.7,2.89a33.94,33.94,0,0,0,4.86,5.07c1,.85,2.45,1.94,3.55,2.71A27.08,27.08,0,0,0,18.31,55c1.17.46,2.75,1,4,1.39,9.72,2,21.13-6.1,28.39-20,.82-2,.21-3.08-.09-3.44" transform="translate(-3.49 -4.33)"></path></g></g></svg>
</a><div class="item-list"><ul id="usenix-login-bar-links" class="usenix-login-bar-links"><li class="login first"><a href="/user?destination=conference/atc24/technical-sessions">Sign In</a></li>
<li class="conference last"><a href="/conferences">Conferences</a></li>
</ul></div></div>
</section>    </div>
    <div class="usenix-announcement-banner-region">
          </div>
    <div class="outer-wrapper">
      <div class="header-col2-wrapper">
            <section class="block block-usenix-event-registration register">

  
<div class="block-content">  <a href="/conference/279014/registration/form" class="register-button">Register Now</a></div>
</section>      </div>
      <div class="header-col1-wrapper">
            <section class="block block-usenix-og-auto-menu usenix-og-auto-header-menu-logo">

  
<div class="block-content">  <a href="/conference/atc24" class="header-menu-logo-link"><img class="header-menu-logo" src="https://www.usenix.org/sites/default/files/styles/neat_conference_menu_logo/public/atc24_logo_wordmark_white_500.png?itok=TdwDjq7h" width="84" height="42" alt=""></a></div>
</section>  <section class="block block-usenix-og-auto-menu usenix-og-auto-header-menu">

  
<div class="block-content">  <div class="tb-megamenu tb-megamenu-menu-og-279014 tb-megamenu-processed" role="navigation" aria-label="Main navigation">
      <button data-target=".nav-collapse" data-toggle="collapse" class="btn btn-navbar tb-megamenu-button menuIstance-processed" type="button">
      <i class="fa fa-reorder"></i>
    </button>
    <div class="nav-collapse  always-show">
    <ul class="tb-megamenu-nav nav level-0 items-5">
    <li data-id="46267" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24#registration" class="dropdown-toggle">
                Attend                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="4" data-hidewcol="0" id="tb-megamenu-column-1" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="47018" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/registration-information">
                Registration Information                      </a>
          </li>

  <li data-id="47019" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/registration-discounts">
                Registration Discounts                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" id="tb-megamenu-column-2" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47021" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/grants">
                Grant Opportunities                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" id="tb-megamenu-column-3" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47024" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/venue-hotel-and-travel">
                Venue, Hotel, and Travel                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="47027" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-1 mega dropdown active active-trail">
          <a href="/conference/atc24/glance" class="dropdown-toggle">
                Program                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="4" data-hidewcol="0" id="tb-megamenu-column-4" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="47030" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/glance">
                Program At a Glance                      </a>
          </li>

  <li data-id="47032" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega active active-trail">
          <a href="/conference/atc24/technical-sessions">
                Technical Sessions                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" id="tb-megamenu-column-5" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47033" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/activities">
                Activities                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="46841" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24/call-for-papers" class="dropdown-toggle">
                Participate                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="4" data-hidewcol="0" id="tb-megamenu-column-6" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="46842" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/call-for-papers">
                Call for Papers                      </a>
          </li>

  <li data-id="46843" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/submission-instructions">
                Submission Instructions                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" id="tb-megamenu-column-7" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47017" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/call-for-artifacts">
                Call for Artifacts                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="4" data-hidewcol="" id="tb-megamenu-column-8" class="tb-megamenu-column span4  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47014" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/instructions-presenters">
                Instructions for Presenters                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="46268" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24#sponsorship" class="dropdown-toggle">
                Sponsors                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="" data-hidewcol="0" id="tb-megamenu-column-9" class="tb-megamenu-column span  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-1">
    <li data-id="47034" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24/exhibitor-services">
                Exhibitor Services                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>

  <li data-id="46269" data-level="1" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-1 mega dropdown">
          <a href="/conference/atc24#marquee" class="dropdown-toggle">
                About                      </a>
        <div data-class="" data-width="" class="tb-megamenu-submenu dropdown-menu mega-dropdown-menu nav-child">
  <div class="mega-dropdown-inner">
    <div class="tb-megamenu-row row-fluid">
  <div data-class="" data-width="6" data-hidewcol="0" id="tb-megamenu-column-10" class="tb-megamenu-column span6  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-2">
    <li data-id="46844" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24#organizers">
                Conference Organizers                      </a>
          </li>

  <li data-id="46845" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="https://www.usenix.org/conferences/byname/131">
                Past Conferences                      </a>
          </li>
</ul>
  </div>
</div>

<div data-class="" data-width="6" data-hidewcol="" id="tb-megamenu-column-11" class="tb-megamenu-column span6  mega-col-nav">
  <div class="tb-megamenu-column-inner mega-inner clearfix">
        <ul class="tb-megamenu-subnav mega-nav level-1 items-3">
    <li data-id="46846" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conferences/values-policies">
                Conference Policies                      </a>
          </li>

  <li data-id="46847" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conferences/cod">
                Code of Conduct                      </a>
          </li>

  <li data-id="46848" data-level="2" data-type="menu_item" data-class="" data-xicon="" data-caption="" data-alignsub="" data-group="0" data-hidewcol="0" data-hidesub="0" header_hide="0" footer_hide="0" id="" style="" class="tb-megamenu-item level-2 mega">
          <a href="/conference/atc24#about">
                Questions                      </a>
          </li>
</ul>
  </div>
</div>
</div>
  </div>
</div>
  </li>
</ul>
      </div>
  </div>
</div>
</section>      </div>
    </div>
  </header>
  
  <main role="main">

    <section id="content">
              <div class="outer-wrapper">
                      <h1 id="page-title">USENIX ATC '24 Technical Sessions</h1>
                  </div>
      
              <div class="outer-wrapper">
                            </div>
      
      
        
  
<div class="block-content">  <article id="node-298665" class="node node-schedule view-mode-full">

        
  
  <div class="content">
    <div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item odd"><!--
<p><strong>All sessions will be held in [INSERT ROOM] unless otherwise noted.</strong></p>

<p>Papers are available for download below to registered attendees now and to everyone beginning [INSERT DATE]. Paper abstracts are available to everyone now. Copyright to the individual works is retained by the author[s]. </p>
-->
</div></div></div><div class="required-fields group-tech-schedule-header-wrap field-group-html-element"><div class="required-fields group-tech-schedule-header field-group-html-element"><div class="tech-schedule-display-switcher"><strong>Display:</strong>
<ul class="links"><li class="0 first"><a href="#switcher" class="tech-schedule-display-switcher-link active display-mode-processed" data-switcher_type="column">Column</a></li>
<li class="1 last"><a href="#switcher" class="tech-schedule-display-switcher-link  display-mode-processed" data-switcher_type="list">List</a></li>
</ul></div><div class="tech-schedule-view-mode-switcher"><strong>View mode:</strong>
<ul class="links"><li class="condensed first"><a href="#switcher" class="tech-schedule-view-mode-switcher-link " data-switcher_type="condensed">condensed</a></li>
<li class="standard"><a href="#switcher" class="tech-schedule-view-mode-switcher-link active" data-switcher_type="standard">Standard</a></li>
<li class="expanded last"><a href="#switcher" class="tech-schedule-view-mode-switcher-link " data-switcher_type="expanded">Expanded</a></li>
</ul></div></div></div>
<div class="paragraphs-items paragraphs-items-field-paragraphs paragraphs-items-field-paragraphs-full paragraphs-items-full">
  <div class="field field-name-field-paragraphs field-type-paragraphs field-label-hidden"><div class="field-items"><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h2 class="field field-name-field-date-text field-type-text field-label-hidden"><span class="field-item odd first last">Wednesday, July 10</span></h2><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">8:00 am–9:00 am</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="mon"></a></div></div></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Continental Breakfast</h2></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">9:00 am–10:00 am</span></h3></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298666" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">USENIX ATC '24 and OSDI '24 Joint Keynote Address</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298776" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wu-joint-keynote">Scaling AI Sustainably: An Uncharted Territory</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-presented-by field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Carole-Jean Wu, <em>Meta</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The past 50 years has seen a dramatic increase in the amount of compute per person, in particular, those enabled by AI. Despite the positive societal benefits, AI technologies come with significant environmental implications. I will talk about the scaling trend and the operational carbon footprint of AI computing by examining the model development cycle, spanning data, algorithms, and system hardware. At the same time, we will consider the life cycle of system hardware from the perspective of hardware architectures and manufacturing technologies. I will highlight key efficiency optimization opportunities for cutting-edge AI technologies, from deep learning recommendation models to multi-modal generative AI tasks. To scale AI sustainably, we need to make AI and computing more broadly efficient and flexible. We must also go beyond efficiency and optimize across the life cycle of computing infrastructures, from hardware manufacturing to datacenter operation and end-of-life processing for the hardware. Based on the industry experience and lessons learned, my talk will conclude with important development and research directions to advance the field of computing in an environmentally responsible and sustainable manner.</p></div><div class="field field-name-field-paper-people field-type-node-reference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298774" class="node node-speaker view-mode-schedule">

                    <h2 class="node-title">
          <a href="/conference/atc24/speaker-or-organizer/carole-jean-wu-meta">Carole-Jean Wu, Meta</a></h2>
            
  
  <div class="content">
    <div class="field field-name-field-speakers-photo field-type-image field-label-hidden"><div class="field-items"><div class="field-item odd"><img src="https://www.usenix.org/sites/default/files/styles/speaker_photo/public/wu_carole_jean_200x230.jpg?itok=IAQq4tpA" width="100" height="115" alt=""></div></div></div><div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Carole-Jean Wu is a Director at Meta. She is a founding member and a Vice President of MLCommons—a non-profit organization that aims to accelerate machine learning for the benefit of all. Dr. Wu also serves on the MLCommons Board as a Director, chaired the MLPerf Recommendation Benchmark Advisory Board, and co-chaired for MLPerf Inference. Prior to Meta/Facebook, She was a tenured professor at ASU. She earned her M.A. and Ph.D. from Princeton and B.Sc. from Cornell.</p>

<p>Dr. Wu's expertise sits at the intersection of computer architecture and machine learning. Her work spans across datacenter infrastructures and edge systems, such as developing energy- and memory-efficient systems and microarchitectures, optimizing systems for machine learning execution at-scale, and designing learning-based approaches for system design and optimization. Dr. Wu's work has been recognized with several awards, including IEEE Micro Top Picks and ACM/IEEE Best Paper Awards. She was the Program Co-Chair of the Conference on Machine Learning and Systems (MLSys) in 2022, the Program Chair of the IEEE International Symposium on Workload Characterization (IISWC) in 2018, and the Editor for the IEEE MICRO Special Issue on Environmentally Sustainable Computing. She currently serves on the ACM SIGARCH/SIGMICRO CARES committee.</p></div></div></div>  </div>

        
    
</article>
</div></div></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">10:00 am–10:30 am</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Break with Refreshments</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">10:30 am–10:45 am</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Opening Remarks, Awards, and Presentation of the 2024 USENIX Lifetime Achievement (Flame) Award</h2>
<p>Program Co-Chairs: Saurabh Bagchi, <em>Purdue University;</em> Yiying Zhang, <em>University of California, San Diego</em></p></div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">10:45 am–12:25 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="wedam"></a></div></div></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298643" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Cloud Computing</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298489" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/liu-qingyuan">Harmonizing Efficiency and Practicability: Optimizing Resource Utilization in Serverless Computing with Jiagu</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Qingyuan Liu, Yanning Yang, Dong Du, and Yubin Xia, <em>Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education;</em> Ping Zhang and Jia Feng, <em>Huawei Cloud;</em> James R. Larus, <em>EPFL;</em> Haibo Chen, <em>Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education; Key Laboratory of System Software (Chinese Academy of Science)</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Current serverless platforms struggle to optimize resource utilization due to their dynamic and fine-grained nature. Conventional techniques like overcommitment and autoscaling fall short, often sacrificing utilization for practicability or incurring performance trade-offs. Overcommitment requires predicting performance to prevent QoS violation, introducing trade-off between prediction accuracy and overheads. Autoscaling requires scaling instances in response to load fluctuations quickly to reduce resource wastage, but more frequent scaling also leads to more cold start overheads. This paper introduces Jiagu to harmonize efficiency with practicability through two novel techniques. First, <em>pre-decision scheduling</em> achieves accurate prediction while eliminating overheads by decoupling prediction and scheduling. Second, \emph{dual-staged scaling} achieves frequent adjustment of instances with minimum overhead. We have implemented a prototype and evaluated it using real-world applications and traces from the public cloud platform. Our evaluation shows a 54.8% improvement in deployment density over commercial clouds (with Kubernetes) while maintaining QoS, and 81.0%–93.7% lower scheduling costs and a 57.4%–69.3% reduction in cold start latency compared to existing QoS-aware schedulers.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298491" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/fu">ALPS: An Adaptive Learning, Priority OS Scheduler for Serverless Functions</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yuqi Fu, <em>University of Virginia;</em> Ruizhe Shi, <em>George Mason University;</em> Haoliang Wang, <em>Adobe Research;</em> Songqing Chen, <em>George Mason University;</em> Yue Cheng, <em>University of Virginia</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>FaaS (Function-as-a-Service) workloads feature unique patterns. Serverless functions are ephemeral, highly concurrent, and bursty, with an execution duration ranging from a few milliseconds to a few seconds. The workload behaviors pose new challenges to kernel scheduling. Linux CFS (Completely Fair Scheduler) is workload-oblivious and optimizes long-term fairness via proportional sharing. CFS neglects the short-term demands of CPU time from short-lived serverless functions, severely impacting the performance of short functions. Preemptive shortest job first—shortest remaining process time (SRPT)—prioritizes shorter functions in order to satisfy their short-term demands of CPU time and, therefore, serves as a best-case baseline for optimizing the turnaround time of short functions. A significant downside of approximating SRPT, however, is that longer functions might be starved.</p>

<p>In this paper, we propose a novel application-aware kernel scheduler, ALPS (Adaptive Learning, Priority Scheduler), based on two key insights. First, approximating SRPT can largely benefit short functions but may inevitably penalize long functions. Second, CFS provides necessary infrastructure support to implement user-defined priority scheduling. To this end, we design ALPS to have a novel, decoupled scheduler frontend and backend architecture, which unifies approximate SRPT and proportional-share scheduling. ALPS’ frontend sits in the user space and approximates SRPT-inspired priority scheduling by adaptively learning from an SRPT simulation on a recent past workload. ALPS’ backend uses eBPF functions hooked to CFS to carry out the continuously learned policies sent from the frontend to inform scheduling decisions in the kernel. This design adds workload intelligence to workload-oblivious OS scheduling while retaining the desirable properties of OS schedulers. We evaluate ALPS extensively using two production FaaS workloads (Huawei and Azure), and results show that ALPS achieves a reduction of 57.2% in average function execution duration compared to CFS.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298493" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/luo">Starburst: A Cost-aware Scheduler for Hybrid Cloud</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Michael Luo, Siyuan Zhuang, Suryaprakash Vengadesan, and Romil Bhardwaj, <em>UC Berkeley;</em> Justin Chang, <em>UC Santa Barbara;</em> Eric Friedman, Scott Shenker, and Ion Stoica, <em>UC Berkeley</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>To efficiently tackle bursts in job demand, organizations employ hybrid cloud architectures to scale their batch workloads from their private clusters to public cloud. This requires transforming cluster schedulers into cloud-enabled versions to navigate the tradeoff between cloud costs and scheduler objectives such as job completion time (JCT). However, our analysis over production-level traces show that existing cloud-enabled schedulers incur inefficient cost-JCT trade-offs due to low cluster utilization. </p>

<p>We present Starburst, a system that maximizes cluster utilization to streamline the cost-JCT tradeoff. Starburst's scheduler dynamically controls jobs' waiting times to improve utilization—it assigns longer waits for large jobs to increase their chances of running on the cluster, and shorter waits to small jobs to increase their chances of running on the cloud. To offer configurability, Starburst provides system administrators a simple <em>waiting budget</em> framework to tune their position on the cost-JCT curve. A departure from traditional cluster schedulers, Starburst operates as a higher-level resource manager over a private cluster and dynamic cloud clusters. Simulations over production-level traces and real-world experiments on a 32-GPU private cluster show that Starburst can reduce cloud costs by up to 54-91% over existing cluster managers, while increasing average JCT by at most 5.8%.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298495" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wu-hao">StreamBox: A Lightweight GPU SandBox for Serverless Inference Workflow</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Hao Wu, Yue Yu, and Junxiao Deng, <em>Huazhong University of Science and Technology;</em> Shadi Ibrahim, <em>Inria;</em> Song Wu and Hao Fan, <em>Huazhong University of Science and Technology and Jinyinhu Laboratory;</em> Ziyue Cheng, <em>Huazhong University of Science and Technology;</em> Hai Jin, <em>Huazhong University of Science and Technology and Jinyinhu Laboratory</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The dynamic workload and latency sensitivity of DNN inference drive a trend toward exploiting serverless computing for scalable DNN inference serving. Usually, GPUs are spatially partitioned to serve multiple co-located functions. However, existing serverless inference systems isolate functions in separate monolithic GPU runtimes (e.g., CUDA context), which is too heavy for short-lived and fine-grained functions, leading to a high startup latency, a large memory footprint, and expensive inter-function communication. In this paper, we present StreamBox, a new lightweight GPU sandbox for serverless inference workflow. StreamBox unleashes the potential of streams and efficiently realizes them for serverless inference by implementing fine-grain and auto-scaling memory management, allowing transparent and efficient intra-GPU communication across functions, and enabling PCIe bandwidth sharing among concurrent streams. Our evaluations over real-world workloads show that StreamBox reduces the GPU memory footprint by up to 82% and improves throughput by 6.7X compared to state-of-the-art serverless inference systems.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298644" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">ML Inference</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298498" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/qiu">Power-aware Deep Learning Model Serving with μ-Serve</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Haoran Qiu, Weichao Mao, Archit Patke, and Shengkun Cui, <em>University of Illinois Urbana-Champaign;</em> Saurabh Jha, Chen Wang, and Hubertus Franke, <em>IBM Research;</em> Zbigniew Kalbarczyk, Tamer Başar, and Ravishankar K. Iyer, <em>University of Illinois Urbana-Champaign</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling. We explore the co-design space and present a novel power-aware model-serving system, µ-Serve. µ-Serve is a model-serving framework that optimizes the power consumption and model serving latency/throughput of serving multiple ML models efficiently in a homogeneous GPU cluster. Evaluation results on production workloads show that µ-Serve achieves 1.2–2.6× power saving by dynamic GPU frequency scaling (up to 61% reduction) without SLO attainment violations.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298500" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/jiang">Fast Inference for Probabilistic Graphical Models</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Jiantong Jiang, <em>The University of Western Australia;</em> Zeyi Wen, <em>HKUST (Guangzhou) and HKUST;</em> Atif Mansoor and Ajmal Mian, <em>The University of Western Australia</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Probabilistic graphical models (PGMs) have attracted much attention due to their firm theoretical foundation and inherent interpretability. However, existing PGM inference systems are inefficient and lack sufficient generality, due to issues with irregular memory accesses, high computational complexity, and modular design limitation. In this paper, we present Fast-PGM, a fast and parallel PGM inference system for importance sampling-based approximate inference algorithms. Fast-PGM incorporates careful memory management techniques to reduce memory consumption and enhance data locality. It also employs computation and parallelization optimizations to reduce computational complexity and improve the overall efficiency. Furthermore, Fast-PGM offers high generality and flexibility, allowing easy integration with all the mainstream importance sampling-based algorithms. The system abstraction of Fast-PGM facilitates easy optimizations, extensions, and customization for users. Extensive experiments show that Fast-PGM achieves 3 to 20 times speedup over the state-of-the-art implementation. Fast-PGM source code is freely available at <a href="https://github.com/jjiantong/FastPGM">https://github.com/jjiantong/FastPGM</a>.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298502" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/gao-bin-cost">Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Bin Gao, <em>National University of Singapore;</em> Zhuomin He, <em>Shanghai Jiaotong University;</em> Puru Sharma, Qingxuan Kang, and Djordje Jevdjic, <em>National University of Singapore;</em> Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo, <em>Huawei Cloud</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8× for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298504" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/lei">PUZZLE: Efficiently Aligning Large Language Models through Light-Weight Context Switch</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Kinman Lei, Yuyang Jin, Mingshu Zhai, Kezhao Huang, Haoxing Ye, and Jidong Zhai, <em>Tsinghua University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Aligning Large Language Models (LLMs) is currently the primary method to ensure AI systems operate in an ethically responsible and socially beneficial manner. Its paradigm differs significantly from standard pre-training or fine-tuning processes, involving multiple <em>models and workloads</em> (context), and necessitates frequently switching execution, introducing significant overhead, such as parameter updates and data transfer, which poses a critical challenge: efficiently switching between different models and workloads.</p>

<p>To address these challenges, we introduce PUZZLE, an efficient system for LLM alignment. We explore model orchestration as well as light-weight and smooth workload switching in aligning LLMs by considering the similarity between different workloads. Specifically, PUZZLE adopts a two-dimensional approach for efficient switching, focusing on both intra- and inter-stage switching. Within each stage, switching costs are minimized by exploring model affinities and overlapping computation via time-sharing. Furthermore, a similarity-oriented strategy is employed to find the optimal inter-stage switch plan with the minimum communication cost. We evaluate PUZZLE on various clusters with up to 32 GPUs. Results show that PUZZLE achieves up to 2.12× speedup compared with the state-of-the-art RLHF training system DeepSpeed-Chat.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">12:25 pm–2:00 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="luncheon-wed"></a></div></div></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Conference Luncheon</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">2:00 pm–3:40 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="wedpm"></a></div></div></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298645" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Storage 1</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298506" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/yi-shushu">ScalaAFA: Constructing User-Space All-Flash Array Engine with Holistic Designs</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Shushu Yi, <em>Peking University and Zhongguancun Laboratory;</em> Xiurui Pan, <em>Peking University;</em> Qiao Li, <em>Xiamen University;</em> Qiang Li, <em>Alibaba;</em> Chenxi Wang, <em>University of Chinese Academy of Sciences;</em> Bo Mao, <em>Xiamen University;</em> Myoungsoo Jung, <em>KAIST and Panmnesia;</em> Jie Zhang, <em>Peking University and Zhongguancun Laboratory</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>All-flash array (AFA) is a popular approach to aggregate the capacity of multiple solid-state drives (SSDs) while guaranteeing fault tolerance. Unfortunately, existing AFA engines inflict substantial software overheads on the I/O path, such as the user-kernel context switches and AFA internal tasks (e.g., parity preparation), thereby failing to adopt next-generation high-performance SSDs. </p>

<p>Tackling this challenge, we propose ScalaAFA, a unique holistic design of AFA engine that can extend the throughput of next-generation SSD arrays in scale with low CPU costs. We incorporate ScalaAFA into user space to avoid user-kernel context switches while harnessing SSD built-in resources for handling AFA internal tasks. Specifically, in adherence to the lock-free principle of existing user-space storage framework, ScalaAFA substitutes the traditional locks with an efficient message-passing-based permission management scheme to facilitate inter-thread synchronization. Considering the CPU burden imposed by background I/O and parity computation, ScalaAFA proposes to offload these tasks to SSDs. To mitigate host-SSD communication overheads in offloading, ScalaAFA takes a novel data placement policy that enables transparent data gathering and in-situ parity computation. ScalaAFA also addresses two AFA intrinsic issues, metadata persistence and write amplification, by thoroughly exploiting SSD architectural innovations. Comprehensive evaluation results indicate that ScalaAFA can achieve 2.5× write throughput and reduce average write latency by a significant 52.7%, compared to the state-of-the-art AFA engines.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298508" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/shirwadkar">FastCommit: resource-efficient, performant and cost-effective file system journaling</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Harshad Shirwadkar, Saurabh Kadekodi, and Theodore Tso, <em>Google</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>JBD2, the current physical journaling mechanism in Ext4 is bulky and resource-hungry. Specifically, in case of metadata-heavy workloads, fsyncs issued by applications cause JBD2 to write copies of changed metadata blocks, incurring high byte and IO overhead. When storing data in Ext4 via NFS (a popular setup), the NFS protocol issues fsyncs for every file metadata update which further exacerbates the problem. In a simple multi-threaded mail-server workload, JBD2 consumed approximately 76% of the disk’s write bandwidth. Higher byte and IO utilization of JBD2 results in reduced application throughput, higher wear-out of flash based media and increased performance provisioning costs in cloud-based storage services.</p>

<p>We present FastCommit: a hybrid journaling approach for Ext4 which performs logical journaling for simple and frequent file system modifications, while relying on JBD2 for more complex and rare modifications. Key design elements of FastCommit are <em>compact logging</em>, <em>selective flushing</em> and <em>inline journaling</em>. The first two techniques work together to ensure that over 80% commits are contained within a single 4KB block and are written to disk without requiring an expensive cache flush operation. Inline journaling minimizes context switching delays. With faster and efficient fsyncs, FastCommit reduces throughput interference of JBD2 by over 2× along with throughput improvements of up to 120%. We implemented FastCommit in Ext4 and successfully merged our code to the upstream Linux kernel.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298510" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/hwang">ZMS: Zone Abstraction for Mobile Flash Storage</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Joo-Young Hwang, Seokhwan Kim, Daejun Park, Yong-Gil Song, Junyoung Han, Seunghyun Choi, and Sangyeun Cho, <em>Samsung Electronics;</em> Youjip Won, <em>Korea Advanced Institute of Science and Technology</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>We propose an I/O stack for ZNS based flash storage in mobile environment, ZMS. The zone interface is known to save the flash storage from two fundamental issues which modern flash storage suffers from: logical-to-physical mapping table size and garbage collection overhead. Through extensive study, we find that realizing the zone interface in mobile environment is more than a challenge due to the unique characteristics of mobile environment: the lack of on-device memory in mobile flash storage and the frequent fsync() calls in mobile applications. Aligned with this, we identify the root causes that need to be addressed in realizing the zone interface in mobile I/O stack: <em>write buffer thrashing and tiny synchronous file update</em>. We develop a filesystem, block I/O layer, and device firmware techniques to address the above mentioned two issues. The three key techniques in ZMS are (i) IOTailor, (ii) budget-based in-place update, and (iii) multi-granularity logical-to-physical mapping. Evaluation on a real production platform shows that ZMS improves write amplification by 2.9–6.4× and random write performance by 5.0–13.6×. With the three techniques, ZMS shows significant performance improvement in writing to the multiple zones concurrently, executing SQLite transactions, and launching the applications.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298512" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/cai">Ethane: An Asymmetric File System for Disaggregated Persistent Memory</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Miao Cai, <em>College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics;</em> Junru Shen, <em>College of Computer Science and Software Engineering, Hohai University;</em> Baoliu Ye, <em>State Key Laboratory for Novel Software Technology, Nanjing University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The ultra-fast persistent memories (PMs) promise a practical solution towards high-performance distributed file systems. This paper examines and reveals a cascade of three performance and cost issues in the current PM provision scheme, namely expensive cross-node interaction, weak single-node capability, and costly scale-out performance, which not only underutilizes fast PM devices but also magnifies its limited storage capacity and high price deficiencies. To remedy this, we introduce Ethane, a file system built on disaggregated persistent memory (DPM). Through resource separation using fast connectivity technologies, DPM achieves efficient and cost-effective PM sharing while retaining low-latency memory access. To unleash such hardware potentials, Ethane incorporates an asymmetric file system architecture inspired by the imbalanced resource provision feature of DPM. It splits a file system into a control-plane FS and a data-plane FS and designs these two planes to make the best use of the respective hardware resources. Evaluation results demonstrate that Ethane reaps the DPM hardware benefits, performs up to 68× better than modern distributed file systems, and improves data-intensive application throughputs by up to 17×.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298646" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Networks 1</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298514" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/lee">PeRF: Preemption-enabled RDMA Framework</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Sugi Lee and Mingyu Choi, <em>Acryl Inc.;</em> Ikjun Yeom, <em>Acryl Inc. and Sungkyunkwan University;</em> Younghoon Kim, <em>Sungkyunkwan University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Remote Direct Memory Access (RDMA) provides high throughput, low latency, and minimal CPU usage for data-intensive applications. However, RDMA was initially designed for single-tenant use, and its application in a multi-tenant cloud environment poses challenges in terms of performance isolation, security, and scalability. This paper proposes a Preemption-enabled RDMA Framework (PeRF), which offers software-based performance isolation for efficient multi-tenancy in RDMA. PeRF leverages a novel RNIC preemption mechanism to dynamically control RDMA resource utilization for each tenant, while ensuring that RNICs remain busy, thereby enabling work conservation. PeRF outperforms existing approaches by achieving flexible performance isolation without compromising RDMA's bare-metal performance.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298516" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xu-tingting">CyberStar: Simple, Elastic and Cost-Effective Network Functions Management in Cloud Network at Scale</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Tingting Xu, <em>Nanjing University;</em> Bengbeng Xue, Yang Song, Xiaomin Wu, Xiaoxin Peng, and Yilong Lyu, <em>Alibaba Group;</em> Xiaoliang Wang, Chen Tian, Baoliu Ye, and Camtu Nguyen, <em>Nanjing University;</em> Biao Lyu and Rong Wen, <em>Alibaba Group;</em> Zhigang Zong, <em>Alibaba Group and Zhejiang University;</em> Shunmin Zhu, <em>Alibaba Group and Tsinghua University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Network functions (NFs) facilitate network operations and have become a critical service offered by cloud providers. One of the key challenges is how to meet the elastic requirements of massive traffic and diverse NF requests of tenants. This paper identifies the opportunity by leveraging cloud elastic compute services (ECS), i.e. containers or virtual machines, to provide the cloud-scale network function services, CyberStar. CyberStar introduces two key designs: (i) resource pooling based on a newly proposed three-tier architecture for scalable network functions; and (ii) on-demand resource assignment while maintaining high resource utilization in terms of both tenant demands and operation cost. Compared to the traditional NFs constructed over bare-metal servers, CyberStar can achieve 100Gbps bandwidth (6.7×) and scale to millions of connections within one second (20×).</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298518" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/khalilov">OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Mikhail Khalilov, Marcin Chrapek, Siyuan Shen, Alessandro Vezzu, Thomas Benz, Salvatore Di Girolamo, and Timo Schneider, <em>ETH Zürich;</em> Daniele De Sensi, <em>ETH Zürich and Sapienza University of Rome;</em> Luca Benini and Torsten Hoefler, <em>ETH Zürich</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Multi-tenancy is essential for unleashing SmartNIC's potential in datacenters. Our systematic analysis in this work shows that existing on-path SmartNICs have resource multiplexing limitations. For example, existing solutions lack multi-tenancy capabilities such as performance isolation and QoS provisioning for compute and IO resources. Compared to standard NIC data paths with a well-defined set of offloaded functions, unpredictable execution times of SmartNIC kernels make conventional approaches for multi-tenancy and QoS insufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager co-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware resource multiplexing of the on-path packet processing data plane. We integrate OSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our performance results demonstrate that OSMOSIS fully supports multi-tenancy and enables broader adoption of SmartNICs in datacenters with low overhead.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298520" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/han">ETC: An Elastic Transmission Control Using End-to-End Available Bandwidth Perception</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Feixue Han, <em>Tsinghua Shenzhen International Graduate School and Peng Cheng Laboratory;</em> Qing Li, <em>Peng Cheng Laboratory;</em> Peng Zhang, <em>Tencent;</em> Gareth Tyson, <em>Hong Kong University;</em> Yong Jiang, <em>Tsinghua Shenzhen International Graduate School and Peng Cheng Laboratory;</em> Mingwei Xu, <em>Tsinghua University;</em> Yulong Lan and ZhiCheng Li, <em>Tencent</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Researchers and practitioners have proposed various transport protocols to keep up with advances in networks and the applications that use them. Current Wide Area Network protocols strive to identify a congestion signal to make distributed but fair judgments. However, existing congestion signals such as RTT and packet loss can only be observed <em>after</em> congestion occurs. We therefore propose Elastic Transmission Control (ETC). ETC exploits the instantaneous receipt rate of <em>N</em> consecutive packets as the congestion signal. We refer to this as the <em>pulling rate</em>, as we posit that the receipt rate can be used to "pull'' the sending rate towards a fair share of the capacity. Naturally, this signal can be measured prior to congestion, as senders can access it immediately after the acknowledgment of the first <em>N</em> packets. Exploiting the pulling rate measurements, ETC calculates the optimal rate update steps following a simple elastic principle: the further away from the pulling rate, the faster the sending rate increases. We conduct extensive experiments using both simulated and real networks. Our results show that ETC outperforms the state-of-the-art protocols in terms of both throughput (15% higher than Copa) and latency (20% lower than BBR). Besides, ETC shows superiority in convergence speed and fairness, with a 10× improvement in convergence time even compared to the protocol with the best convergence performance.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">3:40 pm–4:10 pm</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Break with Refreshments</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">4:10 pm–5:55 pm</span></h3></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298647" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Edge Computing</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298522" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/zhang-li-prototyping">More is Different: Prototyping and Analyzing a New Form of Edge Server with Massive Mobile SoCs</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Li Zhang, <em>Beijing University of Posts and Telecommunications;</em> Zhe Fu, <em>Tsinghua University;</em> Boqing Shi and Xiang Li, <em>Beijing University of Posts and Telecommunications;</em> Rujin Lai and Chenyang Yang, <em>vclusters;</em> Ao Zhou, Xiao Ma, Shangguang Wang, and Mengwei Xu, <em>Beijing University of Posts and Telecommunications</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Huge energy consumption poses a significant challenge for edge clouds. In response to this, we introduce a new type of edge server, namely SoC Cluster, that orchestrates multiple low-power mobile system-on-chips (SoCs) through an on-chip network. For the first time, we have developed a concrete SoC Cluster consisting of 60 Qualcomm Snapdragon 865 SoCs housed in a 2U rack, which has been successfully commercialized and extensively deployed in edge clouds. Cloud gaming emerges as the principal workload on these deployed SoC Clusters, owing to the compatibility between mobile SoCs and native mobile games.</p>

<p>In this study, we aim to demystify whether the SoC Cluster can efﬁciently serve more generalized, typical edge workloads. Therefore, we developed a benchmark suite that employs state-of-the-art libraries for two critical edge workloads, i.e., video transcoding and deep learning inference. This suite evaluates throughput, latency, power consumption, and other application-speciﬁc metrics like video quality. Following this, we conducted a thorough measurement study and directly compared the SoC Cluster with traditional edge servers, with regards to electricity usage and monetary cost. Our results quantitatively reveal when and for which applications mobile SoCs exhibit higher energy efﬁciency than traditional servers, as well as their ability to proportionally scale power consumption with ﬂuctuating incoming loads. These outcomes provide insightful implications and offer valuable direction for further reﬁnement of the SoC Cluster to facilitate its deployment across wider edge scenarios.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298524" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wen">HiP4-UPF: Towards High-Performance Comprehensive 5G User Plane Function on P4 Programmable Switches</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Zhixin Wen and Guanhua Yan, <em>Binghamton University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Due to better cost benefits, P4 programmable switches have been considered in a few recent works to implement 5G User Plane Function (UPF). To circumvent limited resources on P4 programmable switches, they either ignore some essential UPF features or resort to a hybrid deployment approach which requires extra resources. This work is aimed to improve the performance of UPFs with comprehensive features which, except packet buffering, are deployable entirely on commodity P4 programmable switches. We build a baseline UPF based on prior work and analyze its key performance bottlenecks. We propose a three-tiered approach to optimize rule storage on the switch ASICs. We also develop a novel scheme that combines pendulum table access and selective usage pulling to reduce the operational latency of the UPF. Using a commodity P4 programmable switch, the experimental results show that our UPF implementation can support twice as many mobile devices as the baseline UPF and 1.9 times more than SD-Fabric. Our work also improves the throughputs in three common types of 5G call flows by 9-619% over the UPF solutions in two open-source 5G network emulators.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298526" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/ye-ziwen">KEPC-Push: A Knowledge-Enhanced Proactive Content Push Strategy for Edge-Assisted Video Feed Streaming</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Ziwen Ye, <em>Peng Cheng Laboratory and Tsinghua Shenzhen International Graduate School;</em> Qing Li, <em>Peng Cheng Laboratory;</em> Chunyu Qiao, <em>ByteDance;</em> Xiaoteng Ma, <em>Tsinghua Shenzhen International Graduate School;</em> Yong Jiang, <em>Peng Cheng Laboratory and Tsinghua Shenzhen International Graduate School;</em> Qian Ma and Shengbin Meng, <em>ByteDance;</em> Zhenhui Yuan, <em>University of Warwick;</em> Zili Meng, <em>HKUST</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Video Feed Streaming (e.g., TikTok, Reels) is increasingly popular nowadays. Users will be scheduled to the distribution infrastructure, including content distribution network (CDN) and multi-access edge computing (MEC) nodes, to access the content. Our observation is that the existing proactive content push algorithms, which are primarily based on historical access information and designed for on-demand videos, no longer meet the demands of video feed streaming. The main reason is that video feed streaming applications always push recently generated videos to attract users’ interests, thus lacking historical information when pushing. In this case, push mismatches and load imbalances will be observed, resulting in degraded bandwidth cost and user experience. To this end, we propose KEPC-Push, a Knowledge-Enhanced Proactive Content Push strategy with the \textit{knowledge} of video content features. KEPC-Push employs knowledge graphs to determine the popularity correlation among similar videos (with similar authors, contents, length, etc.) and pushes content based on this guidance. Besides, KEPC-Push designs a hierarchical algorithm to optimize the resource allocation in edge nodes with heterogeneous capabilities and runs at the regional level to shorten the communication distance. Trace-driven simulations show that KEPC-Push saves the peak-period CDN bandwidth costs by 20% and improves the average download speeds by 7% against the state-of-the-art solutions.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298528" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/zhang-li-gaming">High-density Mobile Cloud Gaming on Edge SoC Clusters</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Li Zhang, Shangguang Wang, and Mengwei Xu, <em>Beijing University of Posts and Telecommunications</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>System-on-Chip (SoC) Clusters, i.e., servers consisting of many stacked mobile SoCs, have emerged as a popular platform for serving mobile cloud gaming. Sharing the underlying hardware and OS, these SoC Clusters enable native mobile games to be executed and rendered efﬁciently without modiﬁcation. However, the number of deployed game sessions is limited due to conservative deployment strategies and high GPU utilization in current game ofﬂoading methods. To address these challenges, we introduce SFG, the ﬁrst system that enables high-density mobile cloud gaming on SoC Clusters with two novel techniques: (1) It employs a resource-efﬁcient game partitioning and cross-SoC ofﬂoading design that maximally preserves GPU optimization intents in the standard graphics rendering pipeline; (2) It proposes an NPU-enhanced game partition coordination strategy to adjust game performance when co-locating partitioned and complete game sessions. Our evaluation of ﬁve Unity games shows that SFG achieves up to 4.5× higher game density than existing methods with trivial performance loss. Equally important, SFG extends the lifespan of SoC Clusters, enabling outdated SoC Clusters to serve new games that are unfeasible on a single SoC due to GPU resource shortages.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298648" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Operating Systems 1</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298530" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/chen-xiangdong">Limitations and Opportunities of Modern Hardware Isolation Mechanisms</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Xiangdong Chen and Zhaofeng Li, <em>University of Utah;</em> Tirth Jain, <em>Maya Labs;</em> Vikram Narayanan and Anton Burtsev, <em>University of Utah</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>A surge in the number, complexity, and automation of targeted security attacks has triggered a wave of interest in hardware support for isolation. Intel memory protection keys (MPK), ARM pointer authentication (PAC), ARM memory tagging extensions (MTE), and ARM Morello capabilities are just a few hardware mechanisms aimed at supporting low-overhead isolation in recent CPUs. These new mechanisms aim to bring practical isolation to a broad range of systems, e.g., browser plugins, device drivers and kernel extensions, user-defined database and network functions, serverless cloud platforms, and many more. However, as these technologies are still nascent, their advantages and limitations are yet unclear. In this work, we do an in-depth look at modern hardware isolation mechanisms with the goal of understanding their suitability for the isolation of subsystems with the tightest performance budgets. Our analysis shows that while a huge step forward, the isolation mechanisms in commodity CPUs are still lacking implementation of several design principles critical for supporting low-overhead enforcement of isolation boundaries, zero-copy exchange of data, and secure revocation of access permissions.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298532" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/cao">FetchBPF: Customizable Prefetching Policies in Linux with eBPF</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Xuechun Cao, Shaurya Patel, and Soo Yee Lim, <em>University of British Columbia;</em> Xueyuan Han, <em>Wake Forest University;</em> Thomas Pasquier, <em>University of British Columbia</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Monolithic operating systems are infamously complex. Linux in particular has a tendency to intermingle policy and mechanisms in a manner that hinders modularity. This is especially problematic when developers aim to finely optimize performance,since it is often the case that a default policy in Linux, while performing well on average, cannot achieve the optimal performance in all circumstances. However, developing and maintaining a bespoke kernel to satisfy the need of a specific application is usually an unrealistic endeavor due to the high software engineering cost. Therefore, we need a mechanism to easily customize kernel policies and its behavior. In this paper, we design a framework called FetchBPF that addresses this problem in the context of memory prefetching. FetchBPF extends the widely used eBPF framework to allow developers to easily express, develop, and deploy prefetching policies without modifying the kernel codebase. We implement various memory prefetching policies from the literature and demonstrate that our deployment model incurs negligible overhead as compared to the equivalent native kernel implementation.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298534" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/jia">Fast (Trapless) Kernel Probes Everywhere</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Jinghao Jia, <em>University of Illinois Urbana-Champaign;</em> Michael V. Le and Salman Ahmed, <em>IBM T.J. Watson Research Center;</em> Dan Williams, <em>Virginia Tech and IBM T.J. Watson Research Center;</em> Hani Jamjoom, <em>IBM T.J. Watson Research Center;</em> Tianyin Xu, <em>University of Illinois at Urbana-Champaign</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The ability to efficiently probe and instrument a running operating system (OS) kernel is critical for debugging, system security, and performance monitoring. While efforts to optimize the widely used Kprobes in Linux over the past two decades have greatly improved its performance, many fundamental gaps remain that prevent it from being completely efficient. Specifically, we find that Kprobe is only optimized for ~80% of kernel instructions, leaving the remaining probe-able kernel code to suffer the severe penalties of double traps needed by the Kprobe implementation. In this paper, we focus on the design and implementation of an efficient and general trapless kernel probing mechanism (no hardware exceptions) that can be applied to almost all code in Linux. We discover that the main limitation of current probe optimization efforts comes from not being able to assume or change certain properties/layouts of the target kernel code. Our main insight is that by introducing strategically placed <em>nops</em>, thus slightly changing the code layout, we can overcome this main limitation. We implement our mechanism on Linux Kprobe, which is transparent to the users. Our evaluation shows a 10x improvement of probe performance over standard Kprobe while providing this level of performance for 96% of kernel code.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298536" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/ma">HydraRPC: RPC in the CXL Era</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Teng Ma, <em>Alibaba Group;</em> Zheng Liu, <em>Zhejiang University and Alibaba Group;</em> Chengkun Wei, <em>Zhejiang University;</em> Jialiang Huang, <em>Alibaba Group and Tsinghua University;</em> Youwei Zhuo, <em>Alibaba Group and Peking University;</em> Haoyu Li, <em>Zhejiang University;</em> Ning Zhang, Yijin Guan, and Dimin Niu, <em>Alibaba Group;</em> Mingxing Zhang, <em>Tsinghua University;</em> Tao Ma, <em>Alibaba Group</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>In this paper, we present HydraRPC, which utilizes CXL-attached HDM for data transmission. By leveraging CXL, HydraRPC can benefit from memory sharing, memory semantics, and high scalability. As a result, expensive network rounds, memory copying, and serialization/deserialization are eliminated. Since CXL.cache protocols are not fully supported, we employ non-cachable sharing to bypass the CPU cache and design a busy-polling free notification mechanism. This ensures efficient data transmission without the need for constant polling. We conducted evaluations of HydraRPC on real CXL hardware, which showcased the potential efficiency of utilizing CXL HDM to build RPC systems.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298538" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/jalalian">ExtMem: Enabling Application-Aware Virtual Memory Management for Data-Intensive Applications</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Sepehr Jalalian, Shaurya Patel, Milad Rezaei Hajidehi, Margo Seltzer, and Alexandra Fedorova, <em>University of British Columbia</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>For over forty years, researchers have demonstrated that operating system memory managers often fall short in supporting memory-hungry applications. The problem is even more critical today, with disaggregated memory and new memory technologies and in the presence of tera-scale machine learning models, large-scale graph processing, and other memory-intensive applications. Past attempts to provide application-specific memory management either required significant in-kernel changes or suffered from high overhead. We present ExtMem, a flexible framework for providing application-specific memory management. It differs from prior solutions in three ways: (1) It is compatible with today’s Linux deployments, (2) it is a general-purpose substrate for addressing various memory and storage backends, and (3) it is performant in multithreaded environments. ExtMem allows for easy and rapid prototyping of new memory management algorithms, easy collection of memory patterns and statistics, and immediate deployment of isolated custom memory management.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">6:00 pm–7:30 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="posters-osdi"></a></div></div></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>OSDI '24 Poster Session and Reception</h2>
</div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h2 class="field field-name-field-date-text field-type-text field-label-hidden"><span class="field-item odd first last">Thursday, July 11</span></h2><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">8:00 am–9:00 am</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="thu"></a></div></div></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Continental Breakfast</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">9:00 am–10:40 am</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="thuam"></a></div></div></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298649" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Operating Systems 2</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298540" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/nair">Telescope: Telemetry for Gargantuan Memory Footprint Applications</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Alan Nair, Sandeep Kumar, and Aravinda Prasad, <em>Intel Labs;</em> Ying Huang, <em>Intel Corporation;</em> Andy Rudoff and Sreenivas Subramoney, <em>Intel Labs</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Data-hungry applications that require terabytes of memory have become widespread in recent years. To meet the memory needs of these applications, data centers are embracing tiered memory architectures with near and far memory tiers. Precise, efficient, and timely identification of hot and cold data and their placement in appropriate tiers is critical for performance in such systems. Unfortunately, the existing state-of-the-art telemetry techniques for hot and cold data detection are ineffective at terabyte scale.</p>

<p>We propose Telescope, a novel technique that profiles different levels of the application's page table tree for fast and efficient identification of hot and cold data. Telescope is based on the observation that for a memory- and TLB-intensive workload, higher levels of a page table tree are also frequently accessed during a hardware page table walk. Hence, the hotness of the higher levels of the page table tree essentially captures the hotness of its subtrees or address space sub-regions at a coarser granularity. We exploit this insight to quickly converge on even a few megabytes of hot data and efficiently identify several gigabytes of cold data in terabyte-scale applications.
Importantly, such a technique can seamlessly scale to petabyte-scale applications.</p>

<p>Telescope's telemetry achieves 90%+ precision and recall at just 0.9% single CPU utilization for microbenchmarks with 5 TB memory footprint. Memory tiering based on Telescope results in 5.6% to 34% throughput improvement for real-world benchmarks with 1–2 TB memory footprint compared to other state-of-the-art telemetry techniques.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298542" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/li-hongyu">An Empirical Study of Rust-for-Linux: The Success, Dissatisfaction, and Compromise</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Hongyu Li, <em>Beijing University of Posts and Telecommunications;</em> Liwei Guo, <em>University of Electronic Science and Technology of China;</em> Yexuan Yang, Shangguang Wang, and Mengwei Xu, <em>Beijing University of Posts and Telecommunications</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Developed for over 30 years, Linux has already become the computing foundation for today's digital world; from gigantic, complex mainframes (e.g., supercomputers) to cheap, wimpy embedded devices (e.g., IoTs), countless applications are built on top of it. Yet, such an infrastructure has been plagued by numerous memory and concurrency bugs since the day it was born, due to many rogue memory operations are permitted by C language. A recent project Rust-for-Linux (RFL) has the potential to address Linux's safety concerns once and for all -- by embracing Rust's static ownership and type checkers into the kernel code, the kernel may finally be free from memory and concurrency bugs without hurting its performance. While it has been gradually matured and even merged into Linux mainline, however, RFL is rarely studied and still remains unclear whether it has indeed reconciled the safety and performance dilemma for the kernel. </p>

<p>To this end, we conduct the first empirical study on RFL to understand its status quo and benefits, especially on how Rust fuses with Linux and whether the fusion assures driver safety without overhead. We collect and analyze 6 key RFL drivers, which involve hundreds of issues and PRs, thousands of Github commits and mail exchanges of the Linux mailing list, as well as over 12K discussions on Zulip.We have found while Rust mitigates kernel vulnerabilities, it is beyond Rust's capability to fully eliminate them; what is more, if not handled properly, its safety assurance even costs the developers dearly in terms of both runtime overhead and development efforts.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298544" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/gao-bin-scalable">Scalable and Effective Page-table and TLB management on NUMA Systems</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Bin Gao, Qingxuan Kang, and Hao-Wei Tee, <em>National University of Singapore;</em> Kyle Timothy Ng Chu, <em>Horizon Quantum Computing;</em> Alireza Sanaee, <em>Queen Mary University of London;</em> Djordje Jevdjic, <em>National University of Singapore</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Memory management operations that modify page-tables, typically performed during memory allocation/deallocation, are infamous for their poor performance in highly threaded applications, largely due to process-wide TLB shootdowns that the OS must issue due to the lack of hardware support for TLB coherence. We study these operations in NUMA settings, where we observe up to 40x overhead for basic operations such as munmap or mprotect. The overhead further increases if page-table replication is used, where complete coherent copies of the page-tables are maintained across all NUMA nodes. While eager system-wide replication is extremely effective at localizing page-table reads during address translation, we find that it creates additional penalties upon any page-table changes due to the need to maintain all replicas coherent.</p>

<p>In this paper, we propose a novel page-table management mechanism, called <em>Hydra</em>, to enable transparent, on-demand, and partial page-table replication across NUMA nodes in order to perform address translation locally, while avoiding the overheads and scalability issues of system-wide full page-table replication. We then show that Hydra's precise knowledge of page-table sharers can be leveraged to significantly reduce the number of TLB shootdowns issued upon any memory-management operation. As a result, Hydra not only avoids replication-related slowdowns, but also provides significant speedup over the baseline on memory allocation/deallocation and access control operations. We implement Hydra in Linux on x86_64, evaluate it on 4- and 8-socket systems, and show that Hydra achieves the full benefits of eager page-table replication on a wide range of applications, while also achieving a 12% and 36% runtime improvement on Webserver and Memcached respectively due to a significant reduction in TLB shootdowns.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298546" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/zhong">UniMem: Redesigning Disaggregated Memory within A Unified Local-Remote Memory Hierarchy</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yijie Zhong, Minqiang Zhou, and Zhirong Shen, <em>Xiamen University;</em> Jiwu Shu, <em>Xiamen University and Minjiang University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Disaggregated memory (DM) has been proposed as a feasible solution towards scaling memory capacity. A variety of memory disaggregation approaches have been introduced to facilitate the practical use of DM. The cache-coherent-based DM system, which relies on cache-coherent accelerator, can offer network-attached memory as NUMA memory. However, the current cache-coherent-based DM system introduces an extra address translation for each remote memory access. Meanwhile, the local cache mechanism of existing approaches overlooks the inherent issues of cache thrashing and pollution that arise from DM system. This paper presents UniMem, a cache-coherent-based DM system that proposes a unified local-remote memory hierarchy to remove extra indirection layer on remote memory access path. To optimize local memory utilization, UniMem redesigns the local cache mechanism to prevent cache thrashing and pollution. Furthermore, UniMem puts forth a page migration mechanism that promotes frequently used pages from device-attached memory to host memory based not only on page hotness but also on hotness fragmentation. Compared to state-of-the-art systems, UniMem reduces the average memory access time by up to 76.4% and offers substantial improvement in terms of data amplification.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298650" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Correctness</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298548" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/liang">WingFuzz: Implementing Continuous Fuzzing for DBMSs</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Jie Liang, Zhiyong Wu, and Jingzhou Fu, <em>Tsinghua University;</em> Yiyuan Bai and Qiang Zhang, <em>Shuimu Yulin Technology Co., Ltd.;</em> Yu Jiang, <em>Tsinghua University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Database management systems (DBMSs) are critical components within software ecosystems, and their security and stability are paramount. In recent years, fuzzing has emerged as a prominent automated testing technique, effectively identifying vulnerabilities in various DBMSs. Nevertheless, many of these fuzzers require specific adaptation for a DBMS with a particular version. Employing these techniques to test enterprise-level DBMSs continuously poses challenges due to the diverse specifications of DBMSs and the code changes in their rapid version evolution.</p>

<p>In this paper, we present the industry practice of implementing continuous DBMS fuzzing on enterprise-level DBMSs like ClickHouse. We summarize three main obstacles in implementing, namely the diverse SQL grammar in test case generation, the ongoing evolution of codebase in continuous testing, and the disturbance of noises during anomaly analysis. We propose WingFuzz, which utilizes specification-based mutator generation, corpus-driven evolving code fuzzing, and noise-resilient anomaly assessment to address them. By working with the engineers in continuous DBMS fuzzing, we have found a total of 236 previously undiscovered bugs in 12 widely-used enterprise-level DBMSs including ClickHouse, DamengDB, and TenDB. Due to its favorable test results, our efforts received recognition and cooperation invitations from some DBMS vendors. For example, ClickHouse’s CTO praised: "Which tool did you use to find this test case? We need to integrate it into our CI." and WingFuzz has been successfully integrated into its development process.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298550" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/suzuki">Balancing Analysis Time and Bug Detection: Daily Development-friendly Bug Detection in Linux</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Keita Suzuki, <em>Keio University;</em> Kenta Ishiguro, <em>Hosei University;</em> Kenji Kono, <em>Keio University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Linux, a battle-tested codebase, is known to suffer from many bugs despite its extensive testing mechanisms. While many of these bugs require domain-specific knowledge for detection, a significant portion matches well-known bug patterns. Even though these bugs can be found with existing tools, our simple check of Linux kernel patches suggests that these tools are not used much in the developer's daily workflow. The lack of usage is probably due to the well-known trade-off between analysis time and bug detection capabilities: tools typically employ complex analysis to effectively and comprehensively find bugs in return for a long analysis time, or focus on a short analysis time by only employing elementary analyses and thus can only find a very limited number of bugs. Ideally, developers expect the tools to incur short analysis time, while still finding many bugs to use them in daily development.</p>

<p>This paper explores an approach that balances this trade-off by focusing on bugs that can be found with less computationally-complex analysis methods, and limiting the scope to each source code. To achieve this, we propose a combination of computationally lightweight analyses and demonstrate our claim by designing FiTx, a framework for generating daily development-friendly bug checkers that focus on well-known patterns. Despite its simplicity, FiTx successfully identified 47 new bugs in the Linux kernel version 5.15 within 2.5 hours, outperforming Clang Static Analyzer and CppCheck in both speed and bug detection. It demonstrates that focusing on less complex bug patterns can still significantly contribute to the improvement of codebase health. FiTx can be embedded into the daily development routine, enabling early bug detection without sacrificing developers' time.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298552" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/liu-bingzhe">Kivi: Verification for Cluster Management</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Bingzhe Liu and Gangmuk Lim, <em>UIUC;</em> Ryan Beckett, <em>Microsoft;</em> P. Brighten Godfrey, <em>UIUC and Broadcom</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Modern cloud infrastructure is powered by cluster management systems such as Kubernetes and Docker Swarm. While these systems seek to minimize users’ operational burden, the complex, dynamic, and non-deterministic nature of these systems makes them hard to reason about, potentially leading to failures ranging from performance degradation to outages.</p>

<p>We present Kivi, the first system for verifying controllers and their configurations in cluster management systems. Kivi focuses on the popular system Kubernetes, and models its controllers and events into processes whereby their interleavings are exhaustively checked via model checking. Central to handling autoscaling and large-scale deployments are our modeling optimizations and our design which seeks to find violations in a smaller and reduced topology. We show that Kivi is effective and accurate in finding issues in realistic and complex scenarios and showcase two new issues in Kubernetes controller source code.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298554" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/lyu">Monarch: A Fuzzing Framework for Distributed File Systems</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Tao Lyu, <em>EPFL;</em> Liyi Zhang, <em>University of Waterloo;</em> Zhiyao Feng, Yueyang Pan, and Yujie Ren, <em>EPFL;</em> Meng Xu, <em>University of Waterloo;</em> Mathias Payer and Sanidhya Kashyap, <em>EPFL</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Distributed file systems (DFSes) are prone to bugs. Although numerous bug-finding techniques have been applied to DFSes, static analysis does not scale well with the sheer complexity of DFS codebases while dynamic methods (e.g., regression testing) are limited by the quality of test cases. Although both can be improved by pouring in manual effort, they are less practical when facing a diverse set of real-world DFSes. Fuzzing, on the other hand, has shown great success in local systems. However, several problems exist if we apply existing fuzzers to DFSes as they 1) cannot test multiple components of DFSes holistically; 2) miss the critical testing aspects of DFSes (e.g., distributed faults); 3) have not yet explored the practical state representations as fuzzing feedback; and 4) lack checkers for asserting semantic bugs unique to DFSes.</p>

<p>In this paper, we introduce MONARCH, a multi-node fuzzing framework to test all POSIX-compliant DFSes under one umbrella. MONARCH pioneers push-button fuzzing for DFSes with a new set of building blocks to the fuzzing toolbox: 1) A multi-node fuzzing architecture for testing diverse DFSes from a holistic perspective; 2) A two-step mutator for testing DFSes with syscalls and faults; 3) Practical execution state representations with a unified coverage collection scheme across execution contexts; 4) A new DFSes semantic checker SYMSC. We applied MONARCH to six DFSes and uncovered a total of 48 bugs, including a bug whose existence can be traced back to the initial release of the DFSes.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">10:40 am–11:10 am</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Break with Refreshments</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">11:10 am–12:25 pm</span></h3></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298651" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">ML Training</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298556" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/yuan">Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Tailing Yuan, Yuliang Liu, Xucheng Ye, Shenglong Zhang, Jianchao Tan, Bin Chen, Chengru Song, and Di Zhang, <em>Kuaishou Technology</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Recent advancements in training large-scale models have centered on optimizing activation strategies and exploring various parallel training options. One research avenue focuses on enhancing activation-related operations, such as offloading and recomputing. However, there is room for further refinement in these strategies to improve the balance between computation and memory utilization. Another line of work explores different training parallelisms, which often require extensive parameter tuning and achieve suboptimal combinations of parallel options.</p>

<p>To tackle these challenges, this paper introduces a novel method for losslessly accelerating the training of large language models. Specifically, two efficient activation rematerialization strategies are proposed: Pipeline-Parallel-Aware Offloading, which maximizes the utilization of host memory for storing activations, and Compute-Memory Balanced Checkpointing, which seeks a practical equilibrium between activation memory and computational efficiency. Additionally, the paper presents an extremely efficient searching method for optimizing parameters for hybrid parallelism, considering both offloading and checkpointing to achieve optimal performance. The efficacy of the proposed method is demonstrated through extensive experiments on public benchmarks with diverse model sizes and context window sizes. For example, the method significantly increases Model FLOPs Utilization (MFU) from 32.3% to 42.7% for a 175B Llama-like model with a context window size of 32,768 on 256 NVIDIA H800.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298558" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/um">Metis: Fast Automatic Distributed Training on Heterogeneous GPUs</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Taegeon Um, Byungsoo Oh, Minyoung Kang, Woo-Yeon Lee, Goeun Kim, Dongseob Kim, Youngtaek Kim, and Mohd Muzzammil, <em>Samsung Research;</em> Myeongjae Jeon, <em>UNIST</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>As deep learning model sizes expand and new GPUs are released every year, the need for distributed training on heterogeneous GPUs rises to fully harness under-utilized low-end GPUs and reduce the cost of purchasing expensive high-end GPUs. In this paper, we introduce Metis, a system designed to automatically find efficient parallelism plans for distributed training on heterogeneous GPUs. Metis holistically optimizes several key system components, such as profiler, cost estimator, and planner, which were limited to single GPU types, to now efficiently leverage compute powers and memory capacities of diverse GPU types. This enables Metis to achieve fine-grained distribution of training workloads across heterogeneous GPUs, improving resource efficiency. However, the search space designed for automatic parallelism in this complexity would be prohibitively expensive to navigate. </p>

<p>To address this issue, Metis develops a new search algorithm that efficiently prunes large search spaces and balances loads with heterogeneity-awareness, while preferring data parallelism over tensor parallelism within a pipeline stage to take advantage of its superior computation and communication trade-offs. Our evaluation with three large models (GPT-3, MoE, and Wide-Resnet) on combinations of three types of GPUs demonstrates that Metis finds better parallelism plans than traditional methods with $1.05 ~ 8.43× training speed-up, while requiring less profiling searching time. Compared to the oracle planning that delivers the fastest parallel training, Metis finds near-optimal solutions while reducing profiling and search overheads by orders of magnitude.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298560" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xu-mengwei">FwdLLM: Efficient Federated Finetuning of Large Language Models with Perturbed Inferences</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, and Shangguang Wang, <em>Beijing University of Posts and Telecommunications (BUPT)</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, i.e., FedLLM. A vital challenge of FedLLM is the tension between LLM complexity and resource constraint of mobile devices.</p>
<p>In response to this challenge, this work introduces FwdFL, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdFL is to employ backpropagation (BP)-free training methods, requiring devices only to execute ''perturbed inferences''. Consequently, FwdFL delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdFL centers around three key designs: (1) it combines BP-free training with parameter-efficient training methods, an essential way to scale the approach to the LLM era; (2) it systematically and adaptively allocates computational loads across devices, striking a careful balance between convergence speed and accuracy; (3) it discriminatively samples perturbed predictions that are more valuable to model convergence. Comprehensive experiments illustrate FwdFL's significant advantages over conventional methods, including up to three orders of magnitude faster convergence and a 4.6× reduction in memory footprint. Uniquely, FwdFL paves the way for federated billion-parameter LLMs such as LLaMA on COTS mobile devices -- a feat previously unattained.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298652" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Security 1</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298562" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/song">A Secure, Fast, and Resource-Efficient Serverless Platform with Function REWIND</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Jaehyun Song and Bumsuk Kim, <em>Sungkyunkwan University;</em> Minwoo Kwak, <em>Yonsei University;</em> Byoungyoung Lee, <em>Seoul National University;</em> Euiseong Seo, <em>Sungkyunkwan University;</em> Jinkyu Jeong, <em>Yonsei University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Serverless computing often utilizes the warm container technique to improve response times. However, this method, which allows the reuse of function containers across different function requests of the same type, creates persistent vulnerabilities in memory and file systems. These vulnerabilities can lead to security breaches such as data leaks. Traditional approaches to address these issues often suffer from performance drawbacks and high memory requirements due to extensive use of user-level snapshots and complex restoration processes.</p>

<p>The paper introduces REWIND, an innovative and efficient serverless function execution platform designed to address these security and efficiency concerns. REWIND ensures that after each function request, the container is reset to an initial state, free from any sensitive data, including a thorough restoration of the file system to prevent data leakage. It incorporates a kernel-level memory snapshot management system, which significantly lowers memory usage and accelerates the rewind process. Additionally, REWIND optimizes runtime by reusing memory regions and leveraging the temporal locality of function executions, enhancing performance while maintaining strict data isolation between requests. The REWIND prototype is implemented on OpenWhisk and Linux and evaluated with serverless benchmark workloads. The evaluation results have demonstrated that REWIND provides substantial memory saving while providing high function execution performance. Especially, the low memory usage makes more warm containers kept alive thereby improving the throughput as well as the latency of function execution while providing isolation between function requests.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298564" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/sun">SimEnc: A High-Performance Similarity-Preserving Encryption Approach for Deduplication of Encrypted Docker Images</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Tong Sun and Bowen Jiang, <em>Zhejiang University;</em> Borui Li, <em>Southeast University;</em> Jiamei Lv, Yi Gao, and Wei Dong, <em>Zhejiang University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Encrypted Docker images are becoming increasingly popular in Docker registries for privacy. As the Docker registry is tasked with managing an increasing number of images, it becomes essential to implement deduplication to conserve storage space. However, deduplication for encrypted images is difficult because deduplication exploits identical content, while encryption tries to make all contents look random. Existing state-of-the-art works try to decompress images and perform message-locked encryption (MLE) to deduplicate encrypted images. Unfortunately, our measurements uncover two limitations in current works: (i) even minor modifications to the image content can hinder MLE deduplication, (ii) decompressing image layers would increase the size of the storage for duplicate data, and significantly compromise user pull latency and deduplication throughput.</p>

<p>In this paper, we propose <strong>SimEnc</strong>, a high-performance similarity-preserving encryption approach for deduplication of encrypted Docker images. SimEnc is the first work that integrates the semantic hash technique into MLE to extract semantic information among layers for improving the deduplication ratio. SimEnc builds on a fast similarity space selection mechanism for flexibility. Unlike existing works completely decompressing the layer, we explore a new similarity space by Huffman decoding that achieves a better deduplication ratio and performance. Experiments show that SimEnc outperforms both the state-of-the-art encrypted serverless platform and plaintext Docker registry, reducing storage consumption by up to 261.7% and 54.2%, respectively. Meanwhile, SimEnc can surpass them in terms of pull latency.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298566" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/yoon">mmTLS: Scaling the Performance of Encrypted Network Traffic Inspection</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Junghan Yoon, <em>Seoul National University;</em> Seunghyun Do and Duckwoo Kim, <em>KAIST;</em> Taejoong Chung, <em>Virginia Tech;</em> KyoungSoo Park, <em>Seoul National University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Modern network monitoring TLS middleboxes play a critical role in fighting against the abuse by encrypted network traffic. Unfortunately, operating a TLS middlebox often incurs a huge computational overhead as it must translate and relay encrypted traffic from one endpoint to the other. We observe that even a simple TLS proxy drops the throughput of end-to-end TLS sessions by 43% to 73%. What is worse is that recent security enhancement TLS middlebox works levy an even more computational tax.</p>

<p>In this paper, we present mmTLS, a scalable TLS middlebox development framework that significantly improves the traffic inspection performance and provides a TLS event programming library with which one can write a TLS middlebox with ease. mmTLS eliminates the traffic relaying cost as it operates on a single end-to-end TLS session by secure session key sharing. This approach is not only beneficial to performance but it naturally guarantees all end-to-end TLS properties except confidentiality. To detect illegal content modification, mmTLS supplements a TLS record with a private tag whose key is kept secret only to TLS endpoints. We find that the extra overhead for private tag generation and verification is minimal when augmented with the first tag generation. Our evaluation demonstrates that mmTLS outperforms the nginx TLS proxy in the split-connection mode by a factor 2.7 to 41.2, and achieves 179 Gbps of traffic relaying throughput.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">12:25 pm–2:00 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="luncheon-thu"></a></div></div></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Conference Luncheon</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">2:00 pm–3:40 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="thupm"></a></div></div></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298653" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">ML-System Co-Design</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298568" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/mraz">Pecan: Cost-Efficient ML Data Preprocessing with Automatic Transformation Ordering and Hybrid Placement</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Dan Graur, Oto Mraz, Muyu Li, and Sepehr Pourghannad, <em>ETH Zurich;</em> Chandramohan A. Thekkath, <em>Google;</em> Ana Klimovic, <em>ETH Zurich</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Input data preprocessing is a common bottleneck in machine learning (ML) jobs, that can significantly increase training time and cost as expensive GPUs or TPUs idle waiting for input data. Previous work has shown that offloading data preprocessing to remote CPU servers successfully alleviates data stalls and improves training time. However, remote CPU workers in disaggregated data processing systems comprise a significant fraction of total training costs. Meanwhile, current disaggregated solutions often underutilize CPU and DRAM resources available on ML accelerator nodes. We propose two approaches to alleviate ML input data stalls while minimizing costs. First, we dynamically schedule data preprocessing workers on ML accelerator host resources to minimize the number of remote CPU workers needed to achieve peak data ingestion bandwidth. Second, we analyze the characteristics of input pipelines and automatically reorder transformations to increase data preprocessing worker throughput. We observe that relaxing commutativity increases throughput while maintaining high model accuracy for a variety of ML data pipelines. We build Pecan, an ML data preprocessing service that automates data preprocessing worker placement and transformation reordering decisions. Pecan reduces preprocessing costs by 87% on average and total training costs by up to 60% compared to training with state-of-the-art disaggregated data preprocessing and total training costs by 55% on average compared to collocated data preprocessing.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298570" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wang">OPER: Optimality-Guided Embedding Table Parallelization for Large-scale Recommendation Model</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Zheng Wang, <em>University of California, San Diego;</em> Yuke Wang, Boyuan Feng, and Guyue Huang, <em>University of California, Santa Barbara;</em> Dheevatsa Mudigere and Bharath Muthiah, <em>Meta;</em> Ang Li, <em>Pacific Northwest National Laboratory;</em> Yufei Ding, <em>University of California, San Diego</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The deployment of Deep Learning Recommendation Models (DLRMs) involves the parallelization of extra-large embedding tables (EMTs) on multiple GPUs. Existing works overlook the input-dependent behavior of EMTs and parallelize them in a coarse-grained manner, resulting in unbalanced workload distribution and inter-GPU communication.</p>

<p>To this end, we propose <strong>OPER</strong>, an algorithm-system co-design with <strong><u>OP</u></strong>timality-guided <strong><u>E</u></strong>mbedding table parallelization for large-scale <strong><u>R</u></strong>ecommendation model training and inference. The core idea of OPER is to explore the connection between DLRM inputs and the efficiency of distributed EMTs, aiming to provide a near-optimal parallelization strategy for EMTs. Specifically, we conduct an in-depth analysis of various types of EMTs parallelism and propose a heuristic search algorithm to efficiently approximate an empirically near-optimal EMT parallelization. Furthermore, we implement a distributed shared memory-based system, which supports the lightweight but complex computation and communication pattern of fine-grained EMT parallelization, effectively converting theoretical improvements into real speedups. Extensive evaluation shows that OPER achieves 2.3× and 4.0× speedup on average in training and inference, respectively, over state-of-the-art DLRM frameworks.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298572" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/zhang-chen">MAGPY: Compiling Eager Mode DNN Programs by Monitoring Execution States</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Chen Zhang, Rongchao Dong, Haojie Wang, Runxin Zhong, Jike Chen, and Jidong Zhai, <em>Tsinghua University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Real-world deep learning programs are often developed with dynamic programming languages like Python, which usually have complex features, such as built-in functions and dynamic typing. These programs typically execute in eager mode, where tensor operators run without compilation, resulting in poor performance. Conversely, deep learning compilers rely on operator-based computation graphs to optimize program execution. However, complexities in dynamic languages often prevent the conversion of these programs into complete operator graphs, leading to sub-optimal performance. </p>

<p>To address this challenge, we introduce MAGPY to optimize the generation of operator graphs from deep learning programs. MAGPY generates more complete operator graphs by collecting key runtime information through monitoring program execution. MAGPY provides a reference graph to record program execution states and leverages reference relationships to identify state changes that can impact program outputs. This approach significantly reduces analysis complexity, leading to more complete operator graphs. Experimental results demonstrate that MAGPY accelerates complex deep learning programs by up to 2.88× (1.55× on average), and successfully instantiates 93.40% of 1191 real user programs into complete operator graphs.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298574" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xia">Quant-LLM: Accelerating the Serving of Large Language Models via FP6-Centric Algorithm-System Co-Design on Modern GPUs</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Haojun Xia, <em>University of Sydney;</em> Zhen Zheng and Xiaoxia Wu, <em>Microsoft;</em> Shiyang Chen, <em>Rutgers University;</em> Zhewei Yao, Stephen Youn, Arash Bakhtiari, and Michael Wyatt, <em>Microsoft;</em> Donglin Zhuang and Zhongzhu Zhou, <em>University of Sydney;</em> Olatunji Ruwase, Yuxiong He, and Shuaiwen Leon Song, <em>Microsoft</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with non-power-of-two bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of 6-bit and arbitrary bit-width quantization (5-bit, etc.). We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called Quant-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved with 6-bit quantization. Experiments show that Quant-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69×-2.65× higher normalized inference throughput than the FP16 baseline. The source code is publicly available at <a href="https://github.com/usyd-fsalab/fp6_llm">https://github.com/usyd-fsalab/fp6_llm</a>.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298654" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Networks 2</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298576" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wei">QDSR: Accelerating Layer-7 Load Balancing by Direct Server Return with QUIC</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Ziqi Wei, <em>Tsinghua Shenzhen International Graduate School and Peng Cheng Laboratory;</em> Zhiqiang Wang, <em>Tencent and Peng Cheng Laboratory;</em> Qing Li, <em>Peng Cheng Laboratory;</em> Yuan Yang, <em>Tsinghua University;</em> Cheng Luo and Fuyu Wang, <em>Tencent;</em> Yong Jiang, <em>Tsinghua Shenzhen International Graduate School and Peng Cheng Laboratory;</em> Sijie Yang, <em>Tencent;</em> Zhenhui Yuan, <em>Northumbria University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Layer-7(L7) load balancing is a crucial capability for cloud service providers to maintain stable and reliable services. However, high flexibility of the L7 load balancers(LBs) and increasing downlink relaying service result in a heavy workload, which significantly increases the cost of cloud service providers and reduces end-to-end service quality. We proposes QDSR, a new L7 load balancing scheme that uses <u>Q</u>UIC and <u>D</u>irect <u>S</u>erver <u>R</u>eturn(DSR) technology. QDSR divides the QUIC connection into independent streams and distributes them to multiple real servers(RSs), enabling real servers to send data directly to the client simultaneously. Due to the lack of redundant relaying, QDSR enables high performance, low latency, and nearly eliminates additional downlink relaying overhead.</p>

<p>To evaluate the performance of QDSR, we implemented all its components using Nginx and Apache Traffic Server, deployed them in a real environment testbed, and conducted large-scale simulation experiments using mahimahi. The experimental results show that QDSR can process an additional 4.8%-18.5% of client requests compared to traditional L7 proxy-based load balancing schemes. It can achieve a maximum throughput that is 12.2 times higher in high-load scenarios and significantly reduce end-to-end latency and first packet latency.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298578" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/feng-yinxiao">Evaluating Chiplet-based Large-Scale Interconnection Networks via Cycle-Accurate Packet-Parallel Simulation</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yinxiao Feng and Yuchen Wei, <em>Institute for Interdisciplinary Information Sciences, Tsinghua University;</em> Dong Xiang, <em>School of Software, Tsinghua University;</em> Kaisheng Ma, <em>Institute for Interdisciplinary Information Sciences, Tsinghua University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The <em>Chiplet</em> architecture has achieved great success in recent years. However, chiplet-based networks are significantly different from traditional networks, thus presenting new challenges in evaluation. On the one hand, on-chiplet and off-chiplet networks are tightly coupled; therefore, the entire heterogeneous network must be designed and evaluated jointly rather than separately. On the other hand, existing network simulators cannot efficiently evaluate large-scale chiplet-based networks with cycle-accurate accuracy.</p>

<p>In this paper, we present the design and implementation of the <em>Chiplet Network Simulator (CNSim)</em>, a cycle-accurate packet-parallel simulator supporting efficient simulation for large-scale chiplet-based (shared-memory) networks. In <em>CNSim</em>, a packet-centric simulation architecture and an atomic-based hyper-threading mechanism are adopted, accelerating simulation speed by 11× ~ 14× compared with existing cycle-accurate simulators. Besides, we implement the heterogeneous router/link microarchitecture and many other features, including hierarchical topologies, adaptive routing, and real workload traces integration. Based on <em>CNSim</em>, two typical chiplet-based networks, which cannot be efficiently simulated by existing simulators, are systematically evaluated. The advantages and limitations of chiplet-based networks are revealed through systematical cycle-accurate simulations. The simulator and evaluation framework are open-sourced to the community.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298580" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/bin-yahya">Config-Snob: Tuning for the Best Configurations of Networking Protocol Stack</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Manaf Bin-Yahya, Yifei Zhao, and Hossein Shafieirad, <em>Huawei Technologies Canada;</em> Anthony Ho, <em>Huawei Technologies Canada and University of Waterloo;</em> Shijun Yin and Fanzhao Wang, <em>Huawei Technologies China;</em> Geng Li, <em>Huawei Technologies Canada</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Web servers usually use predefined configurations, yet empirical studies have shown that performance can be significantly improved when the configurations of the networking protocol stack (e.g., TCP, QUIC, and congestion control parameters) are carefully tuned due to the fact that a “one-size-fits-all” strategy does not exist. However, dynamically tuning the protocol stack's configurations is challenging: first, the configuration space is ample, and parameters with complex dependencies must be tuned jointly; second, the network condition space is also large, so an adaptive solution is needed to handle clients' diversity and network dynamics; and finally, clients endure unsatisfactory performance degradation due to learning exploration. To this end, we propose Config-Snob, a protocol tuning solution that selects the best configurations based on historical data. Config-Snob exploits the configuration space by tuning several configuration knobs and provides a practical fine-grained client grouping while handling the network environment dynamics. Config-Snob uses a controlled exploration approach to minimize the performance degradation. Config-Snob utilizes causal inference (CI) algorithms to boost the tuning optimization. Config-Snob is implemented in a QUIC-based server and deployed in a large-scale production environment. Our extensive experiments show that the proposed solution improves the completion time over the default configurations by 15% to 36% (mean) and 62% to 70% (median) in the real deployment.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298582" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xiao">Conspirator: SmartNIC-Aided Control Plane for Distributed ML Workloads</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yunming Xiao, <em>Northwestern University;</em> Diman Zad Tootaghaj, Aditya Dhakal, Lianjie Cao, and Puneet Sharma, <em>Hewlett Packard Labs;</em> Aleksandar Kuzmanovic, <em>Northwestern University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Modern machine learning (ML) workloads heavily depend on distributing tasks across clusters of server CPUs and specialized accelerators, such as GPUs and TPUs, to achieve optimal performance. Nonetheless, prior research has highlighted the inefficient utilization of computing resources in distributed ML, leading to suboptimal performance. This inefficiency primarily stems from CPU bottlenecks and suboptimal accelerator scheduling. Although numerous proposals have been put forward to address these issues individually, none have effectively tackled both inefficiencies simultaneously. In this paper, we introduce Conspirator, an innovative control plane design aimed at alleviating both bottlenecks by harnessing the enhanced computing capabilities of SmartNICs. Following the evolving role of SmartNICs, which have transitioned from their initial function of standard networking task offloading to serving as programmable connectors between disaggregated computing resources, Conspirator facilitates efficient data transfer without the involvement of host CPUs and hence circumvents the potential bottlenecks there. Conspirator further integrates a novel scheduling algorithm that takes into consideration of the heterogeneity of accelerators and adapts to changing workload dynamics, enabling the flexibility to mitigate the second bottleneck. Our evaluation demonstrates that Conspirator may provide a 15% end-to-end completion time reduction compared to RDMA-based alternatives while being 17% more cost-effective and 44% more power-efficient. Our proposed scheduler also helps to save 33% GPU hours compared to naive GPU-sharing schedulers by making close-to-optimal decisions while taking much less time than the optimal NP-Hard scheduler.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">3:40 pm–4:10 pm</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Break with Refreshments</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">4:10 pm–5:25 pm</span></h3></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298655" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Memory</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298584" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/tabatabai">FBMM: Making Memory Management Extensible With Filesystems</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Bijan Tabatabai, James Sorenson, and Michael M. Swift, <em>University of Wisconsin—Madison</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>New memory technologies like CXL promise diverse memory configurations such as tiered memory, far memory, and processing in memory. Operating systems must be modified to support these new hardware configurations for applications to make use of them. While many parts of operating systems are extensible, memory management remains monolithic in most systems, making it cumbersome to add support for a diverse set of new memory policies and mechanisms.</p>

<p>Rather than creating a whole new extensible interface for memory managers, we propose to instead use the memory management callbacks provided by the Linux virtual file system (VFS) to write memory managers, called memory management filesystems (MFSs). Memory is allocated by creating and mapping a file in an MFS's mount directory and freed by deleting the file. Use of an MFS is transparent to applications. We call this system <em>File Based Memory Management</em> (FBMM).</p>

<p>Using FBMM, we created a diverse set of standalone memory managers for tiered memory, contiguous allocations, and memory bandwidth allocation, each comprising 500-1500 lines of code. Unlike current approaches that require custom kernels, with FBMM, an MFS can be compiled separately from the kernel and loaded dynamically when needed. We measure the overhead of using filesystems for memory management and found the overhead to be less than 8% when allocating a single page, and less than 0.1% when allocating as little as 128 pages. MFSs perform competitively with kernel implementations, and sometimes better due to simpler implementations.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298586" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/egorov">Mangosteen: Fast Transparent Durability for Linearizable Applications using NVM</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Sergey Egorov, Gregory Chockler, and Brijesh Dongol, <em>University of Surrey, UK;</em> Dan O'Keeffe, <em>Royal Holloway, University of London, UK;</em> Sadegh Keshavarzi, <em>University of Surrey, UK</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The advent of byte-addressable non-volatile memory (NVM) technologies has enabled the development of low-latency high-throughput durable applications, i.e., applications that are capable of recovering from full-system crashes. However, programming such applications is error-prone as efficiency gains often require fine-grained (programmer-controlled) management of low-level persistence instructions.</p>

<p>We propose Mangosteen, a high-level programming framework that allows developers to transform an existing linearizable in-memory application to a corresponding durably linearizable version using NVM. Our framework’s API consists of a set of callback hooks that interpose on an application’s request processing flow with minimal developer effort. Mangosteen executes client operations on DRAM and persists their effects using binary instrumentation and redo logging. Mangosteen’s concurrency control facilitates batching of read-write requests to minimize the cost of persistence, while allowing read-only requests to execute concurrently. A novel intra-batch deduplication mechanism further reduces persistence overheads for common OLTP workloads. Our empirical evaluation results show that Mangosteen-enabled applications outperform state-of-the-art solutions across the entire spectrum of read-write ratios. In particular, the Mangosteen-based version of Redis demonstrates throughput gains of between 2×–5× in comparison to prior work.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298588" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xu-dong">FlexMem: Adaptive Page Profiling and Migration for Tiered Memory</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Dong Xu, <em>University of California, Merced;</em> Junhee Ryu, Jinho Baek, and Kwangsik Shin, <em>SK hynix;</em> Pengfei Su and Dong Li, <em>University of California, Merced</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Tiered memory, combining multiple memory components with different performance and capacity, provides a cost-effective solution to increase memory capacity and improve memory utilization. The existing system software to manage the tiered memory often has limitations: (1) rigid memory profiling methods that cannot timely capture emerging memory access patterns or lose profiling quality, (2) rigid page demotion (i.e., the number of pages for demotion is driven by an invariant requirement on free memory space), and (3) rigid warm page range (i.e., emerging hot pages) that leads to unnecessary page demotion from fast to slow memory. To address the above limitations, we introduce FlexMem, a page profiling and migration system for tiered memory. FlexMem combines the performance counter-based and page hinting fault-based profiling methods to improve profiling quality, dynamically decides the number of pages for demotion based on the needs of accommodating hot pages (i.e., frequently accessed pages), and dynamically decides the warm page range based on how often the pages in the range is promoted to hot pages. We evaluate FlexMem with common memory-intensive benchmarks. Compared to the state-of-the-art (Tiering-0.8, TPP, and MEMTIS), FlexMem improves performance by 32%, 23%, and 27% on average respectively.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298656" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Reliability</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298590" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xiong">SuperBench: Improving Cloud AI Infrastructure Reliability with Proactive Validation</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yifan Xiong, Yuting Jiang, Ziyue Yang, and Lei Qu, <em>Microsoft Research;</em> Guoshuai Zhao, Shuguang Liu, Dong Zhong, Boris Pinzur, Jie Zhang, Yang Wang, Jithin Jose, Hossein Pourreza, Jeff Baxter, Kushal Datta, Prabhat Ram, Luke Melton, and Joe Chau, <em>Microsoft;</em> Peng Cheng, Yongqiang Xiong, and Lidong Zhou, <em>Microsoft Research</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Reliability in cloud AI infrastructure is crucial for cloud service providers, prompting the widespread use of hardware redundancies. However, these redundancies can inadvertently lead to hidden degradation, so called "gray failure", for AI workloads, significantly affecting end-to-end performance and concealing performance issues, which complicates root cause analysis for failures and regressions.</p>

<p>We introduce SuperBench, a proactive validation system for AI infrastructure that mitigates hidden degradation caused by hardware redundancies and enhances overall reliability. SuperBench features a comprehensive benchmark suite, capable of evaluating individual hardware components and representing most real AI workloads. It comprises a Validator which learns benchmark criteria to clearly pinpoint defective components. Additionally, SuperBench incorporates a Selector to balance validation time and issue-related penalties, enabling optimal timing for validation execution with a tailored subset of benchmarks. Through testbed evaluation and simulation, we demonstrate that SuperBench can increase the mean time between incidents by up to 22.61×. SuperBench has been successfully deployed in Azure production, validating hundreds of thousands of GPUs over the last two years.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298592" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wu-ronglong">Removing Obstacles before Breaking Through the Memory Wall: A Close Look at HBM Errors in the Field</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Ronglong Wu, Shuyue Zhou, Jiahao Lu, Zhirong Shen, and Zikang Xu, <em>Xiamen University;</em> Jiwu Shu, <em>Xiamen University and Minjiang University;</em> Kunlin Yang and Feilong Lin, <em>Huawei Technologies Co., Ltd;</em> Yiming Zhang, <em>Xiamen University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p><em>High-bandwidth memory</em> (HBM) is regarded as a promising technology for fundamentally overcoming the memory wall. It stacks up multiple DRAM dies vertically to dramatically improve the memory access bandwidth. However, this architecture also comes with more severe reliability issues, since HBM not only inherits error patterns of the conventional DRAM, but also introduces new error causes. </p>

<p>In this paper, we conduct the first systematical study on HBM errors, which cover over 460 million error events collected from nineteen data centers and span over two years of deployment under a variety of services. Through error analyses and methodology validations, we confirm that the HBM exhibits different error patterns from conventional DRAM, in terms of spatial locality, temporal correlation, and sensor metrics which make empirical prediction models for DRAM error prediction ineffective for HBM. We design and implement Calchas, a hierarchical failure prediction framework for HBM based on our findings, which integrate spatial, temporal, and sensor information from various device levels to predict upcoming failures. The results demonstrate the feasibility of failure prediction across hierarchical levels.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298594" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/zhang-yuqi">MSFRD: Mutation Similarity based SSD Failure Rating and Diagnosis for Complex and Volatile Production Environments</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yuqi Zhang, Tianyi Zhang, Wenwen Hao, Shuyang Wang, Na Liu, and Xing He, <em>Samsung R&amp;D Institute China Xi'an, Samsung Electronics;</em> Yang Zhang, Weixin Wang, Yongguang Cheng, Huan Wang, Jie Xu, Feng Wang, and Bo Jiang, <em>ByteDance Inc.;</em> Yongwong Gwon, Jongsung Na, Zoe Kim, and Geunrok Oh, <em>Samsung Electronics</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>SSD failures have an increasing impact on storage reliability and performance in data centers. Some manufacturers have customized fine-grained Telemetry attributes to analyze and identify SSD failures. Based on Telemetry data, this paper proposes the mutation similarity based failure rating and diagnosis (MSFRD) scheme to predict failures in dynamic environment of data centers and improve failure handling efficiency. MSFRD dynamically detects the internal mutations of SSDs in real time and measures their similarity to the mutations of historical failed SSDs and healthy SSDs for failure prediction and early rating. Based on the rating, unavailable SSDs with serious failures are handled immediately, while available SSDs with less serious failures will be continuously tracked and diagnosed. The MSFRD is evaluated on real Telemetry datasets collected from large-scale SSDs in data centers. Compared with the existing schemes, MSFRD improves precision by 23.8% and recall by 38.9% on average for failure prediction. The results also show the effectiveness of MSFRD on failure rating and progressive diagnosis.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">6:00 pm–7:30 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="posters-atc"></a></div></div></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>USENIX ATC '24 Poster Session and Reception</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h2 class="field field-name-field-date-text field-type-text field-label-hidden"><span class="field-item odd first last">Friday, July 12</span></h2><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">8:00 am–9:00 am</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="fri"></a></div></div></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Continental Breakfast</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">9:00 am–10:15 am</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="friam"></a></div></div></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298657" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Deployed Systems</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298596" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wang-zhe">Diagnosing Application-network Anomalies for Millions of IPs in Production Clouds</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Zhe Wang, <em>Shanghai Jiao Tong University;</em> Huanwu Hu, <em>Alibaba Cloud;</em> Linghe Kong, <em>Shanghai Jiao Tong University;</em> Xinlei Kang and Teng Ma, <em>Alibaba Cloud;</em> Qiao Xiang, <em>Xiamen University;</em> Jingxuan Li and Yang Lu, <em>Alibaba Cloud;</em> Zhuo Song, <em>Shanghai Jiao Tong University and Alibaba Cloud;</em> Peihao Yang, <em>Alibaba Cloud;</em> Jiejian Wu, <em>Shanghai Jiao Tong University;</em> Yong Yang and Tao Ma, <em>Alibaba Cloud;</em> Zheng Liu, <em>Alibaba Cloud and Zhejiang University;</em> Xianlong Zeng and Dennis Cai, <em>Alibaba Cloud;</em> Guihai Chen, <em>Shanghai Jiao Tong University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Timely detection and diagnosis of application-network anomalies is a key challenge of operating large-scale production clouds. We reveal three practical issues in a cloud-native era. First, impact assessment of anomalies at a (micro)service level is absent in currently deployed monitoring systems. Ping systems are oblivious to the "actual weights'' of application traffic, <em>e.g.</em>, traffic volume and the number of connections/instances. Failures of critical (micro)services with large weights can be easily overlooked by probing systems under prevalent network jitters. Second, the efficiency of anomaly routing (to a blamed application/network team) is still low with multiple attribution teams involved. Third, collecting fine-grained metrics at a (micro)service level incurs considerable computational/storage overheads, however, is indispensable for accurate impact assessment and anomaly routing.</p>

<p>We introduce the application-network diagnosing (AND) system in Alibaba cloud. AND exploits the single metric of TCP retransmission (<em>retxs</em>) to capture anomalies at (micro)service levels and correlates applications with networks end-to-end. To resolve deployment challenges, AND further proposes three core designs: (1) a collecting tool to perform filtering/statistics on massive <em>retxs</em> at the (micro)service level, (2) a real-time detection procedure to extract anomalies from ‘noisy’ <em>retxs</em> with millions of time series, (3) an anomaly routing model to delimit anomalies among multiple target teams/scenarios. AND has been deployed in Alibaba cloud for over three years and enables minute-level anomaly detection/routing and fast failure recovery.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298598" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/tang">Data Caching for Enterprise-Grade Petabyte-Scale OLAP</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Chunxu Tang and Bin Fan, <em>Alluxio;</em> Jing Zhao and Chen Liang, <em>Uber, Inc;</em> Yi Wang and Beinan Wang, <em>Alluxio;</em> Ziyue Qiu, <em>Carnegie Mellon University and Uber, Inc.;</em> Lu Qiu, Bowen Ding, Shouzhuo Sun, Saiguang Che, Jiaming Mai, Shouwei Chen, Yu Zhu, and Jianjian Xie, <em>Alluxio;</em> Yutian (James) Sun, <em>Meta, Inc.;</em> Yao Li and Yangjun Zhang, <em>Uber, Inc.;</em> Ke Wang, <em>Meta, Inc.;</em> Mingmin Chen, <em>Uber, Inc.</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>With the exponential growth of data and evolving use cases, petabyte-scale OLAP data platforms are increasingly adopting a model that decouples compute from storage. This shift, evident in organizations like Uber and Meta, introduces operational challenges including massive, read-heavy I/O traffic with potential throttling, as well as skewed and fragmented data access patterns. Addressing these challenges, this paper introduces the Alluxio local (edge) cache, a highly effective architectural optimization tailored for such environments. This embeddable cache, optimized for petabyte-scale data analytics, leverages local SSD resources to alleviate network I/O and API call pressures, significantly improving data transfer efficiency. Integrated with OLAP systems like Presto and storage services like HDFS, the Alluxio local cache has demonstrated its effectiveness in handling large-scale, enterprise-grade workloads over three years of deployment at Uber and Meta. We share insights and operational experiences in implementing these optimizations, providing valuable perspectives on managing modern, massive-scale OLAP workloads.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298600" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/yang">Full Lifecycle Data Analysis on a Large-scale and Leadership Supercomputer: What Can We Learn from It?</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Bin Yang, <em>Tsinghua University and National Supercomputer Center in Wuxi;</em> Hao Wei, <em>Tsinghua University;</em> Wenhao Zhu, <em>Shandong University and National Supercomputer Center in Wuxi;</em> Yuhao Zhang, <em>Tsinghua University;</em> Weiguo Liu, <em>Shandong University;</em> Wei Xue, <em>Tsinghua University, Qinghai University and Intelligent Computing and Application Laboratory of Qinghai Province, and National Supercomputer Center in Wuxi</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The system architecture of contemporary supercomputers is growing increasingly intricate with the ongoing evolution of system-wide network and storage technologies, making it challenging for application developers and system administrators to manage and utilize the escalating complexity of supercomputers effectively. Moreover, the limited experience of application developers and system administrators in conducting insightful analyses of diverse High-Performance Computing (HPC) workloads and the resulting array of resource utilization characteristics exacerbate the challenge. To address this issue, we undertake a comprehensive analysis of six years' worth of 40 TB data (comprising I/O performance data and job running information) from Sunway TaihuLight, boasting 41508 nodes and currently ranked as the world's 11th-fastest supercomputer. Our study provides valuable insights into operational management strategies for HPC systems (i.e., job hanging caused by heavy-load benchmark testing, job starvation caused by aggressive scheduling policies) and I/O workload characteristics (i.e., getattr operations spiking caused by massive access to grid files, a large number of files accessed by many applications in a short period), shedding light on both challenges and opportunities for improvements in the HPC environment. This paper delineates our methodology, findings, and the significance of this study. Additionally, we discuss the potential of our research for future studies and practice within this domain.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298658" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Wide Area Network</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298602" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/li-geng">Panorama: Optimizing Internet-scale Users’ Routes from End to End</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Geng Li, Shuihai Hu, and Kun Tan, <em>Huawei Technologies</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Network performance is critical to the user experience of many real-time interactive applications, such as video conferencing and live streaming. Empirical studies show that transport latency over 300ms would become unacceptable, leading to significant user satisfaction declining. Unfortunately, due to the best-effort nature of Internet, such strict performance requirement can hardly be fully met. Despite continuous efforts have been made to improve the performance of Internet (e.g., overlay routing optimization, traffic engineering and content delivery network), we are still far from delivering satisfying network performance for these applications. The stringent network requirements, the world-wide cross-continental network transfers, and the large-scale Internet-wide users, together make it a complex challenge to deliver ideal user experience for emerging real-time interactive applications.</p>

<p>In this paper, we present <em>Panorama</em>, a scalable system for delivering desired user experience to real-time interactive applications over a globally distributed overlay network. To achieve ideal user experience, <em>Panorama</em> takes a centralized approach to do global end-to-end traffic engineering optimization, and overcomes the scalability issue by intelligent measurement-based user grouping and scalable, parallelizable route computation. <em>Panorama</em> has been deployed in a large global real-time overlay network since 2021. We evaluate <em>Panorama</em> based on 81 million selected real-world traces in deployment environment with clients across 66 countries. The extensive evaluation demonstrates that Panorama can support a routing service for millions of users, while providing latency lower than 200ms for 96.34% of the communication sessions, and improving SLA satisfaction by up to 88.0%.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298604" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/zhang-rui-xiao">Enhancing Resource Management of the World's Largest PCDN System for On-Demand Video Streaming</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Rui-Xiao Zhang, <em>UIUC;</em> Haiping Wang, Shu Shi, Xiaofei Pang, Yajie Peng, and Zhichen Xue, <em>ByteDance;</em> Jiangchuan Liu, <em>Simon Fraser University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The rapid growth of video services has led to the significant requirement for efficient content delivery. Traditional approaches mainly rely on Content Delivery Networks (CDNs), which unfortunately incur significant bandwidth cost for video providers. To resolve this problem, the cost-efficient edge resources have emerged as a new solution to replace CDNs. However, their heterogeneous hardware and poor performance still present challenges in their effective utilization. In this paper, we present how ByteDance explores the use of these cost-efficient but less performant resources. Specifically, we first present an extensive overview of PCDN, ByteDance's alternative delivery network for CDNs. Second, as PCDN encounters significant resource imbalances after years of deployment, we further introduce PCDN<sup>+</sup>, the enhanced iteration of PCDN. Specifically, by integrating a well-designed centralized/decentralized framework, we evolve previous "static'' and "uncontrolled'' PCDN into a "dynamic'' and "controlled'' system. The extensive A/B test and real-world deployment have demonstrated that PCDN<sup>+</sup> 1) effectively alleviates overloading issues, 2) significantly improves the utilization of low-cost resources, and 3) provides higher service speed.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298606" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/chaudhary">TileClipper: Lightweight Selection of Regions of Interest from Videos for Traffic Surveillance</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Shubham Chaudhary and Aryan Taneja, <em>IIIT Delhi;</em> Anjali Singh, <em>Indira Gandhi Delhi Technology University for Women;</em> Purbasha Roy, Sohum Sikdar, Mukulika Maity, and Arani Bhattacharya, <em>IIIT Delhi</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>With traffic surveillance increasingly used, thousands of cameras on roads send video feeds to cloud servers to run computer vision algorithms, requiring high bandwidth. State-of-the-art techniques reduce the bandwidth requirement by either sending a limited number of frames/pixels/regions or relying on re-encoding the important parts of the video. This imposes significant overhead on both the camera side and server side compute as re-encoding is expensive. In this work, we propose TILECLIPPER, a system that utilizes tile sampling, where a limited number of rectangular areas within the frames, known as tiles, are sent to the server. TILECLIPPER selects the tiles adaptively by utilizing its correlation with the tile bitrates. We evaluate TILECLIPPER on different datasets having 55 videos in total to show that, on average, our technique reduces ≈ 22% of data sent to the cloud while providing a detection accuracy of 92% with minimal calibration and compute compared to prior works. We show real-time tile filtering of TILECLIPPER even on cheap edge devices like Raspberry Pi 4 and nVidia Jetson Nano. We further create a live deployment of TILECLIPPER to show that it provides over 87% detection accuracy and over 55% bandwidth savings.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">10:15 am–10:50 am</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Break with Refreshments</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">10:50 am–12:05 pm</span></h3></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298659" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Virtualization</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298608" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/pang">Expeditious High-Concurrency MicroVM SnapStart in Persistent Memory with an Augmented Hypervisor</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Xingguo Pang, Yanze Zhang, and Liu Liu, <em>University of Macau;</em> Dazhao Cheng, <em>WuHan University;</em> Chengzhong Xu and Xiaobo Zhou, <em>University of Macau</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The industry has embraced snapshotting to tackle the cold starts and efficiently manage numerous short-lived functions for microservice-native architectures, serverless computing, and machine learning inference. A cutting-edge research approach FaaSnap, while innovative in reducing page faults during on-demand paging through prefetching the profiled working set pages into DRAM, incurs high caching overheads and I/O demands, potentially degrading system efficiency and limiting concurrent MicroVM executions.</p>

<p>This paper introduces PASS, a system leveraging byte-addressable persistent memory (PMEM) for cost-effective and highly concurrent MicroVM SnapStart execution. PASS, functioning as a PMEM-aware augmented hypervisor in the user space, revolutionizes MicroVM memory restoration. It constructs complete address indexing of the guest memory mapped to single-tier PMEM space, enabling zero-copy on-demand paging by exploiting PMEM's direct access feature. This approach bypasses the cache layer and maintains guest OS transparency, avoiding invasive modifications. Experimental results, derived from real-world applications, reveal that PASS substantially decreases SnapStart execution time, achieving up to 72% reduction compared to the Firecracker hypervisor on the PMEM filesystem, and 47% reduction compared to FaaSnap. Moreover, PASS achieves double the maximum concurrency compared to both Firecracker and FaaSnap. It improves the cost-effectiveness by 2.2x and 1.6x over the Firecracker and FaaSnap, respectively.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298610" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/li-chuandong">Taming Hot Bloat Under Virtualization with HUGESCOPE</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Chuandong Li, <em>National Key Laboratory for Multimedia Information Processing, School of CS, Peking University and Zhongguancun Laboratory;</em> Sai Sha, <em>National Key Laboratory for Multimedia Information Processing, School of CS, Peking University and Beijing Huawei Digital Technologies;</em> Yangqing Zeng and Xiran Yang, <em>National Key Laboratory for Multimedia Information Processing, School of CS, Peking University;</em> Yingwei Luo and Xiaolin Wang, <em>National Key Laboratory for Multimedia Information Processing, School of CS, Peking University and Zhongguancun Laboratory;</em> Zhenlin Wang, <em>Michigan Tech;</em> Diyu Zhou, <em>National Key Laboratory for Multimedia Information Processing, School of CS, Peking University and EPFL</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Huge pages are effective in reducing the address translation overhead under virtualization. However, huge pages suffer from the hot bloat problem, where accesses to a huge page are skewed towards a few base pages (i.e., 4KB page), making the hypervisor (mistakenly) classify the whole huge page as hot. Hot bloat renders several critical techniques used in virtualization ineffective, including tiered memory and page sharing. Prior work addressing hot bloat either requires hardware modification or targets a specific scenario and is not applicable to a hypervisor.</p>

<p>This paper presents HugeScope, a lightweight, effective and generic system that addresses the hot bloat problem under virtualization based on commodity hardware. HugeScope includes an efficient and precise page tracking mechanism, leveraging the other level of indirect memory translation in the hypervisor. HugeScope provides a generic framework to support page splitting and coalescing policies, considering the memory pressure, as well as the recency, frequency, and skewness of page access. Moreover, HugeScope is general and modular, thereby can be easily applied to various scenarios concerning hot bloat, including tiered memory management (HS-TMM) and page sharing (HS-Share). Evaluation shows that HugeScope incurs less than 4% overhead, and by addressing hot bloat, HS-TMM improves performance by up to 61% over vTMM while HS-Share saves 41% more memory than Ingens while offering comparable performance.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298612" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/gao-chen">CrossMapping: Harmonizing Memory Consistency in Cross-ISA Binary Translation</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Chen Gao and Xiangwei Meng, <em>Lanzhou University;</em> Wei Li, <em>Tsinghua University;</em> Jinhui Lai, <em>Lanzhou University;</em> Yiran Zhang, <em>Beijing University of Posts and Telecommunications;</em> Fengyuan Ren, <em>Lanzhou University and Tsinghua University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>The increasing prevalence of new Instruction Set Architectures (ISAs) necessitates the migration of closed-source binary programs across ISAs. Dynamic Binary Translation (DBT) stands out as a crucial technology for the cross-ISA emulation of binary programs. However, due to the mismatch in memory consistency between guest ISA and host ISA, DBT systems face substantial challenges in guaranteeing correctness and translation performance for concurrent programs. Despite several attempts to bridge the memory inconsistency between guest and host ISA, prior work is either not universal for cross-ISA DBT systems or inefficient and even error-prone in translation.</p>

<p>This work presents CrossMapping, a general primitive mapping framework to enhance existing DBT systems for cross-ISA translation. By harmonizing memory consistency across diverse ISAs, CrossMapping enables smooth cross-ISA translation and accomplishes correct emulation. CrossMapping introduces specification tables to describe memory models in a unified and precise format, which facilitates the derivation of concurrent primitive mapping schemes based on a convenient comparison and analysis of memory models. The correctness of cross-ISA emulation is guaranteed by harmoniously integrating the derived mapping schemes with existing DBT systems. We evaluate CrossMapping for x86, ARMv8, and RISC-V on top of QEMU using the PARSEC benchmark suite. The results show that the average performance improvement can reach 8.5% when emulating x86 on ARMv8 and 7.3% when emulating x86 on RISC-V.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298660" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Security 2</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298614" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/chai">Efficient Decentralized Federated Singular Vector Decomposition</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Di Chai, Junxue Zhang, Liu Yang, and Yilun Jin, <em>Hong Kong University of Science and Technology;</em> Leye Wang, <em>Peking University;</em> Kai Chen, <em>Hong Kong University of Science and Technology;</em> Qiang Yang, <em>Hong Kong University of Science and Technology and Webank</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Federated singular value decomposition (SVD) is a foundation for many real-world distributed applications. Existing federated SVD studies either require external servers which downgrade privacy protection or leverage homomorphic encryption (HE) to get rid of external servers (<em>e.g.</em>, being decentralized) but suffer from significant inefficiencies caused by extensive computational and communication overhead.</p>

<p>This paper presents Excalibur, an efficient decentralized federated SVD system. At its core, Excalibur proposes a lightweight matrix protection method to reduce the computational degradation caused by cryptographic operations, improving computation performance. Furthermore, it designs a communication-efficient decentralized SVD workflow based on the quantitative analysis of the design space, optimizing communication performance. To validate the efficiency of Excalibur, we implement a fully functional Excalibur system and evaluate it with real-world applications. Our results show that Excalibur not only removes the external servers but also achieves 3.1× ~ 6.0× faster performance than state-of-the-art (SOTA) server-aided method on different shapes of billion-scale data. In addition, Excalibur exhibits &gt; 23000× larger throughput than the SOTA HE-based system.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298616" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xu-he">Models on the Move: Towards Feasible Embedded AI for Intrusion Detection on Vehicular CAN Bus</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>He Xu, Di Wu, Yufeng Lu, and Jiwu Lu, <em>Hunan University and ExponentiAI Innovation;</em> Haibo Zeng, <em>Virginia Tech</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Controller Area Network (CAN) protocol is widely used in vehicles as an efficient standard enabling communication among Electronic Control Units (ECUs). However, the CAN bus is vulnerable to malicious attacks because of a lack of defense features. To achieve efficient and effective intrusion detection system (IDS) design for hardware and embedded system security in vehicles, we have specifically tackled the challenge that existing IDS techniques rarely consider attacks with small-batch. We propose a model with hardware implementation to function in the vehicular CAN bus, namely MULSAM which employing multi-dimensional long short-term memory with the self-attention mechanism. The self-attention mechanism can enhance the characteristics of CAN bus-oriented attack behavior and the multi-dimensional long short-term memory can effectively extract the in-depth features of time series data. The MULSAM model has been compared with other baselines on five attacks generated by extracting benign CAN data from the actual vehicle. Our experimental results demonstrate that MULSAM has the best training stability and detection accuracy (98.98%) to identify small-batch injection attacks. Furthermore, to speed up the inference of MULSAM as an embedded unit in vehicles, hardware accelerator has been implemented on FPGA to achieve a better energy efficiency than other embedded platform. Even with a certain degree of quantification, the acceleration model for MULSAM still presents a high detection accuracy of 98.81% and a low latency of 1.88 ms, leading to a new cyber-physical system security solution towards feasible embedded AI for intrusion detection on vehicular CAN bus.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298618" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/chen-jiahao">CPC: Flexible, Secure, and Efficient CVM Maintenance with Confidential Procedure Calls</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Jiahao Chen, <em>Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University;</em> Zeyu Mi and Yubin Xia, <em>Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China;</em> Haibing Guan, <em>Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University;</em> Haibo Chen, <em>Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Confidential virtual machines (CVMs), while providing strong data privacy for cloud tenants, pose significant challenges to VM maintenance like live migration and snapshotting. Traditional host-based maintenance, while applicable to conventional VMs, is infeasible for CVMs due to the lack of trust in the host and the prevention of mandated intrusive access from the host. State-of-the-art approaches depend on non-trivial modifications to hardware and firmware and thus lead to notable compromises in security and/or performance. Furthermore, such approaches lack flexibility for upgrades and cross-platform compatibility, hindering the popularity of CVMs on the cloud.</p>

<p>In this paper, we introduce Confidential Procedure Calls (CPCs), a flexible approach to the efficient and secure execution of CVM maintenance modules from within the guest. We have implemented prototypes on two leading CVM platforms. Our prototype on AMD SEV showcases the high performance of CPCs, with 3× (resource reclamation) or even 138× (live migration) faster than existing approaches. Our prototype on ARM CCA further confirms CPCs' outstanding security and flexibility.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">12:05 pm–1:40 pm</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Lunch (on your own)</h2>
</div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">1:40 pm–3:20 pm</span></h3></div><div class="field field-name-field-anchor-id field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd"><a class="anchor" name="fripm"></a></div></div></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 1</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298661" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Storage 2</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298620" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/ha">RL-Watchdog: A Fast and Predictable SSD Liveness Watchdog on Storage Systems</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Jin Yong Ha, <em>Seoul National University;</em> Sangjin Lee, <em>Chung-Ang University;</em> Heon Young Yeom, <em>Seoul National University;</em> Yongseok Son, <em>Chung-Ang University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>This paper proposes a reinforcement learning-based watchdog (RLW) that examines solid-state drive (SSD) liveness or failures by faults (e.g., controller/power faults and high temperature) quickly, precisely, and online to minimize application data loss. To do this, we first provide a lightweight watchdog (LWW) to actively and lightly examine SSD liveness by issuing a liveness-dedicated command to the SSD. Second, we introduce a reinforcement learning-based timeout predictor (RLTP) which predicts the timeout of the dedicated command, enabling the detection of a failure point regardless of the SSD model. Finally, we propose fast failure notification (FFN) to immediately notify the applications of the failure to minimize their potential data loss. We implement RLW with three techniques in a Linux kernel 6.0.0 and evaluate it in a single SSD and RAID using realistic power fault injection. The experimental results reveal that RLW reduces the data loss by up to 96.7% compared with the existing scheme, and its accuracy in predicting failure points reaches up to 99.8%.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298622" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/gu-yunfei">Exploit both SMART Attributes and NAND Flash Wear Characteristics to Effectively Forecast SSD-based Storage Failures in Clusters</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yunfei Gu and Chentao Wu, <em>Shanghai Jiao Tong University;</em> Xubin He, <em>Temple University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Solid State Drives (SSDs) based on flash technology are extensively employed as high-performance storage solutions in supercomputing data centers. However, SSD failures are frequent in these environments, resulting in significant performance issues. To ensure the reliability and accessibility of HPC storage systems, it is crucial to predict failures in advance, enabling timely preventive measures. Although many failure prediction methods focus on improving SMART attributes and system telemetry logs, their predictive efficacy is constrained due to the limited capacity of these logs to directly elucidate the root causes of SSD failures at the device level. In this paper, we revisit the underlying causes of SSD failures and first utilize the device-level flash wear characteristics of SSDs as a critical indicator instead of solely relying on SMRAT data. We propose a novel Aging-Aware Pseudo Twin Network (APTN) based SSD failure prediction approach, exploiting both SMART and device-level NAND flash wear characteristics, to effectively forecast SSD failures. In practice, we also adapt APTN to the online learning framework. Our evaluation results demonstrate that APTN improves the F1-score by 51.2% and TPR by 40.1% on average compared to the existing schemes. This highlights the potential of leveraging device-level wear characteristics in conjunction with SMART attributes for more accurate and reliable SSD failure prediction.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298624" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/li-zhiyue">StreamCache: Revisiting Page Cache for File Scanning on Fast Storage Devices</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Zhiyue Li and Guangyan Zhang, <em>Tsinghua University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Buffered I/O via page cache is used for file scanning in many cases as page cache can provide buffering, data aggregation, I/O alignment and prefetching transparently. However, our study indicates that employing page cache for file scanning on fast storage devices presents two performance issues: it offers limited I/O bandwidth that does not align with the performance of fast storage devices, and the intensive background writeback onto fast storage devices can significantly interfere with foreground I/O requests.</p>

<p>In this paper, we propose StreamCache, a new page cache management system for file scanning on fast storage devices. StreamCache exploits three techniques to achieve high I/O performance. First, it uses a lightweight stream tracking method to record the states of cached pages at the granularity of sequential streams. Second, it uses a stream-based page reclaiming method to lower the interference to foreground I/O requests. Third, it uses a two-layer memory management method to accelerate page allocation by leveraging CPU cache locality.</p>

<p>We implement StreamCache in XFS. Experimental results show that compared with existing methods, StreamCache can increase the I/O bandwidth of scientific applications by 44%, and reduce the checkpoint/restart time of large language models by 15.7% on average.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298626" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/tian">Scalable Billion-point Approximate Nearest Neighbor Search Using SmartSSDs</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Bing Tian, Haikun Liu, Zhuohui Duan, Xiaofei Liao, Hai Jin, and Yu Zhang, <em>Huazhong University of Science and Technology</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p><em>Approximate nearest neighbor search</em> (ANNS) in high-dimensional vector spaces has become increasingly crucial in database and machine learning applications. Most previous ANNS algorithms require TB-scale memory to store indices of billion-scale datasets, making their deployment extremely expensive for high-performance search. The emerging SmartSSD technology offers an opportunity to achieve scalable ANNS via <em>near data processing</em> (NDP). However, there remain challenges to directly adopt existing ANNS algorithms on multiple SmartSSDs.</p>

<p>In this paper, we present SmartANNS, a SmartSSD-empowered billion-scale ANNS solution based on a hierarchical indexing methodology. We propose several novel designs to achieve near-linear scaling with multiple SmartSSDs. First, we propose a "host CPUs + SmartSSDs'' cooperative architecture incorporated with hierarchical indices to significantly reduce data accesses and computations on SmartSSDs. Second, we propose dynamic task scheduling based on optimized data layout to achieve both load balancing and data reusing for multiple SmartSSDs. Third, we further propose a learning-based shard pruning algorithm to eliminate unnecessary computations on SmartSSDs. We implement SmartANNS using Samsung’s commercial SmartSSDs. Experimental results show that SmartANNS can improve <em>query per second</em> (QPS) by up to 10.7× compared with the state-of-the-art SmartSSD-based ANNS solution—CSDANNS. Moreover, SmartANNS can achieve near-linear performance scalability for large-scale datasets using multiple SmartSSDs.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div><div class="field-item even"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-track-title field-type-text field-label-hidden"><div class="field-items"><div class="field-item odd">Track 2</div></div></div><div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298662" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Hardware</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298628" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/gu-yicheng">gVulkan: Scalable GPU Pooling for Pixel-Grained Rendering in Ray Tracing</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Yicheng Gu, Yun Wang, Yunfan Sun, Yuxin Xiang, Yufan Jiang, Xuyan Hu, Zhengwei Qi, and Haibing Guan, <em>Shanghai Jiao Tong University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Ray tracing rendering technology enhances scene realism and offers immersive experiences. However, it demands significant computational resources to trace and compute light-object interactions. As a result, traditional local GPU rendering might not meet the demands for high image quality and low latency. Moreover, many applications are tailored to utilize the resources of a single GPU, limiting their capacity to increase computational power through additional GPUs.</p>

<p>This paper presents gVulkan, the first transparent multi-GPU acceleration rendering solution for Vulkan-based ray tracing. To address the bottleneck caused by limited local GPU resources, gVulkan can offload ray tracing rendering to the cloud via API-forwarding. In the cloud, gVulkan employs Split Frame Rendering (SFR) to enable an arbitrary number of GPUs to accelerate rendering in parallel, while dynamically self-rebalancing the workload at a pixel-grained level across GPUs. Experiments demonstrate that gVulkan can accelerate Vulkan-based ray tracing programs in an application-unaware manner. By dynamically rebalancing each GPU's workload, gVulkan achieves good linearity with 3.81× speedup across 4 GPUs on average.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298630" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/chen-jiyang">vFPIO: A Virtual I/O Abstraction for FPGA-accelerated I/O Devices</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Jiyang Chen, Harshavardhan Unnibhavi, Atsushi Koshiba, and Pramod Bhatotia, <em>Technical University of Munich</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Modern cloud systems have adopted a variety of FPGA-accelerated I/O devices, such as SmartNICs and computational storage, while they face programmability and portability challenges. Existing FPGA frameworks either directly expose device-specific I/O interfaces to user logic or offer virtualized I/Os limited to a single device type. The lack of I/O abstraction imposes high engineering costs, less design portability, and even unexpected throughput degradation. </p>

<p>We introduce vFPIO, an FPGA-based I/O acceleration framework that brings better programmability and design portability. vFPIO extends modern FPGA OSes to expose <em>virtual I/O ports</em> to user logic, which abstracts device-dependent I/O specifications and makes the user logic design platform-agnostic. The connectivity between virtual and physical I/O ports can be easily configured by host applications using POSIX-like file APIs. vFPIO also offers a preemptive I/O transaction scheduler that alleviates the I/O throughput degradation caused by concurrent I/O requests from multiple accelerators in a multi-tenant environment.</p>

<p>We implement a prototype of the vFPIO framework on x86 servers equipped with AMD Xilinx Alveo U280 cards. Our prototype supports four different I/O interfaces: PCIe, DRAM, HBM, and network. Our evaluation highlights that vFPIO incurs negligible performance overheads compared to Coyote, one of the latest FPGA OSes, while preserving the maximum I/O throughput for high-priority tasks even under resource contention.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298632" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/peng">ScalaCache: Scalable User-Space Page Cache Management with Software-Hardware Coordination</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Li Peng and Yuda An, <em>Peking University;</em> You Zhou, <em>Huazhong University of Science and Technology;</em> Chenxi Wang, <em>University of Chinese Academy of Sciences;</em> Qiao Li, <em>Xiamen University;</em> Chuanning Cheng, <em>Huawei;</em> Jie Zhang, <em>Peking University and Zhongguancun Laboratory</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Due to the host-centric design principle, the existing page cache management suffers from CPU consumption, communication costs, and garbage collection (GC) interference. To address these challenges, we propose ScalaCache, a scalable user-space page cache with software-hardware coordination. Specifically, to reduce the host CPU overhead, we offload the cache management into computational storage drives (CSDs) and further merge the indirection layers in both the cache and flash firmware, which facilitates lightweight cache management. To further boost scalability, we build a lockless resource management framework that allows multiple CSD internal cores to manage the cache space concurrently. ScalaCache also aggregates the computing power of multiple CSDs to deliver scalable I/O performance. Moreover, ScalaCache reduces communication costs by trimming the I/O control path while mitigating GC interference via a GC-aware replacement policy, thereby enhancing its efficiency and performance stability. Our evaluation results reveal that ScalaCache exhibits 5.12× and 1.70× bandwidth improvements, respectively, compared to kernel page cache and the state-of-the-art user-space one. ScalaCache is open source and available at <a href="https://github.com/ChaseLab-PKU/ScalaCache">https://github.com/ChaseLab-PKU/ScalaCache</a>.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298634" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/xie">Centimani: Enabling Fast AI Accelerator Selection for DNN Training with a Novel Performance Predictor</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Zhen Xie, <em>Binghamton University;</em> Murali Emani, <em>Argonne National Laboratory;</em> Xiaodong Yu, <em>Stevens Institute of Technology;</em> Dingwen Tao, <em>Indiana University;</em> Xin He, <em>Xidian University;</em> Pengfei Su, <em>University of California, Merced;</em> Keren Zhou, <em>George Mason University;</em> Venkatram Vishwanath, <em>Argonne National Laboratory</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>For an extended period, graphics processing units (GPUs) have stood as the exclusive choice for training deep neural network (DNN) models. Over time, to serve the growing demands in a more targeted manner, various artificial intelligence-specific hardware, referred to as <em>AI accelerators</em>, have been vigorously developed, aiming to provide more efficient DNN acceleration solutions. However, sufficient solutions are also heterogeneous and thus introduce complexities in accelerator selection. Given a DNN model and a training objective, such as throughput or price-performance ratio, it remains challenging to arrive at the optimal decision among many options due to high reimplementation costs and unexpected performance.</p>

<p>To tackle this challenge, we propose <em>Centimani</em>, a performance predictor that accurately and rapidly predicts DNN training throughput on various AI accelerators, thereby facilitating the accelerator selection process. To achieve this goal, we first analyze typical AI accelerators and draw observations that abstract AI accelerator designs and guide our performance modeling approach. In particular, we construct a memory estimation model and decoupled performance models to select the most appropriate batch size and predict the execution time of DNN training. We validate our approach by applying Centimani to six common DNN models on four typical AI accelerators. Results show that Centimani predicts the throughput with an average accuracy of 93.1% on single-device training and 90.4% on multiple-device training, thus the optimal accelerator corresponding to the user's training objective can be obtained.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">3:20 pm–3:40 pm</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Break with Refreshments</h2>
<!--<p>[INSERT ROOM]</p>-->
</div></div></div>  </div>
</div>
</div><div class="field-item odd">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">3:40 pm–5:10 pm</span></h3></div><div class="field-collection-container clearfix"><div class="field field-name-field-tracks field-type-field-collection field-label-hidden"><div class="field-items"><div class="field-item odd"><div class="field-collection-view clearfix view-mode-full field-collection-view-final"><div class="entity entity-field-collection-item field-collection-item-field-tracks clearfix">
  <div class="content">
    <div class="field field-name-field-sessions-ref field-type-entityreference field-label-hidden"><div class="field-items"><div class="field-item odd"><article id="node-298663" class="node node-session view-mode-schedule tech-schedule-track-accordion-processed"><p class="tech-schedule-track-accordion-links"><a href="#accordion" class="tech-schedule-track-accordion-toggle is-toggled">Hide details &nbsp;▾</a></p>

                    <h2 class="node-title">Potpourri</h2>
            
  
  <div class="content">
    <div class="field field-name-field-session-papers field-type-node-reference field-label-hidden" style=""><div class="field-items"><div class="field-item odd"><article id="node-298636" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/williams">A Difference World: High-performance, NVM-invariant, Software-only Intermittent Computation</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Harrison Williams, <em>Virginia Tech;</em> Saim Ahmad, <em>Amazon;</em> Matthew Hicks, <em>Virginia Tech</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Supporting long life, high performance, intermittent computation is an essential challenge in allowing energy harvesting devices to fulfill the vision of smart dust. Intermittent computation is the extension of long-running computation across the frequent, unexpected, power cycles that result from replacing batteries with harvested energy. The most promising intermittent computation support strategies combine programmer direction and compiler analysis to minimize run-time overhead and provide programmer control—without specialized hardware support. While such strategies succeed in reducing the size of non-volatile memory writes due to checkpointing, they must checkpoint continuously. Unfortunately, for Flash-based devices (by far the most ubiquitous), writing checkpoints is slow and gradually kills the device. Without intervention, Flash devices and software-only intermittent computation are fundamentally incompatible.</p>

<p>To enable ubiquitous programmer-guided intermittent computation we design and implement Camel. The key idea behind Camel is the systematic bifurcation of program state into two "worlds'' of differing volatility. Programmers compose intermittent programs by stitching together atomic units of computation called tasks. The Camel compiler ensures that all within-task data is placed in the volatile world and all between-task data is placed in the non-volatile world. Between tasks, Camel swaps the worlds, atomically locking-in the forward progress of the preceding task. In preparation for the next task, Camel resolves differences in world view by copying only differences due to the preceding task's updates. This systematic decomposition into a mixed-volatility memory allows—for the first time—long-life and high performance programmer-guided intermittent computation on Flash devices: Camel outperforms the state-of-the-art checkpointing system for Flash-based devices by up to 5x while eliminating the need for hardware support. Beyond Flash, Camel's differential buffer system improves performance by a factor of 2x compared to existing task-based approaches on FRAM platforms.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298638" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/wang-rui">Efficient Large Graph Processing with Chunk-Based Graph Representation Model</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Rui Wang, <em>Zhejiang University and Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security;</em> Weixu Zong, Shuibing He, Xinyu Chen, Zhenxin Li, and Zheng Dang, <em>Zhejiang University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Existing external graph processing systems face challenges in terms of low I/O efficiency, expensive computation overhead, and high graph algorithm development costs when running on emerging NVMe SSDs, due to their reliance on complex loading and computing models that aim to convert numerous random I/Os into a few sequential I/Os. While in-memory graph systems working with memory-storage cache systems like OS page cache or TriCache, offer a promising solution for large graph processing with fine-grained I/Os and easy algorithm programming, they often overlook the specific characteristics of graph applications, resulting in inefficient graph processing. To address these challenges, we introduce ChunkGraph, an I/O-efficient graph system designed for processing large-scale graphs on NVMe SSDs. ChunkGraph introduces a novel chunk-based graph representation model, featuring classified and hierarchical vertex storage, and efficient chunk layout optimization. Evaluations show that ChunkGraph can outperform existing external graph systems, as well as in-memory graph systems relying on general cache systems, running several times faster.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item odd"><article id="node-298640" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/feng-hang">SlimArchive: A Lightweight Architecture for Ethereum Archive Nodes</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>Hang Feng, Yufeng Hu, and Yinghan Kou, <em>Zhejiang University;</em> Runhuai Li and Jianfeng Zhu, <em>BlockSec;</em> Lei Wu and Yajin Zhou, <em>Zhejiang University</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>With the rapid development of Ethereum, archive nodes that record all historical states have become a critical component of the infrastructure. However, current archive nodes suffer enormous storage requirements and poor performance due to the inefficient authenticated Merkle Patricia Trie and coarse-grained state granularity.</p>

<p>This paper presents a lightweight and high-performance architecture for Ethereum archive nodes to address the two limitations mentioned earlier. The core idea of our approach is to maintain compacted, flattened, and fine-grained (i.e., transaction-level) historical states by flattening the minimum state changes of each transaction required for the world state. Our method maintains an archive node with minimum storage requirements while providing high-performance state access. We have implemented a prototype system named SLIMARCHIVE for Ethereum. The evaluation results demonstrate that our approach reduces storage requirements by 98.1%, improves state access throughput by 19.0×, and speeds up transaction execution by an average of 1112.5×, compared to vanilla Geth.</p></div></div></div>  </div>

        
    
</article>
</div><div class="field-item even"><article id="node-298642" class="node node-paper view-mode-schedule tech-schedule-accordion-processed">

                    <h2 class="node-title">
          <a href="/conference/atc24/presentation/hildebrand">Every Mapping Counts in Large Amounts: Folio Accounting</a></h2>
            
  
  <div class="content">
    <div class="required-fields group-text-wrapper field-group-html-element"><div class="field field-name-field-paper-people-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><p>David Hildenbrand, <em>Technical University of Munich and Red Hat GmbH;</em> Martin Schulz, <em>Technical University of Munich;</em> Nadav Amit, <em>Technion, Israel Institute of Technology</em></p></div></div></div><div class="tech-schedule-presentation-accordion-toggle-wrapper"><a href="#accordion" class="tech-schedule-presentation-accordion-toggle">Show details &nbsp;▸</a>&nbsp;&nbsp;</div><div class="required-fields group-schedule-accordion field-group-html-element" style=""><div class="field field-name-field-paper-description-long field-type-text-long field-label-hidden"><p>Operating systems can significantly enhance performance by utilizing large contiguous memory regions, even when the memory is not mapped using huge pages, by streamlining memory management. To harness these advantages, Linux has introduced "folios," representing multiple contiguous pages. Unlike traditional huge pages, folios can be partially mapped, which complicates folio accounting and hinders both performance and memory savings.</p>

<p>Accurate and efficient folio accounting is crucial for optimizing memory management operations, enforcing various memory management policies, and performing Unique Set Size accounting in the operating system. In particular, determining whether a folio is exclusively mapped in a single address space is essential for avoiding unnecessary Copy-On-Write operations when memory is no longer shared.</p>

<p>We introduce a novel tracking scheme to determine, with negligible overhead, whether a folio is exclusively mapped in a single address space. Our solution achieves a memory overhead that grows sublinearly with the number of pages per folio. By implementing our method in Linux, we demonstrate a notable improvement in fork and unmap operations by 1.9x and 4.2x respectively, and in the performance of fork-intensive workloads, such as Redis, achieving up to a 2.2x speedup.</p></div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>

        
    
</article>
</div></div></div>  </div>
</div>
</div></div></div></div></div>  </div>
</div>
</div><div class="field-item even">
<div class="entity entity-paragraphs-item paragraphs-item-conference-schedule-slot  ">
  <div class="content">
    <div class="tech-schedule-sticky-header-wrapper"><h3 class="field field-name-field-time-text field-type-text field-label-hidden sticky-header-processed"><span class="field-item odd first last">5:10 pm–5:20 pm</span></h3></div><div class="field field-name-field-additional-text field-type-text-long field-label-hidden"><div class="field-items"><div class="field-item odd"><h2>Closing Remarks</h2>
<p>Program Co-Chairs: Saurabh Bagchi, <em>Purdue University;</em> Yiying Zhang, <em>University of California, San Diego</em></p></div></div></div>  </div>
</div>
</div></div></div></div>
  </div>

        
    
</article>
</div>
    </section>

  </main>

      <footer id="site-footer" role="contentinfo" class="site-footer">
              <section class="footer-top">
              <section class="block block-usenix-og-auto-menu usenix-og-auto-footer-menu">

  
<div class="block-content">  <ul class="menu"><li class="first expanded"><a href="/conference/atc24#registration">Attend</a><ul class="menu"><li class="first leaf"><a href="/conference/atc24/registration-information">Registration Information</a></li>
<li class="leaf"><a href="/conference/atc24/registration-discounts">Registration Discounts</a></li>
<li class="leaf"><a href="/conference/atc24/grants">Grant Opportunities</a></li>
<li class="last leaf"><a href="/conference/atc24/venue-hotel-and-travel">Venue, Hotel, and Travel</a></li>
</ul></li>
<li class="expanded"><a href="/conference/atc24/glance">Program</a><ul class="menu"><li class="first leaf"><a href="/conference/atc24/glance">Program At a Glance</a></li>
<li class="leaf"><a href="/conference/atc24/technical-sessions" class="active">Technical Sessions</a></li>
<li class="last leaf"><a href="/conference/atc24/activities">Activities</a></li>
</ul></li>
<li class="expanded"><a href="/conference/atc24/call-for-papers">Participate</a><ul class="menu"><li class="first leaf"><a href="/conference/atc24/call-for-papers">Call for Papers</a></li>
<li class="leaf"><a href="/conference/atc24/submission-instructions">Submission Instructions</a></li>
<li class="leaf"><a href="/conference/atc24/call-for-artifacts">Call for Artifacts</a></li>
<li class="last leaf"><a href="/conference/atc24/instructions-presenters">Instructions for Presenters</a></li>
</ul></li>
<li class="expanded"><a href="/conference/atc24#sponsorship">Sponsors</a><ul class="menu"><li class="first last leaf"><a href="/conference/atc24/exhibitor-services">Exhibitor Services</a></li>
</ul></li>
<li class="last expanded"><a href="/conference/atc24#marquee">About</a><ul class="menu"><li class="first leaf"><a href="/conference/atc24#organizers">Conference Organizers</a></li>
<li class="leaf"><a href="https://www.usenix.org/conferences/byname/131">Past Conferences</a></li>
<li class="leaf"><a href="/conferences/values-policies">Conference Policies</a></li>
<li class="leaf"><a href="/conferences/cod">Code of Conduct</a></li>
<li class="last leaf"><a href="/conference/atc24#about">Questions</a></li>
</ul></li>
</ul></div>
</section>        </section>
                    <section class="footer-bottom">
              <section class="block block-usenix-conference og-conference-social-icon">

  
<div class="block-content">  <article id="node-279014" class="node node-conference view-mode-social-icon">

  <div class="content">
    <a href="https://www.linkedin.com/company/usenix-association" target="_blank"><span class="fab fa-linkedin"></span><span class="offscreen">LinkedIn</span></a><a href="https://www.facebook.com/usenixassociation/" target="_blank"><span class="fab fa-facebook-f"></span><span class="offscreen">Facebook</span></a><a href="https://www.youtube.com/user/USENIXAssociation" target="_blank"><span class="fab fa-youtube"></span><span class="offscreen">Youtube</span></a><a href="http://twitter.com/usenix" target="_blank"><span class="fab fa-x-twitter"></span><span class="offscreen">Twitter</span></a><a href="https://infosec.exchange/@usenixassociation" target="_blank"><span class="fab fa-mastodon"></span><span class="offscreen">Mastodon</span></a>  </div>

</article>
</div>
</section>  <section class="block block-menu menu-footer">

  
<div class="block-content">  <ul class="menu"><li class="first leaf"><a href="/privacy-policy">Privacy Policy</a></li>
<li class="last leaf"><a href="/contact">Contact Us</a></li>
</ul></div>
</section>  <section class="block block-block 27">

  
<div class="block-content">  <div style="padding-top: .25em;">
	<p style="color: inherit;">© USENIX <script>new Date().getFullYear()>document.write(new Date().getFullYear());</script>2024</p>
</div></div>
</section>        </section>
          </footer>
  </div>
  <script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"beacon":"bam.nr-data.net","licenseKey":"d823139095","applicationID":"509444","transactionName":"YVJVZksCXkEEVhIMWFgYYkBQTBodDFsCAE8YR19C","queueTime":0,"applicationTime":1597,"atts":"TRVWEAMYTU8=","errorBeacon":"bam.nr-data.net","agent":""}</script>

<iframe name="__uspapiLocator" tabindex="-1" role="presentation" aria-hidden="true" title="Blank" style="display: none; position: absolute; width: 1px; height: 1px; top: -9999px;"></iframe><iframe name="__privateStripeMetricsController1280" frameborder="0" allowtransparency="true" scrolling="no" role="presentation" allow="payment *" src="https://js.stripe.com/v3/m-outer-3437aaddcdf6922d623e172c2d6f9278.html#url=https%3A%2F%2Fwww.usenix.org%2Fconference%2Fatc24%2Ftechnical-sessions&amp;title=USENIX%20ATC%20'24%20Technical%20Sessions%20%7C%20USENIX&amp;referrer=&amp;muid=NA&amp;sid=NA&amp;version=6&amp;preview=false" aria-hidden="true" tabindex="-1" style="border: none !important; margin: 0px !important; padding: 0px !important; width: 1px !important; min-width: 100% !important; overflow: hidden !important; display: block !important; visibility: hidden !important; position: fixed !important; height: 1px !important; pointer-events: none !important; user-select: none !important;"></iframe><iframe tabindex="-1" role="presentation" aria-hidden="true" title="Blank" src="https://consentcdn.cookiebot.com/sdk/bc-v4.min.html" style="position: absolute; width: 1px; height: 1px; top: -9999px;"></iframe></body></html>