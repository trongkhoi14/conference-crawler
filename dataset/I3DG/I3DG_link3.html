<!DOCTYPE html><html><head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="https://i3dsymposium.org/2024/favicon.ico">

    <title>Papers Program | I3D 2024</title>
    <meta name="description" content="ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games">

    <link rel="canonical" href="https://i3dsymposium.org/2024/papers.html">

    
    <meta name="author" content="I3D Publicity Chair (general@i3dsymposium.org)">

    <link rel="alternate" type="application/atom+xml" href="https://i3dsymposium.org/news-feed.xml" title="I3D Conference News">

    

    <link href="css/i3d.css" rel="stylesheet" media="screen">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!-- SEO metadata for social posts -->
    <meta property="og:url" content="https://i3dsymposium.org/2024/papers.html">
    <meta property="og:title" content="Papers Program | I3D 2024">
    <meta property="og:description" content="ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games">
    <meta property="og:image" content="https://i3dsymposium.org/2024/img/social-card.png">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="https://i3dsymposium.org/img/logo-square-bg.png">
    <meta name="twitter:site" content="@I3DCONF">
    <link rel="me" href="https://mastodon.acm.org/@i3d">


</head>

<body>
    <header>
        <a href="index.html">
            <img id="banner" src="img/banner.png" alt="I3D 2024">
        </a>
        <nav id="nav-menu">
            <input type="checkbox" id="nav-menu-open">
            <label for="nav-menu-open">
                <span class="hamburger"></span>
            </label>
            <span id="nav-menu-open-bg"></span>
            <ul id="nav-list">
                <li>
                            <a href="index.html">Home</a>
                        </li>
                    
                        <li class="submenu"><span class="submenu-title">Attend</span>
                            <ul>
                        <li>
                                <a href="registration.html">Register</a>
                            </li>
                        <li>
                                <a href="venue.html">Venue</a>
                            </li>
                        <li>
                                <a href="http://www.acm.org/special-interest-groups/volunteer-resources/officers-manual/policy-against-discrimination-and-harassment" target="_blank">Code of Conduct</a>
                            </li>
                        
                            </ul>
                        </li>
                        <li class="submenu sub-active"><span class="submenu-title">Program</span>
                            <ul>
                        <li>
                                <a href="program.html">Schedule</a>
                            </li>
                        <li class="active">
                                <a href="papers.html">Papers</a>
                            </li>
                        <li>
                                <a href="posters.html">Posters</a>
                            </li>
                        <li>
                                <a href="keynotes.html">Keynotes</a>
                            </li>
                        <li>
                                <a href="sponsored-talks.html">Sponsored Talks</a>
                            </li>
                        <li>
                                <a href="awards.html">Awards</a>
                            </li>
                        
                            </ul>
                        </li>
                        <li class="submenu"><span class="submenu-title">Participate</span>
                            <ul>
                        <li>
                                <a href="cfp.html">Paper Submissions</a>
                            </li>
                        <li>
                                <a href="cfp-posters.html">Poster Submissions</a>
                            </li>
                        
                            </ul>
                        </li>
                        <li class="submenu"><span class="submenu-title">Organization</span>
                            <ul>
                        <li>
                                <a href="contact.html">Contact Us</a>
                            </li>
                        <li>
                                <a href="committee.html">Committee</a>
                            </li>
                        
                            </ul>
                        </li>
            </ul>
        </nav>
        <div id="page-title">
            <h1>Papers Program</h1>
            <div id="conference-info-card">Cesium, Philadelphia, PA, USA<br>8-10 May 2024</div>
        </div>
    </header>

    <div class="clearfix"></div>

    <div id="main-wrapper">
		
        <main id="main">
            

            <ul id="markdown-toc">
  <li><a href="#interaction-and-vr" id="markdown-toc-interaction-and-vr">Papers 1: Interaction and VR</a></li>
  <li><a href="#light-transport-and-storage" id="markdown-toc-light-transport-and-storage">Papers 2: Light Transport and Storage</a></li>
  <li><a href="#volumes-and-fields" id="markdown-toc-volumes-and-fields">Papers 3: Volumes and Fields</a></li>
  <li><a href="#points-and-splats" id="markdown-toc-points-and-splats">Papers 4: Points and Splats</a></li>
  <li><a href="#noise-and-reconstruction" id="markdown-toc-noise-and-reconstruction">Papers 5: Noise and Reconstruction</a></li>
  <li><a href="#efficient-forward-and-differentiable-rendering" id="markdown-toc-efficient-forward-and-differentiable-rendering">Papers 6: Efficient Forward and Differentiable Rendering</a></li>
  <li><a href="#learning-to-move" id="markdown-toc-learning-to-move">Papers 7: Learning to Move</a></li>
  <li><a href="#solvers-and-simulation" id="markdown-toc-solvers-and-simulation">Papers 8: Solvers and Simulation</a></li>
</ul>

<p>Over the past three decades, the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games has showcased exceptional progress from academic and industrial research covering all aspects of interactive computer graphics.</p>

<p>This year, we continue a track record of excellence with 23 high-quality papers selected by the international paper committee for publication and presentation at the conference.</p>

<p>Conference papers have been published in <a href="https://dl.acm.org/toc/pacmcgit/2024/7/1" target="_blank">PACM CGIT</a>. We will also publish links to author versions of the papers as they provide them. Refresh this page periodically, or use <a href="https://www.hongkiat.com/blog/detect-website-change-notification/" target="_blank">a web page monitoring tool</a>, to check this page for updates. Also, in the meantime, you can check Ke-Sen’s page where they are also collecting material for these papers:<br>
<a href="https://www.realtimerendering.com/kesen/i3d2024Papers.htm">https://www.realtimerendering.com/kesen/i3d2024Papers.htm</a></p>

<h5 id="invited-papers">Invited papers</h5>
<p>The program also includes 3 papers originally published in the <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">IEEE Transactions on Visualization and Computer Graphics</a> (TVCG) that will be presented during the conference.</p>

<h2 id="interaction-and-vr">Papers 1: Interaction and VR</h2>

<ul class="unstyled">
  <li>Session chair: Omer Shapira</li>
</ul>

<dl class="papers-list">
  <dt>Collaborating with my Doppelgänger: The Effects of Self-similar Appearance and Voice of a Virtual Character during a Jigsaw Puzzle Co-solving Task</dt>
  <dd>Siqi Guo, Minsoo Choi, Dominic Kao and Christos Mousas</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651288">DOI link</a>, <a href="https://web.ics.purdue.edu/~cmousas/papers/jour24-PACMCGIT-SelfSimiarAppearanceVoice.pdf">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>The research community has long been interested in human interaction with embodied virtual characters in virtual reality (VR). At the same time, interaction with self-similar virtual characters, or virtual doppelgängers, has become a prominent topic in both VR and psychology due to the intriguing psychological effects these characters can have on people. However, studies on human interaction with self-similar virtual characters are still limited.</p>
  
<p>To address this research gap, we designed and conducted a 2 (appearance: self-similar vs. non-self-similar appearance) x 2 (voice: self-similar vs. non-self-similar voice) within-group study (N=25) to explore how combinations of appearance and voice factors influence participants' perception of virtual characters. During the study, we asked participants to collaborate with a virtual character in solving a VR jigsaw puzzle. After each experimental condition, we had participants complete a survey about their experiences with the virtual character.</p>
  
<p>Our findings showed that 1) the virtual characters' self-similarity in appearance enhanced the sense of co-presence and perceived intelligence, but it also elicited higher eeriness; 2) the self-similar voices led to higher ratings on the characters' likability and believability; however, they also induced a more eerie sensation; and 3) we observed an interaction effect between appearance and voice factors for ratings on believability, where the virtual characters were considered more believable when their self-similarity in appearance matched that of their voices. This study provided valuable insights and comprehensive guidance for creating novel collaborative experiences with self-similar virtual characters in immersive environments.</p>
  
<img src="img/paper_thumbnails/01_01_guo.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Perceptions of Hybrid Lighting for Virtual Reality</dt>
  <dd>Martin Dinkov, Sumanta Pattanaik and Ryan McMahan</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651292">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Virtual reality (VR) applications are computationally demanding due to high frame rate requirements, which precludes large numbers of realtime lights from being used.</p>
  
<p>In this paper, we explore a hybrid lighting approach that combines the benefits of realtime and baked lights based on the importance of each source. First, we demonstrate that the hybrid approach affords better frames per second than realtime and mixed lighting (which combines the direct qualities of realtime and the indirect qualities of baked lighting). We then present the results of an online paired-comparison study, in which participants (n=60) compared videos of the four lighting conditions (baked, mixed, realtime, and hybrid) in terms of preference.</p>
  
<p>Our results indicate that the hybrid lighting approach is better than realtime lighting in terms of graphical performance and also yields better perceptions of quality for scenes with a small to moderately large number of lights.</p>
  
<img src="img/paper_thumbnails/01_02_dinkov.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Skill-Based Matchmaking for Competitive Two-Player Games</dt>
  <dd>Cem Yuksel</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651303">DOI link</a>, <a href="http://www.cemyuksel.com/research/matchmaking/">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Skill-based matchmaking is a crucial component of competitive multiplayer games and it is directly tied to how the players would enjoy the game.</p>
  
<p>We present a simple matchmaking algorithm that aims to achieve a target win rate for all players, making long win/loss streaks less probable. It is based on the estimated rating of players. Therefore, we also present a raking estimation for players that does not require any game-specific information and purely relies on game outcomes. Our evaluation shows that our methods are effective in estimating a player’s rating, responding to changes in rating, and achieving a desirable win rate that avoids long win/loss streaks in competitive two-player games.</p>
  
<img src="img/paper_thumbnails/01_03_yuksel.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Impact of Tutorial Modes with Different Time Flow Rates in Virtual Reality Games</dt>
  <dd>Boyuan Chen, Xinan Yan, Xuning Hu, Dominic Kao and Hai-Ning Liang</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651296">DOI link</a>, <a href="https://www.researchgate.net/publication/379638881_Impact_of_Tutorial_Modes_with_Different_Time_Flow_Rates_in_Virtual_Reality_Games">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>The disparities between virtual reality (VR) technology and traditional media make VR games to adopt innovative interaction modes and tutorial methodologies. Existing research in related fields predominantly concentrates on the performance of VR tutorial modes, such as the placement of text and diagrams within tutorial content. However, few studies have delved into other attributes of tutorials. This study categorizes 4 VR game tutorial modes based on the time flow: traditional instruction screen, slow motion, bullet time, and context-sensitive mode.</p>
  
<p>This paper evaluates the impact of these 4 VR game tutorial modes with varying time flow rates on control learnability, engagement-related outcomes, and player performance. We conducted a between-subjects experiment with 59 participants. Results indicated that bullet time significantly enhanced control learnability, reduced cognitive load, and improved player performance when compared to other tutorial modes.</p>
  
<p>Our research contributes to a more comprehensive understanding of VR game tutorials and offers practical guidance for game designers, underscoring the potential of bullet time to enhance learnability and game experience.</p>
  
<img src="img/paper_thumbnails/01_04_chen.jpg" class="paper-thumbnail">
</details>
  </dd>
</dl>

<hr>

<h2 id="light-transport-and-storage">Papers 2: Light Transport and Storage</h2>

<ul class="unstyled">
  <li>Session chair: William Donnelly</li>
</ul>

<dl class="papers-list">
  <dt>Bounded VNDF Sampling for the Smith–GGX BRDF</dt>
  <dd>Yusuke Tokuyoshi and Kenta Eto</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651291">DOI link</a>, <a href="https://gpuopen.com/advanced-rendering-research/">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Sampling according to a visible normal distribution function (VNDF) is often used to sample rays scattered by glossy surfaces, such as the Smith–GGX microfacet model. However, for rough reflections, existing VNDF sampling methods can generate undesirable reflection vectors occluded by the surface. Since these occluded reflection vectors must be rejected, VNDF sampling is inefficient for rough reflections.</p>
  
<p>This paper introduces an unbiased method to reduce the number of rejected samples for Smith–GGX VNDF sampling. Our method limits the sampling range for a state-of-the-art VNDF sampling method that uses a spherical cap-based sampling range. By using our method, we can reduce the variance for highly rough and low-anisotropy surfaces. Since our method only modifies the spherical cap range in the existing sampling routine, it is simple and easy to implement.</p>
  
<img src="img/paper_thumbnails/02_01_tokuyoshi.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>ZH3: Quadratic Zonal Harmonics</dt>
  <dd>Thomas Roughton, Peter-Pike Sloan, Ari Silvennoinen, Michal Iwanicki and Peter Shirley</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651294">DOI link</a>, <a href="http://torust.me/ZH3.pdf">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Spherical Harmonics (SH) have been used widely to represent lighting in games and film. While the quadratic (SH3) and higher order spherical harmonics represent irradiance well, they are expensive to store and evaluate, requiring 27 coefficients per sample. Linear SH (SH2), requiring only 12 coefficients, are sometimes used, but they do not represent irradiance signals accurately and can have challenges with negative reconstruction.</p>
  
<p>We introduce a new representation (ZH3) that augments linear SH with just the zonal coefficient of quadratic SH, yielding significant visual improvement with just 15 coefficients, and discuss how solving for a luminance zonal axis can significantly improve reconstruction accuracy and reduce color artifacts. We also discuss how, rather than storing the ZH3 coefficients explicitly, we can hallucinate them from the linear SH, improving reconstruction accuracy over linear SH at minimal extra cost.</p>
  
<img src="img/paper_thumbnails/02_02_roughton.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Interactive Rendering of Caustics using Dimension Reduction for Manifold Next-Event Estimation</dt>
  <dd>Ana Granizo-Hidalgo and Nicolas Holzschuch</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651297">DOI link</a>, <a href="https://inria.hal.science/hal-04561024">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Specular surfaces, like water surfaces, create caustics by focusing the light being refracted or reflected. These caustics are very important for scene realism, but also challenging to render: to compute them, we need to find the exact path connecting two points through a specular reflection or refraction. This requires finding the roots of a complicated function on the surface. Manifold-Exploration methods find these roots using the Newton-Raphson method, but this involves computing path derivatives at each step, which can be challenging.</p>
  
<p>We show that these roots lie on a curve on the surface, which reduces the dimensionality of the search. This dimension reduction greatly improves the search, allowing for interactive rendering of caustics. It also makes implementation easier, as we do not need to compute path derivatives.</p>
  
<img src="img/paper_thumbnails/02_03_granizo-hidalgo.jpg" class="paper-thumbnail">
</details>
  </dd>
</dl>

<hr>

<h2 id="volumes-and-fields">Papers 3: Volumes and Fields</h2>

<ul class="unstyled">
  <li>Session chair: Ricardo Marroquim</li>
</ul>

<dl class="papers-list">
  <dt>ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context</dt>
  <dd>Binglun Wang, Niladri Shekhar Dutt and Niloy Mitra</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651290">DOI link</a>, <a href="https://proteusnerf.github.io">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Neural Radiance Fields (NeRFs) have recently emerged as a popular option for photo-realistic object capture due to their ability to faithfully capture high-fidelity volumetric content even from handheld video input. Although much research has been devoted to efficient optimization leading to real-time training and rendering, options for interactive editing NeRFs remain limited.</p>
  
<p>We present a very simple but effective neural network architecture that is fast and efficient while maintaining a low memory footprint. This architecture can be incrementally guided through user-friendly image-based edits. Our representation allows straightforward object selection via semantic feature distillation at the training stage. More importantly, we propose a local 3D-aware image context to facilitate view-consistent image editing that can then be distilled into fine-tuned NeRFs, via geometric and appearance adjustments. We evaluate our setup on a variety of examples to demonstrate appearance and geometric edits and report 10-30× speedup over concurrent work focusing on text-guided NeRF editing.</p>
  
<img src="img/paper_thumbnails/03_01_wang.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>FaceFolds: Meshed Radiance Manifolds for Efficient Volumetric Rendering of Dynamic Faces</dt>
  <dd>Safa Medin, Gengyan Li, Ruofei Du, Stephan Garbin, Philip Davidson, Gregory Wornell, Thabo Beeler and Abhimitra Meka</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651304">DOI link</a>, <a href="https://syntec-research.github.io/FaceFolds/">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>3D rendering of dynamic face captures is a challenging problem, and it demands improvements on several fronts—photorealism, efficiency, compatibility, and configurability.</p>
  
<p>We present a novel representation that enables high-quality volumetric rendering of an actor’s dynamic facial performances with minimal compute and memory footprint. It runs natively on commodity graphics soft- and hardware, and allows for a graceful trade-off between quality and efficiency. Our method utilizes recent advances in neural rendering, particularly learning discrete radiance manifolds to sparsely sample the scene to model volumetric effects. We achieve efficient modeling by learning a single set of manifolds for the entire dynamic sequence, while implicitly modeling appearance changes as temporal canonical texture. We export a single layered mesh and view-independent RGBA texture video that is compatible with legacy graphics renderers without additional ML integration. We demonstrate our method by rendering dynamic face captures of real actors in a game engine, at comparable photorealism to state-of-the-art neural rendering techniques at previously unseen frame rates.</p>
  
<img src="img/paper_thumbnails/03_02_medin.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Efficient Visibility Approximation for Game AI using Neural Omnidirectional Distance Fields</dt>
  <dd>Zhi Ying, Nicholas Edwards and Mikhail Kutuzov</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651289">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Visibility information is critical in game AI applications, but the computational cost of raycasting-based methods poses a challenge for real-time systems.</p>
  
<p>To address this challenge, we propose a novel method that represents a partitioned game scene as neural Omnidirectional Distance Fields (ODFs), allowing scalable and efficient visibility approximation between positions without raycasting. For each position of interest, we map its omnidirectional distance data from the spherical surface onto a UV plane. We then use multi-resolution grids and bilinearly interpolated features to encode directions. This allows us to use a compact multi-layer perceptron (MLP) to reconstruct the high-frequency directional distance data at these positions, ensuring fast inference speed. We demonstrate the effectiveness of our method through offline experiments and in-game evaluation.</p>
  
<p>For in-game evaluation, we conduct a side-by-side comparison with raycasting-based visibility tests in three different scenes. Using a compact MLP (128 neurons and 2 layers), our method achieves an average cold start speedup of 9.35 times and warm start speedup of 4.8 times across these scenes. In addition, unlike the raycasting-based method, whose evaluation time is affected by the characteristics of the scenes, our method's evaluation time remains constant.</p>
  
<img src="img/paper_thumbnails/03_03_ying.jpg" class="paper-thumbnail">
</details>
  </dd>
</dl>

<hr>

<h2 id="points-and-splats">Papers 4: Points and Splats</h2>

<ul class="unstyled">
  <li>Session chair: Peter Shirley</li>
</ul>

<dl class="papers-list">
  <dt>Reducing the Memory Footprint of 3D Gaussian Splatting</dt>
  <dd>Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin and George Drettakis</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651282">DOI link</a>, <a href="https://repo-sam.inria.fr/fungraph/reduced_3dgs/">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>3D Gaussian splatting provides excellent visual quality for novel view synthesis, with fast training and real-time rendering; unfortunately, the memory requirements of this method are unreasonably high. We first analyze the reasons for this, identifying three main areas where storage can be reduced: the number of 3D Gaussian primitives used to represent a scene, the number of coefficients for the spherical harmonics used to represent directional radiance and the precision required to store Gaussian primitive attributes.</p>
  
<p>We present a solution to each of these issues. First, we propose an efficient, resolution-aware primitive pruning approach, reducing primitive count by half. Second, we introduce an adaptive adjustment method to choose the number of coefficients used to represent directional radiance for each Gaussian primitive, and finally a codebook-based quantization method, together with a half-float representation for further memory reduction. Taken together, these three components result in a ×27 reduction in overall memory usage on the standard datasets we tested, along with a ×1.7 speedup in rendering speed. We demonstrate our method on standard datasets, and show how our solution results in significantly reduced download times when using the method on a mobile device.</p>
  
<img src="img/paper_thumbnails/04_01_papantonakis.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>SimLOD: Simultaneous LOD Generation and Rendering for Point Clouds</dt>
  <dd>Markus Schütz, Lukas Herzberger and Michael Wimmer</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651287">DOI link</a>, <a href="https://arxiv.org/abs/2310.03567">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>We propose an incremental LOD generation approach for point clouds that allows us to simultaneously load points from disk, update an octree-based level-of-detail representation, and render the intermediate results in real time while additional points are still being loaded from disk. LOD construction and rendering are both implemented in CUDA and share the GPU’s processing power, but each incremental update is lightweight enough to leave enough time to maintain real-time frame rates.</p>
  
<p>Background: LOD construction is typically implemented as a preprocessing step that requires users to wait before they are able to view the results in real time. This approach allows users to view intermediate results right away.  Results: Our approach is able to stream points from an SSD and update the octree on the GPU at rates of up to 580 million points per second (~9.3GB/s from a PCIe 5.0 SSD) on an RTX 4090. Depending on the data set, our approach spends an average of about 1 to 2 ms to incrementally insert 1 million points into the octree, allowing us to insert several million points per frame into the LOD structure and render the intermediate results within the same frame.</p>
  
<p>Discussion/Limitations: We aim to provide near-instant, real-time visualization of large data sets without preprocessing. Out-of-core processing of arbitrarily large data sets and color-filtering for higher-quality LODs are subject to future work.</p>
  
<img src="img/paper_thumbnails/04_02_schutz.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images</dt>
  <dd>Wenbo Chen and Ligang Liu</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651301">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Novel view synthesis has undergone a revolution thanks to the radiance field method. The introduction of 3D Gaussian splatting (3DGS) has successfully addressed the issues of prolonged training times and slow rendering speeds associated with Neural Radiance Field (NeRF), all while preserving the quality of reconstructions. However, 3DGS remains heavily reliant on the quality of input images and their initial camera pose initialization. In cases where input images are blurred, the reconstruction results suffer from blurriness and artifacts.</p>
  
<p>In this paper, we propose the Deblur-GS method for reconstructing 3D Gaussian points to create a sharp radiance field from a camera motion blurred image set. We model the problem of motion blur as a joint optimization challenge involving camera trajectory estimation and time sampling. We cohesively optimize the parameters of the Gaussian points and the camera trajectory during the shutter time. Deblur-GS consistently achieves superior performance and rendering quality when compared to previous methods, as demonstrated in evaluations conducted on both synthetic and real datasets.</p>
  
<img src="img/paper_thumbnails/04_03_chen.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Light Field Display Point Rendering</dt>
  <dd>Ajinkya Gavane and Benjamin Watson</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651300">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Rendering for light field displays (LFDs) requires rendering of dozens or hundreds of views, which must then be combined into a single image on the display, making real-time LFD rendering extremely difficult.</p>
  
<p>We introduce light field display point rendering (LFDPR), which meets these challenges by improving eye-based point rendering [12] with texture-based splatting, which avoids oversampling of triangles mapped to only a few texels; and with LFD-biased sampling, which adjusts horizontal and vertical triangle sampling to match the sampling of the LFD itself. To improve image quality, we introduce multiview mipmapping, which reduces texture aliasing even though compute shaders do not support hardware mipmapping. We also introduce angular supersampling and reconstruction to combat LFD view aliasing and crosstalk. The resulting LFDPR is 2-8× times faster than multiview rendering, with similar comparable quality.</p>
  
<img src="img/paper_thumbnails/04_04_gavane.jpg" class="paper-thumbnail">
</details>
  </dd>
</dl>

<hr>

<h2 id="noise-and-reconstruction">Papers 5: Noise and Reconstruction</h2>

<ul class="unstyled">
  <li>Session chair: Eric Haines</li>
</ul>

<dl class="papers-list">
  <dt>FAST: Filter-Adapted Spatio-Temporal Sampling for Real-Time Rendering</dt>
  <dd>William Donnelly, Alan Wolfe, Judith Bütepage and Jon Valdés</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651283">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Stochastic sampling techniques are ubiquitous in real-time rendering, where performance constraints force the use of low sample counts, leading to noisy intermediate results. To remove this noise, the post-processing step of temporal and spatial denoising is an integral part of the real-time graphics pipeline.</p>
  
<p>The main insight presented in this paper is that we can optimize the samples used in stochastic sampling such that the post-processing error is minimized. The core of our method is an analytical loss function which measures post-filtering error for a class of integrands — multidimensional Heaviside functions. These integrands are an approximation of the discontinuous functions commonly found in rendering. Our analysis applies to arbitrary spatial and spatiotemporal filters, scalar and vector sample values, and uniform and non-uniform probability distributions. We show that the spectrum of Monte Carlo noise resulting from our sampling method is adapted to the shape of the filter, resulting in less noisy final images. We demonstrate improvements over state-of-the-art sampling methods in three representative rendering tasks: ambient occlusion, volumetric ray-marching, and color image dithering. Common use noise textures, and noise generation code is available in supplemental material.</p>
  
<img src="img/paper_thumbnails/05_01_donnelly.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Filtering After Shading With Stochastic Texture Filtering</dt>
  <dd>Matt Pharr, Bartlomiej Wronski, Marco Salvi and Marcos Fajardo</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651293">DOI link</a>, <a href="https://research.nvidia.com/labs/rtr/publication/pharr2024stochtex/">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>2D texture maps and 3D voxel arrays are widely used to add rich detail to the surfaces and volumes of rendered scenes, and filtered texture lookups are integral to producing high-quality imagery.</p>
  
<p>We show that applying the texture filter after evaluating shading generally gives more accurate imagery than filtering textures before BSDF evaluation, as is current practice. These benefits are not merely theoretical, but are apparent in common cases. We demonstrate that practical and efficient filtering after shading is possible through the use of stochastic sampling of texture filters.</p>
  
<p>Stochastic texture filtering offers additional benefits, including efficient implementation of high-quality texture filters and efficient filtering of textures stored in compressed and sparse data structures, including neural representations. We demonstrate applications in both real-time and offline rendering and show that the additional error from stochastic filtering is minimal. We find that this error is handled well by either spatiotemporal denoising or moderate pixel sampling rates.</p>
  
<img src="img/paper_thumbnails/05_02_pharr.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>A Fast GPU Schedule For À-Trous Wavelet-Based Denoisers</dt>
  <dd>Reiner Dolp, Johannes Hanika and Carsten Dachsbacher</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651299">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Given limitations of contemporary graphics hardware, real-time ray-traced global illumination is only estimated using a few samples per pixel. This consequently causes stochastic noise in the resulting frame sequences which requires wide filter support during denoising for temporally stable estimates. The edge avoiding à-trous wavelet transform amortizes runtime cost by hierarchical filtering using a constant number of increasingly dilated tabs in each iteration. While the number of taps stays constant, the runtime of each iteration increases in these usually memory-throughput bound shaders with increasing dilation.</p>
  
<p>We present a scheduling approach that optimizes usage of the memory subsystem through permutation of global invocation indices and array index access functions. In contrast to prior approaches, our method has identical performance charateristics for each iteration, effectively decreasing maintenance cost and improving performance predictability. Furthermore, we are able to leverage on-chip memory and hardware texture interpolation. Our permutation strategy is trivial to integrate into existing wavelet filters as a permutation before and after each level of the wavelet filter. We achieve speedups between 1.3 and 3.8 for usual wavelet configurations in Monte Carlo denoising and computational photography.</p>
  
<img src="img/paper_thumbnails/05_03_dolp.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Cone-Traced Supersampling for Signed Distance Field Rendering</dt>
  <dd>Andrei Chubarau, Yangyang Zhao, Ruby Rao, Derek Nowrouzezahrai and Paul G. Kry</dd>
  <dd>(invited TVCG paper presentation) <a href="https://ieeexplore.ieee.org/document/10360320">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>While signed distance fields (SDFs) in theory offer infinite level of detail, they are typically rendered using the sphere tracing algorithm at finite resolutions, which causes the common rasterized image synthesis problem of aliasing. Most existing optimized antialiasing solutions rely on polygon mesh representations; SDF-based geometry can only be directly antialiased with the computationally expensive supersampling or with post-processing filters that may produce undesirable blurriness and ghosting.</p>
  
<p>In this work, we present cone-traced supersampling (CTSS), an efficient and robust spatial antialiasing solution that naturally complements the sphere tracing algorithm, does not require casting additional rays per pixel or offline prefiltering, and can be easily implemented in existing real-time SDF renderers. CTSS performs supersampling along the traced ray near surfaces with partial visibility – object contours – identified by evaluating cone intersections within a pixel's view frustum.</p>
  
<p>We further introduce subpixel edge reconstruction (SER), a technique that extends CTSS to locate and resolve complex pixels with geometric edges in relatively flat regions, which are otherwise undetected by cone intersections. Our combined solution relies on a specialized sampling strategy to minimize the number of shading computations and correlates sample visibility to aggregate the samples. With comparable antialiasing quality at significantly lower computational cost, CTSS is a reliable practical alternative to conventional supersampling.</p>
</details>
  </dd>
</dl>

<hr>

<h2 id="efficient-forward-and-differentiable-rendering">Papers 6: Efficient Forward and Differentiable Rendering</h2>

<ul class="unstyled">
  <li>Session chair: Michael Vance</li>
</ul>

<dl class="papers-list">
  <dt>Efficient Particle-Based Fluid Surface Reconstruction Using Mesh Shaders and Bidirectional Two-Level Grids</dt>
  <dd>Yuki Nishidate and Issei Fujishiro</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651285">DOI link</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>In this paper, we introduce a novel method for particle-based fluid surface reconstruction that incorporates mesh shaders for the first time.</p>
  
<p>This approach eliminates the need to store triangle meshes in the GPU's global memory, resulting in a significant memory size reduction.</p>
  
<p>Furthermore, our method employs a bidirectional two-level uniform grid, which not only accelerates the computationally expensive stage of surface cell detection but also effectively addresses the issue of vertex overflow among the mesh shaders.</p>
  
<p>Experimental results proved that our method outperforms the state-of-the-art method, achieving both acceleration and memory reduction simultaneously, without sacrificing quality.</p>
  
<p>The method is highly practical, with no major limitations other than requiring a GPU that supports the mesh shaders.</p>
  
<img src="img/paper_thumbnails/06_01_nishidate.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>ShaderPerFormer: Platform-independent Context-aware Shader Performance Predictor</dt>
  <dd>Zitan Liu, Yikai Huang and Ligang Liu</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651295">DOI link</a>, <a href="https://github.com/libreliu/ShaderPerFormer/blob/main/ShaderPerFormer.pdf">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>The ability to model and predict execution time of GPU computations is crucial for real-time graphics application development and optimization. While there are many existing methodologies for graphics programmers to provide such estimates, those methods are often vendor-dependent, require the platforms to be tested, or fail to capture the contextual influences among shader instructions.</p>
  
<p>To address this challenge, we propose ShaderPerFormer, a platform-independent, context-aware deep-learning approach to model GPU performance and provide end-to-end performance predictions on a per-shader basis. In order to provide more accurate predictions, our method contains a separate stage to gather platform-independent shader program trace information.</p>
  
<p>We also provide a dataset consisting of a total of 54,667 fragment shader performance samples on 5 different platforms. Compared to the PILR and SH baseline methods, our approach reduces the average MAPE across five platforms by 8.26% and 25.25%, respectively.</p>
  
<img src="img/paper_thumbnails/06_02_liu.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Transforming a Non-Differentiable Rasterizer into a Differentiable One with Stochastic Gradient Estimation</dt>
  <dd>Thomas Deliot, Eric Heitz and Laurent Belcour</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651298">DOI link</a>, <a href="https://ggx-research.github.io/publication/2024/04/17/publication-intel-diffrast.html">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>We show how to transform a non-differentiable rasterizer into a differentiable one with minimal engineering efforts and no external dependencies (no Pytorch/Tensorflow). We rely on Stochastic Gradient Estimation, a technique that consists of rasterizing after randomly perturbing the scene's parameters such that their gradient can be stochastically estimated and descended. This method is simple and robust but does not scale in dimensionality (number of scene parameters).</p>
  
<p>Our insight is that the number of parameters contributing to a given rasterized pixel is bounded. Estimating and averaging gradients on a per-pixel basis hence bounds the dimensionality of the underlying optimization problem and makes the method scalable. Furthermore, it is simple to track per-pixel contributing parameters by rasterizing ID- and UV-buffers, which are trivial additions to a rasterization engine if not already available. With these minor modifications, we obtain an in-engine optimizer for 3D assets with millions of geometry and texture parameters.</p>
  
<img src="img/paper_thumbnails/06_03_deliot.jpg" class="paper-thumbnail">
</details>
  </dd>
</dl>

<hr>

<h2 id="learning-to-move">Papers 7: Learning to Move</h2>

<ul class="unstyled">
  <li>Session chair: Sylvain Lefebvre</li>
</ul>

<dl class="papers-list">
  <dt>Efficient Deformation Learning of Varied Garments with a Structure-Preserving Multilevel Framework</dt>
  <dd>Tianxing Li, Rui Shi, Zihui Li, Takashi Kanai and Qing Zhu</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651286">DOI link</a>, <a href="https://shirui-homepage.com/files/pdf/research/202404psdunet-I3D.pdf">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Due to the highly nonlinear behavior of clothing, modelling fine-scale garment deformation on arbitrary meshes under varied conditions within a unified network poses a significant challenge. Existing methods often compromise on either model generalization, deformation quality, or runtime speed, making them less suitable for real-world applications.</p>
  
<p>To address it, we propose to incorporate multi-source graph construction and pooling to achieve a novel graph learning scheme. We first introduce methods for extracting cues from different deformation correlations and transform the garment mesh into a comprehensive graph enriched with deformation-related information.</p>
  
<p>To enhance the learning capability and generalizability of the model, we present structure-preserving pooling and unpooling strategies for the mesh deformation task, thereby improving information propagation across the mesh and enhancing the realism of deformation.</p>
  
<p>Lastly, we conduct an attribution analysis and visualize the contribution of various vertices in the graph to the output, providing insights into the deformation behavior. The experimental results demonstrate superior performance against state-of-the-art methods.</p>
  
<img src="img/paper_thumbnails/07_01_li.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>Learning Crowd Motion Dynamics with Crowds</dt>
  <dd>Bilas Talukdar, Yunhao Zhang and Tomer Weiss</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651302">DOI link</a>, <a href="https://drive.google.com/file/d/1zIjE_O9P4LQjQOGvtsQULaPc3VyEkjPl/view">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Reinforcement Learning (RL) has become a popular framework for learning desired behaviors for computational agents in graphics and games. In a multi-agent crowd, one major goal is for agents to avoid collisions while navigating in a dynamic environment. Another goal is to simulate natural-looking crowds, which is difficult to define due to the ambiguity as to what is a natural crowd motion.</p>
  
<p>We introduce a novel methodology for simulating crowds, which learns most-preferred crowd simulation behaviors from crowd-sourced votes via Bayesian optimization. Our method uses deep reinforcement learning for simulating crowds, where crowd-sourcing is used to select policy hyper-parameters. Training agents with such parameters results in a crowd simulation that is preferred to users. We demonstrate our method's robustness in multiple scenarios and metrics, where we show it is superior compared to alternate policies and prior work.</p>
  
<img src="img/paper_thumbnails/07_02_talukdar.jpg" class="paper-thumbnail">
</details>
  </dd>
</dl>

<hr>

<h2 id="solvers-and-simulation">Papers 8: Solvers and Simulation</h2>

<ul class="unstyled">
  <li>Session chair: Cem Yuksel</li>
</ul>

<dl class="papers-list">
  <dt>Windblown Sand Around Obstacles – Simulation And Validation Of Deposition Patterns</dt>
  <dd>Nicolas Rosset, Régis Duvigneau, Adrien Bousseau and Guillaume Cordonnier</dd>
  <dd><a href="https://dl.acm.org/doi/10.1145/3651284">DOI link</a>, <a href="http://www-sop.inria.fr/reves/Basilic/2024/RDBC24/">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>Sand dunes are iconic landmarks of deserts, but can also put human infrastructures at risk, for instance by forming near buildings or roads. We present a simulator of sand erosion and deposition to predict how dunes form around and behind obstacles under wind.</p>
  
<p>Inspired by both computer graphics and geo-sciences, our algorithm couples a fast wind flow simulation with physical laws of sand saltation and avalanching, which suffices to reproduce characteristic patterns of sand deposition. In particular, we validate our approach by reproducing real-world erosion and deposition measurements collected by prior work under controlled conditions.</p>
  
<img src="img/paper_thumbnails/08_01_rosset.jpg" class="paper-thumbnail">
</details>
  </dd>
  <dt>A Unified Particle-Based Solver for Non-Newtonian Behaviors Simulation</dt>
  <dd>Chunlei Li, Yang Gao, Jiayi He, Tianwei Cheng, Shuai Li, Aimin Hao and Hong Qin</dd>
  <dd>(invited TVCG paper presentation) <a href="https://ieeexplore.ieee.org/document/10354362">DOI link</a>, <a href="https://arxiv.org/abs/2312.04814v1">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>In this paper, we present a unified framework to simulate non-Newtonian behaviors. We combine viscous and elasto-plastic stress into a unified particle solver to achieve various non-Newtonian behaviors ranging from fluid-like to solid-like.</p>
  
<p>Our constitutive model is based on a Generalized Maxwell model, which incorporates viscosity, elasticity and plasticity in one non-linear framework by a unified way. On the one hand, taking advantage of the viscous term, we construct a series of strain-rate dependent models for classical non-Newtonian behaviors such as shear-thickening, shear-thinning, Bingham plastic, etc. On the other hand, benefiting from the elasto-plastic model, we empower our framework with the ability to simulate solid-like non-Newtonian behaviors, i.e., visco-elasticity/plasticity.</p>
  
<p>In addition, we enrich our method with a heat diffusion model to make our method flexible in simulating phase change. Through sufficient experiments, we demonstrate a wide range of non-Newtonian behaviors ranging from viscous fluid to deformable objects. We believe this non-Newtonian model will enhance the realism of physically-based animation, which has great potential for computer graphics.</p>
</details>
  </dd>
  <dt>MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction</dt>
  <dd>Jin Li, Yang Gao, Ju Dai, Shuai Li, Aimin Hao and Hong Qin</dd>
  <dd>(invited TVCG paper presentation) <a href="https://ieeexplore.ieee.org/abstract/document/10113697">DOI link</a>, <a href="https://arxiv.org/abs/2305.03315">preprint</a></dd>
  <dd>
    <details>
<summary>Abstract</summary>

<p>High-accuracy, high-efficiency physics-based fluid-solid interaction is essential for reality modeling and computer animation in online games or real-time Virtual Reality (VR) systems. However, the large-scale simulation of incompressible fluid and its interaction with the surrounding solid environment is either time-consuming or suffering from the reduced time/space resolution due to the complicated iterative nature pertinent to numerical computations of involved Partial Differential Equations (PDEs). In recent years, we have witnessed significant growth in exploring a different, alternative data-driven approach to addressing some of the existing technical challenges in conventional model-centric graphics and animation methods.</p>
  
<p>This paper showcases some of our exploratory efforts in this direction. One technical concern of our research is to address the central key challenge of how to best construct the numerical solver effectively and how to best integrate spatiotemporal/dimensional neural networks with the available MPM's pressure solvers. In particular, we devise the MPMNet, a hybrid data-driven framework supporting the popular and powerful Material Point Method (MPM), to combine the comprehensive properties of MPM in numerically handling physical behaviors ranging from fluid to deformable solids and the high efficiency of data-driven models. At the architectural level, our MPMNet comprises three primary components: A data processing module to describe the physical properties by way of the input fields; A deep neural network group to learn the spatiotemporal features; And an iterative refinement process to continue to reduce possible numerical errors. The goal of these special technical developments is to aim at involved numerical acceleration while preserving physical accuracy, realizing efficient and accurate fluid-solid interactions in a data-driven fashion. The extensive experimental results verify that our MPMNet can tremendously speed up the computation compared with the popular numerical methods as the complexity of interaction scenes increases while better retaining the numerical accuracy.</p>
</details>
  </dd>
</dl>



            
            <div id="contact-panel" class="highlight-bg flex">
                <div class="one-third">
                    <h2>Contact</h2>
                </div>
                <div class="two-thirds">
                    <p>
                        Send questions to <a href="mailto:general@i3dsymposium.org">general@i3dsymposium.org</a>
                        for general inquiries, registration, and sponsorship.
                    </p>
                    <p>
                        Direct queries about paper submissions to <a href="mailto:papers@i3dsymposium.org">papers@i3dsymposium.org</a>
                        and poster submissions to <a href="mailto:posters@i3dsymposium.org">posters@i3dsymposium.org</a>.
                    </p>
                </div>
            </div>
            

        </main> <!-- #main -->

    </div> <!-- #main-wrapper -->

    <div class="clearfix"></div>

    <footer id="footer">
        <a href="index.html">
            <img id="footer-logo" src="img/banner.png" alt="I3D 2024">
        </a>

        <!-- social media links -->
        <ul id="footer-links">
            <ul>
                <li><a href="contact.html#mailing_list">Join our Mailing List</a></li><li><a href="conference.html">Past Conferences</a></li><li><a href="sponsors.html">Sponsorship</a></li><li><a href="media-kit.html">Media Kit</a></li><li><a href="http://www.acm.org/special-interest-groups/volunteer-resources/officers-manual/policy-against-discrimination-and-harassment" target="_blank">Code of Conduct</a></li>
            </ul>

            <ul>
                <li><a href="https://www.youtube.com/channel/UCZNuH6k97yBaiTerqd0u1_Q/" target="_blank">Youtube</a></li><li><a href="https://mastodon.acm.org/@i3d" target="_blank">Mastodon</a></li><li><a href="http://www.facebook.com/pages/I3D-Symposium-on-Interactive-3D-Graphics-and-Games/147772558570274" target="_blank">Facebook</a></li><li><a href="https://twitter.com/I3DCONF" target="_blank">Twitter</a></li>
            </ul>
        </ul>

        <div id="footer-copy">
            © 2024
            ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games
        </div>

    </footer>

    <script>
        (function(){
            var mails = document.querySelectorAll(".e-mail[data-e-user][data-e-domain]");
            for (var i = 0; i < mails.length; ++i) {
                var user = mails[i].attributes["data-e-user"].value;
                var domain = mails[i].attributes["data-e-domain"].value;
                mails[i].textContent = user + '@' + domain;
                mails[i].classList.remove("e-mail");
            }
        })();
    </script>
  

</body></html>