<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head>
    <meta charset="UTF-8">
    
    <title>UIST 2023</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="Cache-control" content="public">
    <meta http-equiv="content-type" content="text/html">
    <link rel="icon" href="/2023/favicon.ico?v=2" type="image/x-icon">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/2023/assets/style.css?v=1.2">
  </head>
  <body>
    

<script src="/2023/assets/js/navControls.js"></script>
<div class="topnav" id="myTopnav" onmouseleave="disableNav()">
  <a href="/2023/">Home</a>
  <a target="_blank" href="https://programs.sigchi.org/uist/2023/">Program</a>

  <a href="/2023/cfp/">For Authors</a>
  <a href="/2023/attending/">For Attendees</a>
  <a href="/2023/sponsors/">For Sponsors</a>
  
  <div class="dropdown">
    <button id="more-button" class="dropbtn">More <i class="fa fa-caret-down"></i></button>
    <div class="dropdown-content">
      <a class="dropdown-item" href="/2023/organizers/">Organizers</a>
      <a class="dropdown-item" href="/2023/workshops/">Workshops</a>
      <a class="dropdown-item" href="/2023/diversity/">Diversity</a>
      <a class="dropdown-item" href="/2023/sf-guide/">SF Guide</a>
      <a class="dropdown-item" href="/2023/archive/">Archive</a>
      <!-- <a class="dropdown-item" href="/2023/badge/">Welcome</a> -->
    </div>
  </div>
  <a class="icon" href="javascript:void(0);" onmouseover="enableNav()" onclick="toggleNav()">☰</a>
</div> 
<div class="content">
  <div class="logobox">
<img id="mainlogo" src="/2023/assets/images/uist_banner1920.jpg" alt="UIST 2023 Logo">
</div>

<center><b>The Fairmont, San Francisco, California USA | Oct. 29 – Nov. 1, 2023</b></center>

<p>The ACM Symposium on User Interface Software and Technology (UIST) is the premier forum for innovations in human-computer interfaces. Sponsored by ACM special interest groups on computer-human interaction (SIGCHI) and computer graphics (SIGGRAPH), UIST brings together people from diverse areas including graphical &amp; web user interfaces, tangible &amp; ubiquitous computing, virtual &amp; augmented reality, multimedia, new input &amp; output devices, Human-Centered AI, and CSCW. The intimate size and intensive program make UIST an ideal opportunity to exchange research results and ideas.</p>

<table class="dates">
  <tbody>
    <tr>
      <td><strong>Updates</strong></td>
    </tr>
    <tr>
      <td><strong>11/1/2023</strong>  — Closing presentation slides <a href="/2023/assets/files/UIST-2023-Closing-Presentation.pdf">available here</a>.</td>
    </tr>
    <tr>
      <td><strong>11/2/2023</strong>  — UIST 2024 will be held in Pittsburgh, PA, USA!</td>
    </tr>
  </tbody>
</table>

<!-- | [9/19/2023 — Proceedings are now up!](/2023/proceedings/)      |           <em> Program to follow soon. </em>       |
| [9/8/2023 — Early-bird Registration Deadline](/2023/attending/#registration)      |           <em> Registration is now open! </em>       | -->
<!-- | [10/6/2023 — Hotel registration deadline](/2023/attending/#accommodations)     |              -->
<!-- | [10/12/2023 — Women of UIST Lunch RSVP deadline](https://forms.gle/juUpLjVU5TecWE2k7)       |              -->

<!-- [Registration and the Hotel Reservations at the Fairmont are now open! Click here for more details.](https://cvent.me/gvMkYo){:target="_blank"} -->
<!-- [Registration is still open! Click here for more details.](/2023/attending/#accommodations)
{: .hover-arrow } -->

<h2 id="awards">Awards</h2>
<h4 id="lasting-impact">Lasting Impact</h4>
<p><b>Vizwiz: Nearly Real-Time Answers to Visual Questions</b>
<br>
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, Tom Yeh. UIST 2010.</p>

<h4 id="papers---best-papers">Papers - Best Papers</h4>
<p><b>Going Incognito in the Metaverse: Achieving Theoretically Optimal Privacy-Usability Tradeoffs in VR</b><br>
Vivek Nair, Gonzallo Munilla-Garrido, Dawn Song</p>

<p><b>GenAssist: Making Image Generation Accessible</b><br>
Mina Huh, Yi-Hao Peng, Amy Pavel</p>

<p><b>Generative Agents: Interactive Simulacra of Human Behavior</b><br>
Joon Sung Park, Joey O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein</p>

<h4 id="papers---honorable-mentions">Papers - Honorable Mentions</h4>
<p><b>ecoEDA: Recycling E-waste During Electronics Design</b><br>
Jasmine Lu, Beza Desta, K.D. Wu, Romain Nith, Joyce E. Passananti, Pedro Lopes</p>

<p><b>Sustainflatable: Harvesting, Storing and Utilizing Ambient energy for Pneumatic morphing Interfaces</b><br>
Qiuyu Lu, Tianyu Yu, Semina Yi, Yuran Ding, Haipeng Mi, Lining Yao</p>

<p><b>Living Papers: A Language Toolkit for Augmented Scholarly Communication</b><br>
Jeffrey Heer, Matthew Conlen, Vishal Devireddy, Tu Nguyen, Joshua Horowitz</p>

<p><b>Scene Responsiveness for Visuotactile Illusions in Mixed Reality</b><br>
Mohamed Kari, Reinhard Schütte, Raj Sodhi</p>

<h4 id="demos----best-demo">Demos -  Best Demo</h4>
<p><b>Constraint-Driven Robotic Surfaces, at Human-Scale </b><br>
Jesse T Gonzalez, Sonia Prashant,Sapna Tayal, Juhi Kedia,
Alexandra Ion, and Scott E Hudson</p>

<p><b>Z-Ring: Context-Aware Subtle Input Using Single-Point Bio-Impedance Sensing </b><br>
Anandghan Waghmare, Jiexin Ding, Ishan Chatterjee, and Shwetak Patel</p>

<h4 id="demos---honorable-mention">Demos - Honorable Mention</h4>
<p><b>Masonview: Content-Driven Viewport Management</b><br>
Bryan Min, Matthew T Beaudouin-Lafon, Sangho Suh, and Haijun Xia</p>

<p><b>Biohybrid Devices: Prototyping Interactive Devices with Growable Materials</b><br>
Madalina Luciana Nicolae, Vivien Roussel, Marion Koelle, Samuel Huron, Jürgen Steimle, and Marc Teyssier</p>

<p><b>SuperMagneShape: Interactive Usage of a Passive Pin-Based Shape-Changing Display</b><br>
Kentaro Yasu</p>

<p><b>Fluid Reality: High-Resolution, Untethered Haptic Gloves using Electroosmotic Pump Arrays</b><br>
Vivian Shen, Tucker Rae-Grant, Joe Mullenbach, Chris Harrison, and Craig Shultz</p>

<p><b>Taste Retargeting via Chemical Taste Modulators
</b><br>
Jas Brooks, Noor Amin, and Pedro Lopes</p>

<h4 id="demos---peoples-choice">Demos - People’s Choice</h4>

<p><b>ChromaNails: Re-Programmable Multi-Colored High-Resolution On-Body Interfaces using Photochromic Nail Polish
 </b><br>
Magnus Frisk,&nbsp;Mads Vejrup,&nbsp;Frederik Kjaer Soerensen,&nbsp;and Michael Wessely</p>

<p><b>Mo2Hap: Rendering VR Performance Motion Flow to Upper-body Vibrotactile Haptic Feedback</b><br>
Kyungeun Jung,&nbsp;and Sang Ho Yoon</p>

<h4 id="demos---peoples-choice-honorable-mentions">Demos - People’s Choice Honorable Mentions</h4>
<p><b>Fluid Reality: High-Resolution, Untethered Haptic Gloves using
Electroosmotic Pump Arrays</b><br>
Vivian Shen,&nbsp;Tucker Rae-Grant,&nbsp;Joe Mullenbach,&nbsp;Chris Harrison,&nbsp;and Craig Shultz</p>

<p><b>Double-Sided Tactile Interactions for Grasping in VR
</b><br>
Arata Jingu,&nbsp;Anusha Withana,&nbsp;and Jürgen Steimle</p>

<p><b>SuperMagneShape: Interactive Usage of a Passive Pin-Based Shape-Changing Display
</b><br>
Kentaro Yasu</p>

<h4 id="poster---peoples-choice-best-poster">Poster - People’s Choice Best Poster</h4>
<p><b>Virtual Buddy: Redefining Conversational AI Interactions for Individuals with Hand Motor Disabilities</b><br>
Atieh Taheri, Purav Bhardwaj, Arthur Caetano, Alice Zhong, Misha Sra</p>

<h4 id="poster---peoples-choice-honorable-mentions">Poster - People’s Choice Honorable Mentions</h4>
<p><b>SketchingRelatedWork: Finding and Organizing Papers through Inking a Node-Link Diagram</b><br>
Donghyeok Ma, Joon Hyub Lee, Junwoo Yoon, Taegyu Jin, Seok-Hyung Bae</p>

<p><b>An Interactive System for Drawing Cars in Perspective</b><br>
Seung-Jun Lee, Taegyu Jin, Joon Hyub Lee, Seok-Hyung Bae</p>

<p><b>Generative Facial Expressions and Eye Gaze Behavior from Prompts for Multi-Human-Robot Interaction</b><br>
Gabriel J. Serfaty, Virgil O. Barnard IV, Joseph P. Salisbury</p>

<h4 id="student-innovation-contest-sic---best-sic-award">Student Innovation Contest (SIC) - Best SIC Award</h4>
<p><b>LingoLand: An AI-Assisted Immersive Game for Language Learning </b><br>
Olivia Seow (Harvard Univeristy)</p>

<h4 id="student-innovation-contest---sic-honorable-mentions">Student Innovation Contest - SIC Honorable Mentions</h4>
<p><b>AudiLens: Configurable LLM-Generated Audiences for Public Speech Practice </b><br>
Jeongeon Park and DaEun Choi (KAIST)</p>

<p><b>Docent: Digital Operation-Centric Elicitation of Novice-friendly Tutorials  </b><br>
Yihao Zhu, Qinyi Zhou (Tsinghua University)</p>

<h4 id="student-innovation-contest---sic-peoples-choice">Student Innovation Contest - SIC People’s Choice</h4>

<p><b>Smart-Pikachu: Extending Interactivity of Stuffed Animals with Large Language Models </b><br>
Toma Itagaki (Columbia), Richard Li (U. Washington)</p>

<h4 id="student-innovation-contest---sic-peoples-choice-honorable-mentions">Student Innovation Contest - SIC People’s Choice Honorable Mentions</h4>
<p><b>4-Frame Manga Drawing Support System</b><br>
Yuto Nagao, Soichiro Fukuda (Kwansei Gakuin University)</p>

<p><b>ZINify: Transforming Research Papers into Engaging Zines with Large Language Models</b><br>
Jaidev Shriram, S.P.K Sreekala (University of California - San Diego)</p>

<h2 id="sponsors">Sponsors</h2>

<p>UIST 2023 is possible in part due to gracious support from our sponsors.</p>

<p class="hover-arrow"><a href="/2023/sponsors/">Interested in sponsoring?</a></p>

<h4 id="platinum-sponsors">Platinum Sponsors</h4>

<p><a href="https://about.meta.com/realitylabs/" class="platinum"><img src="/2023/assets/sponsors/meta.png" alt="Meta logo"></a></p>

<h4 id="silver-sponsors">Silver Sponsors</h4>

<p><a href="https://www.adobe.com/" class="wide"><img src="/2023/assets/sponsors/adobe.png" alt="Adobe logo"></a></p>

<p><a href="https://www.apple.com/" class="square"><img src="/2023/assets/sponsors/apple.png" alt="Apple logo"></a></p>

<p><a href="https://www.autodesk.com/" class="verywide"><img src="/2023/assets/sponsors/autodesk.png" alt="Autodesk logo"></a></p>

<p><a href="https://www.google.com/" class="wide"><img src="/2023/assets/sponsors/google.png" alt="Google logo"></a></p>

<h4 id="bronze-sponsors">Bronze Sponsors</h4>

<p><a href="https://www.tri.global/" class="wide"><img src="/2023/assets/sponsors/trilogo.png" alt="Toyota Research Institute logo"></a></p>

<p><a href="https://www.jpmorganchase.com/" class="wide"><img src="/2023/assets/sponsors/chase.png" alt="Chase logo"></a></p>

<p><a href="https://www.csiro.au/en/careers/career-opportunities/Data61-careers" class="square"><img src="/2023/assets/sponsors/csiro.png" alt="CSIRO logo"></a></p>

<h2 id="david-holz-opening-keynote">David Holz (Opening Keynote)</h2>

<h4 id="abstract">Abstract</h4>
<p>A wide-ranging conversation with David Holz, founder of Midjourney and Leap Motion, moderated by Jeff Han.</p>

<h4 id="bio">Bio</h4>
<p><img width="384px" src="/2023/assets/keynote_david_holz.png" alt="David Holz"></p>

<p>David Holz is the founder and CEO of Midjourney. Previously, he was co-founder and CTO of Leap Motion. David contracted for NASA’s Langley Research Center and conducted neuroscience research while at the Max Planck Institute. He studied Applied Math at the University of North Carolina at Chapel Hill, ultimately leaving his PhD program to co-found Leap Motion in 2010. David is based here in San Francisco, California.</p>

<h2 id="judy-fan-closing-keynote">Judy Fan (Closing Keynote)</h2>

<h4 id="abstract-1">Abstract</h4>

<p>Cognitive tools for uncovering useful abstractions</p>

<p>In the 17th century, the Cartesian coordinate system was groundbreaking. It exposed the unity between algebra and geometry, accelerating the development of the math that took humans to the moon. It was not just another concept, but a cognitive tool that people could wield to express abstract ideas in visual form, thereby expanding their capacity to think and generate new insights about a variety of other problems. Research in my lab aims to uncover the psychological mechanisms that explain how people have come to deploy these technologies in such innovative ways to learn, share knowledge, and create new things. In the first part of this talk, I will provide an overview of our recent work investigating drawing — one of our most enduring and versatile tools. Across several empirical and computational studies, I’ll argue that drawing not only provides a window into how we perceive and understand the visual world, but also accelerates humans’ ability to learn and communicate useful abstractions. In the second part, I will describe an emerging line of work investigating how humans discover new abstractions when building physical structures, and externalize these abstractions to support planning and collaboration. I will close by noting the broader implications of embracing such complex, naturalistic behaviors for advancing theories of human cognition and enhancing real-world impact, including in AI and education.</p>

<h4 id="bio-1">Bio</h4>
<p><img width="384px" src="/2023/assets/keynote_judy_fan.jpeg" alt="Judy Fan"></p>

<p>Judy Fan is an Assistant Professor of Psychology at Stanford University. Research in her lab aims to reverse engineer the human cognitive toolkit, especially how people use physical representations of thought to learn, communicate, and solve problems. Towards this end, her lab employs converging approaches from cognitive science, computational neuroscience, and artificial intelligence. She previously held a faculty appointment at the University of California, San Diego, received her PhD in Psychology from Princeton University, and received her AB in Neurobiology and Statistics from Harvard College.</p>


  <small class="footer">© Copyright 2023 ACM SIGCHI</small>
</div>

  

</body></html>